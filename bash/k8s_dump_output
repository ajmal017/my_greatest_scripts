Client Version: version.Info{Major:"1", Minor:"8", GitVersion:"v1.8.4", GitCommit:"9befc2b8928a9426501d3bf62f72849d5cbcd5a3", GitTreeState:"clean", BuildDate:"2017-11-20T05:28:34Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"8", GitVersion:"v1.8.4", GitCommit:"9befc2b8928a9426501d3bf62f72849d5cbcd5a3", GitTreeState:"clean", BuildDate:"2017-11-20T05:17:43Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
[0;32mKubernetes master[0m is running at [0;33mhttps://10.10.97.20:6443[0m
[0;32mElasticsearch[0m is running at [0;33mhttps://10.10.97.20:6443/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy[0m
[0;32mKibana[0m is running at [0;33mhttps://10.10.97.20:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy[0m
[0;32mKubeDNS[0m is running at [0;33mhttps://10.10.97.20:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy[0m

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

================
kubectl get all --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                     DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE       CONTAINERS                IMAGES                                                                                                         SELECTOR                            LABELS
default       ds/ccphxvolume           2         2         2         2            2           <none>          4d        hxvolume                  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge                                                name=ccphxvolume                    name=ccphxvolume
default       ds/kube-keepalived-vip   1         1         1         1            1           <none>          3h        kube-keepalived-vip       k8s.gcr.io/kube-keepalived-vip:0.11                                                                            name=kube-keepalived-vip            name=kube-keepalived-vip
kube-system   ds/calico-node           2         2         2         2            2           <none>          4d        calico-node,install-cni   registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master,registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master   k8s-app=calico-node                 k8s-app=calico-node
kube-system   ds/fluentd-es-v2.0.2     1         1         1         1            1           <none>          4d        fluentd-es                registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2                              k8s-app=fluentd-es,version=v2.0.2   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,version=v2.0.2
kube-system   ds/kube-proxy            2         2         2         2            2           <none>          4d        kube-proxy                gcr.io/google_containers/kube-proxy-amd64:v1.8.4                                                               k8s-app=kube-proxy                  k8s-app=kube-proxy

NAMESPACE     NAME                                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                               LABELS
default       deploy/my-nginx-ingress-controller        1         1         1            1           50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,release=my-nginx-ingress        app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       deploy/my-nginx-ingress-default-backend   1         1         1            1           50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,release=my-nginx-ingress   app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
kube-system   deploy/calico-typha                       0         0         0            0           4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha                                                   k8s-app=calico-typha
kube-system   deploy/kibana-logging                     1         1         1            1           4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging                                                 addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true
kube-system   deploy/kube-dns                           1         1         1            1           4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns                                                       k8s-app=kube-dns
kube-system   deploy/kubernetes-dashboard               1         1         1            1           4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard                                           k8s-app=kubernetes-dashboard
kube-system   deploy/tiller-deploy                      1         1         1            1           4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller                                                   app=helm,name=tiller

NAMESPACE     NAME                                             DESIRED   CURRENT   READY     AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                                                            LABELS
default       rs/my-nginx-ingress-controller-d95d4979d         1         1         1         50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress         app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
default       rs/my-nginx-ingress-default-backend-855d89f775   1         1         1         50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress   app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
kube-system   rs/calico-typha-65b7467b56                       0         0         0         4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha,pod-template-hash=2163023612                                                   k8s-app=calico-typha,pod-template-hash=2163023612
kube-system   rs/kibana-logging-767cf49759                     1         1         1         4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging,pod-template-hash=3237905315                                                 k8s-app=kibana-logging,pod-template-hash=3237905315
kube-system   rs/kube-dns-545bc4bfd4                           1         1         1         4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns,pod-template-hash=1016706980                                                       k8s-app=kube-dns,pod-template-hash=1016706980
kube-system   rs/kubernetes-dashboard-7798c48646               1         1         1         4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard,pod-template-hash=3354704202                                           k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
kube-system   rs/tiller-deploy-546cf9696c                      1         1         1         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=1027952527                                                   app=helm,name=tiller,pod-template-hash=1027952527
kube-system   rs/tiller-deploy-5b9d65c7f                       0         0         0         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=165821739                                                    app=helm,name=tiller,pod-template-hash=165821739

NAMESPACE     NAME                                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                               LABELS
default       deploy/my-nginx-ingress-controller        1         1         1            1           50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,release=my-nginx-ingress        app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       deploy/my-nginx-ingress-default-backend   1         1         1            1           50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,release=my-nginx-ingress   app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
kube-system   deploy/calico-typha                       0         0         0            0           4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha                                                   k8s-app=calico-typha
kube-system   deploy/kibana-logging                     1         1         1            1           4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging                                                 addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true
kube-system   deploy/kube-dns                           1         1         1            1           4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns                                                       k8s-app=kube-dns
kube-system   deploy/kubernetes-dashboard               1         1         1            1           4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard                                           k8s-app=kubernetes-dashboard
kube-system   deploy/tiller-deploy                      1         1         1            1           4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller                                                   app=helm,name=tiller

NAMESPACE     NAME                                 DESIRED   CURRENT   AGE       CONTAINERS              IMAGES                                                                      LABELS
kube-system   statefulsets/elasticsearch-logging   2         2         4d        elasticsearch-logging   registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4

NAMESPACE     NAME                     DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE       CONTAINERS                IMAGES                                                                                                         SELECTOR                            LABELS
default       ds/ccphxvolume           2         2         2         2            2           <none>          4d        hxvolume                  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge                                                name=ccphxvolume                    name=ccphxvolume
default       ds/kube-keepalived-vip   1         1         1         1            1           <none>          3h        kube-keepalived-vip       k8s.gcr.io/kube-keepalived-vip:0.11                                                                            name=kube-keepalived-vip            name=kube-keepalived-vip
kube-system   ds/calico-node           2         2         2         2            2           <none>          4d        calico-node,install-cni   registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master,registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master   k8s-app=calico-node                 k8s-app=calico-node
kube-system   ds/fluentd-es-v2.0.2     1         1         1         1            1           <none>          4d        fluentd-es                registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2                              k8s-app=fluentd-es,version=v2.0.2   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,version=v2.0.2
kube-system   ds/kube-proxy            2         2         2         2            2           <none>          4d        kube-proxy                gcr.io/google_containers/kube-proxy-amd64:v1.8.4                                                               k8s-app=kube-proxy                  k8s-app=kube-proxy

NAMESPACE     NAME                                             DESIRED   CURRENT   READY     AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                                                            LABELS
default       rs/my-nginx-ingress-controller-d95d4979d         1         1         1         50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress         app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
default       rs/my-nginx-ingress-default-backend-855d89f775   1         1         1         50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress   app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
kube-system   rs/calico-typha-65b7467b56                       0         0         0         4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha,pod-template-hash=2163023612                                                   k8s-app=calico-typha,pod-template-hash=2163023612
kube-system   rs/kibana-logging-767cf49759                     1         1         1         4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging,pod-template-hash=3237905315                                                 k8s-app=kibana-logging,pod-template-hash=3237905315
kube-system   rs/kube-dns-545bc4bfd4                           1         1         1         4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns,pod-template-hash=1016706980                                                       k8s-app=kube-dns,pod-template-hash=1016706980
kube-system   rs/kubernetes-dashboard-7798c48646               1         1         1         4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard,pod-template-hash=3354704202                                           k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
kube-system   rs/tiller-deploy-546cf9696c                      1         1         1         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=1027952527                                                   app=helm,name=tiller,pod-template-hash=1027952527
kube-system   rs/tiller-deploy-5b9d65c7f                       0         0         0         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=165821739                                                    app=helm,name=tiller,pod-template-hash=165821739

NAMESPACE     NAME                                                   READY     STATUS    RESTARTS   AGE       IP             NODE                    LABELS
default       po/ccphxvolume-t972j                                   1/1       Running   0          4d        192.168.2.3    vhosakot1-wc80d3e5ab6   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       po/ccphxvolume-tgb9r                                   1/1       Running   0          4d        192.168.1.2    vhosakot1-we2d86faeb2   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       po/ccphxvolume-vlgmz                                   1/1       Running   0          4d        192.168.0.2    vhosakot1-m51b5b468be   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       po/kube-keepalived-vip-fph6b                           1/1       Running   3          3h        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=617734581,name=kube-keepalived-vip,pod-template-generation=1
default       po/my-nginx-ingress-controller-d95d4979d-kzgk7         1/1       Running   0          50m       192.168.2.19   vhosakot1-wc80d3e5ab6   app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
default       po/my-nginx-ingress-default-backend-855d89f775-p8jk6   1/1       Running   0          50m       192.168.2.18   vhosakot1-wc80d3e5ab6   app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
default       po/tea-rc-96442                                        1/1       Running   0          1d        192.168.2.15   vhosakot1-wc80d3e5ab6   app=tea
default       po/tea-rc-fxj6x                                        1/1       Running   0          1d        192.168.2.14   vhosakot1-wc80d3e5ab6   app=tea
default       po/tea-rc-v8rxv                                        1/1       Running   0          1d        192.168.1.52   vhosakot1-we2d86faeb2   app=tea
kube-system   po/calico-node-2cpcv                                   2/2       Running   0          4d        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   po/calico-node-g99qc                                   2/2       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   po/calico-node-kw9hm                                   2/2       Running   0          4d        10.10.97.46    vhosakot1-we2d86faeb2   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   po/elasticsearch-logging-0                             1/1       Running   0          4d        192.168.1.4    vhosakot1-we2d86faeb2   controller-revision-hash=elasticsearch-logging-7978c6964c,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4
kube-system   po/elasticsearch-logging-1                             1/1       Running   0          4d        192.168.2.5    vhosakot1-wc80d3e5ab6   controller-revision-hash=elasticsearch-logging-7978c6964c,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4
kube-system   po/etcd-vhosakot1-m51b5b468be                          1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=etcd,tier=control-plane
kube-system   po/fluentd-es-v2.0.2-9bmjw                             1/1       Running   0          4d        192.168.2.2    vhosakot1-wc80d3e5ab6   controller-revision-hash=1193446001,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,pod-template-generation=1,version=v2.0.2
kube-system   po/fluentd-es-v2.0.2-kmbqt                             1/1       Running   0          4d        192.168.1.3    vhosakot1-we2d86faeb2   controller-revision-hash=1193446001,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,pod-template-generation=1,version=v2.0.2
kube-system   po/kibana-logging-767cf49759-f8zjt                     1/1       Running   0          4d        192.168.2.4    vhosakot1-wc80d3e5ab6   k8s-app=kibana-logging,pod-template-hash=3237905315
kube-system   po/kube-apiserver-vhosakot1-m51b5b468be                1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-apiserver,tier=control-plane
kube-system   po/kube-controller-manager-vhosakot1-m51b5b468be       1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-controller-manager,tier=control-plane
kube-system   po/kube-dns-545bc4bfd4-rzpz4                           3/3       Running   0          4d        192.168.0.4    vhosakot1-m51b5b468be   k8s-app=kube-dns,pod-template-hash=1016706980
kube-system   po/kube-proxy-8vlsv                                    1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   po/kube-proxy-nfkkf                                    1/1       Running   0          4d        10.10.97.46    vhosakot1-we2d86faeb2   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   po/kube-proxy-q8ng8                                    1/1       Running   0          4d        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   po/kube-scheduler-vhosakot1-m51b5b468be                1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-scheduler,tier=control-plane
kube-system   po/kubernetes-dashboard-7798c48646-rjmch               1/1       Running   0          4d        192.168.0.3    vhosakot1-m51b5b468be   k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
kube-system   po/tiller-deploy-546cf9696c-w4kq6                      1/1       Running   0          4d        192.168.2.6    vhosakot1-wc80d3e5ab6   app=helm,name=tiller,pod-template-hash=1027952527

NAMESPACE   NAME        DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES             SELECTOR   LABELS
default     rc/tea-rc   3         3         3         1d        tea          nginxdemos/hello   app=tea    app=tea

NAMESPACE     NAME                                   TYPE        CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE       SELECTOR                                                               LABELS
default       svc/kubernetes                         ClusterIP   10.96.0.1       <none>         443/TCP          4d        <none>                                                                 component=apiserver,provider=kubernetes
default       svc/my-nginx-ingress-controller        ClusterIP   10.107.38.119   10.10.97.200   80/TCP,443/TCP   50m       app=nginx-ingress,component=controller,release=my-nginx-ingress        app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       svc/my-nginx-ingress-default-backend   ClusterIP   10.96.69.91     <none>         80/TCP           50m       app=nginx-ingress,component=default-backend,release=my-nginx-ingress   app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
default       svc/tea-svc                            ClusterIP   10.96.196.71    <none>         80/TCP           1d        app=tea                                                                app=tea
kube-system   svc/calico-typha                       ClusterIP   10.99.51.177    <none>         5473/TCP         4d        k8s-app=calico-typha                                                   k8s-app=calico-typha
kube-system   svc/elasticsearch-logging              ClusterIP   10.105.23.60    <none>         9200/TCP         4d        k8s-app=elasticsearch-logging                                          addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Elasticsearch
kube-system   svc/kibana-logging                     NodePort    10.96.208.116   <none>         5601:30601/TCP   4d        k8s-app=kibana-logging                                                 addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Kibana
kube-system   svc/kube-dns                           ClusterIP   10.96.0.10      <none>         53/UDP,53/TCP    4d        k8s-app=kube-dns                                                       k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeDNS
kube-system   svc/kubernetes-dashboard               NodePort    10.106.31.65    <none>         443:31443/TCP    4d        k8s-app=kubernetes-dashboard                                           k8s-app=kubernetes-dashboard
kube-system   svc/tiller-deploy                      ClusterIP   10.104.87.82    <none>         44134/TCP        4d        app=helm,name=tiller                                                   app=helm,name=tiller

================
kubectl get certificatesigningrequests --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                                                   AGE       REQUESTOR                 CONDITION         LABELS
            node-csr-8Aa5zv54iOqpJoBvflvn2nWYyB8c7zTe1Ptsk8YFepY   4d        system:bootstrap:e06da1   Approved,Issued   <none>
            node-csr-rU0zAAXNDV-3D7DihpcOemzwANHaDyeaLGBYVz8lO1U   4d        system:bootstrap:e06da1   Approved,Issued   <none>

================
kubectl get clusterrolebindings --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                                            AGE       ROLE                                                                               USERS                            GROUPS                                            SERVICEACCOUNTS                          LABELS
            calico-node                                     4d        ClusterRole/calico-node                                                                                                                                               kube-system/calico-node                  <none>
            cluster-admin                                   4d        ClusterRole/cluster-admin                                                                                           system:masters                                                                             kubernetes.io/bootstrapping=rbac-defaults
            elasticsearch-logging                           4d        ClusterRole/elasticsearch-logging                                                                                                                                     kube-system/elasticsearch-logging        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true
            fluentd-es                                      4d        ClusterRole/fluentd-es                                                                                                                                                kube-system/fluentd-es                   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true
            kube-keepalived-vip                             7h        ClusterRole/kube-keepalived-vip                                                                                                                                       default/kube-keepalived-vip              <none>
            kubeadm:kubelet-bootstrap                       4d        ClusterRole/system:node-bootstrapper                                                                                system:bootstrappers:kubeadm:default-node-token                                            <none>
            kubeadm:node-autoapprove-bootstrap              4d        ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient                                        system:bootstrappers:kubeadm:default-node-token                                            <none>
            kubeadm:node-autoapprove-certificate-rotation   4d        ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient                                    system:nodes                                                                               <none>
            kubeadm:node-proxier                            4d        ClusterRole/system:node-proxier                                                                                                                                       kube-system/kube-proxy                   <none>
            kubernetes-dashboard                            4d        ClusterRole/cluster-admin                                                                                                                                             kube-system/kubernetes-dashboard         k8s-app=kubernetes-dashboard
            my-nginx-ingress                                50m       ClusterRole/my-nginx-ingress                                                                                                                                          default/my-nginx-ingress                 app=nginx-ingress,chart=nginx-ingress-0.9.2,heritage=Tiller,release=my-nginx-ingress
            system:basic-user                               4d        ClusterRole/system:basic-user                                                                                       system:authenticated, system:unauthenticated                                               kubernetes.io/bootstrapping=rbac-defaults
            system:controller:attachdetach-controller       4d        ClusterRole/system:controller:attachdetach-controller                                                                                                                 kube-system/attachdetach-controller      kubernetes.io/bootstrapping=rbac-defaults
            system:controller:certificate-controller        4d        ClusterRole/system:controller:certificate-controller                                                                                                                  kube-system/certificate-controller       kubernetes.io/bootstrapping=rbac-defaults
            system:controller:cronjob-controller            4d        ClusterRole/system:controller:cronjob-controller                                                                                                                      kube-system/cronjob-controller           kubernetes.io/bootstrapping=rbac-defaults
            system:controller:daemon-set-controller         4d        ClusterRole/system:controller:daemon-set-controller                                                                                                                   kube-system/daemon-set-controller        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:deployment-controller         4d        ClusterRole/system:controller:deployment-controller                                                                                                                   kube-system/deployment-controller        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:disruption-controller         4d        ClusterRole/system:controller:disruption-controller                                                                                                                   kube-system/disruption-controller        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:endpoint-controller           4d        ClusterRole/system:controller:endpoint-controller                                                                                                                     kube-system/endpoint-controller          kubernetes.io/bootstrapping=rbac-defaults
            system:controller:generic-garbage-collector     4d        ClusterRole/system:controller:generic-garbage-collector                                                                                                               kube-system/generic-garbage-collector    kubernetes.io/bootstrapping=rbac-defaults
            system:controller:horizontal-pod-autoscaler     4d        ClusterRole/system:controller:horizontal-pod-autoscaler                                                                                                               kube-system/horizontal-pod-autoscaler    kubernetes.io/bootstrapping=rbac-defaults
            system:controller:job-controller                4d        ClusterRole/system:controller:job-controller                                                                                                                          kube-system/job-controller               kubernetes.io/bootstrapping=rbac-defaults
            system:controller:namespace-controller          4d        ClusterRole/system:controller:namespace-controller                                                                                                                    kube-system/namespace-controller         kubernetes.io/bootstrapping=rbac-defaults
            system:controller:node-controller               4d        ClusterRole/system:controller:node-controller                                                                                                                         kube-system/node-controller              kubernetes.io/bootstrapping=rbac-defaults
            system:controller:persistent-volume-binder      4d        ClusterRole/system:controller:persistent-volume-binder                                                                                                                kube-system/persistent-volume-binder     kubernetes.io/bootstrapping=rbac-defaults
            system:controller:pod-garbage-collector         4d        ClusterRole/system:controller:pod-garbage-collector                                                                                                                   kube-system/pod-garbage-collector        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:replicaset-controller         4d        ClusterRole/system:controller:replicaset-controller                                                                                                                   kube-system/replicaset-controller        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:replication-controller        4d        ClusterRole/system:controller:replication-controller                                                                                                                  kube-system/replication-controller       kubernetes.io/bootstrapping=rbac-defaults
            system:controller:resourcequota-controller      4d        ClusterRole/system:controller:resourcequota-controller                                                                                                                kube-system/resourcequota-controller     kubernetes.io/bootstrapping=rbac-defaults
            system:controller:route-controller              4d        ClusterRole/system:controller:route-controller                                                                                                                        kube-system/route-controller             kubernetes.io/bootstrapping=rbac-defaults
            system:controller:service-account-controller    4d        ClusterRole/system:controller:service-account-controller                                                                                                              kube-system/service-account-controller   kubernetes.io/bootstrapping=rbac-defaults
            system:controller:service-controller            4d        ClusterRole/system:controller:service-controller                                                                                                                      kube-system/service-controller           kubernetes.io/bootstrapping=rbac-defaults
            system:controller:statefulset-controller        4d        ClusterRole/system:controller:statefulset-controller                                                                                                                  kube-system/statefulset-controller       kubernetes.io/bootstrapping=rbac-defaults
            system:controller:ttl-controller                4d        ClusterRole/system:controller:ttl-controller                                                                                                                          kube-system/ttl-controller               kubernetes.io/bootstrapping=rbac-defaults
            system:discovery                                4d        ClusterRole/system:discovery                                                                                        system:authenticated, system:unauthenticated                                               kubernetes.io/bootstrapping=rbac-defaults
            system:kube-controller-manager                  4d        ClusterRole/system:kube-controller-manager                                         system:kube-controller-manager                                                                                              kubernetes.io/bootstrapping=rbac-defaults
            system:kube-dns                                 4d        ClusterRole/system:kube-dns                                                                                                                                           kube-system/kube-dns                     kubernetes.io/bootstrapping=rbac-defaults
            system:kube-scheduler                           4d        ClusterRole/system:kube-scheduler                                                  system:kube-scheduler                                                                                                       kubernetes.io/bootstrapping=rbac-defaults
            system:node                                     4d        ClusterRole/system:node                                                                                                                                                                                        kubernetes.io/bootstrapping=rbac-defaults
            system:node-proxier                             4d        ClusterRole/system:node-proxier                                                    system:kube-proxy                                                                                                           kubernetes.io/bootstrapping=rbac-defaults
            tiller-cluster-rule                             4d        ClusterRole/cluster-admin                                                                                                                                             kube-system/tiller                       <none>

================
kubectl get clusterroles --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                                                                   AGE       LABELS
            admin                                                                  4d        kubernetes.io/bootstrapping=rbac-defaults
            calico-node                                                            4d        <none>
            cluster-admin                                                          4d        kubernetes.io/bootstrapping=rbac-defaults
            edit                                                                   4d        kubernetes.io/bootstrapping=rbac-defaults
            elasticsearch-logging                                                  4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true
            fluentd-es                                                             4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true
            kube-keepalived-vip                                                    7h        <none>
            my-nginx-ingress                                                       50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,heritage=Tiller,release=my-nginx-ingress
            system:auth-delegator                                                  4d        kubernetes.io/bootstrapping=rbac-defaults
            system:basic-user                                                      4d        kubernetes.io/bootstrapping=rbac-defaults
            system:certificates.k8s.io:certificatesigningrequests:nodeclient       4d        kubernetes.io/bootstrapping=rbac-defaults
            system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:attachdetach-controller                              4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:certificate-controller                               4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:cronjob-controller                                   4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:daemon-set-controller                                4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:deployment-controller                                4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:disruption-controller                                4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:endpoint-controller                                  4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:generic-garbage-collector                            4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:horizontal-pod-autoscaler                            4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:job-controller                                       4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:namespace-controller                                 4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:node-controller                                      4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:persistent-volume-binder                             4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:pod-garbage-collector                                4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:replicaset-controller                                4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:replication-controller                               4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:resourcequota-controller                             4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:route-controller                                     4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:service-account-controller                           4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:service-controller                                   4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:statefulset-controller                               4d        kubernetes.io/bootstrapping=rbac-defaults
            system:controller:ttl-controller                                       4d        kubernetes.io/bootstrapping=rbac-defaults
            system:discovery                                                       4d        kubernetes.io/bootstrapping=rbac-defaults
            system:heapster                                                        4d        kubernetes.io/bootstrapping=rbac-defaults
            system:kube-aggregator                                                 4d        kubernetes.io/bootstrapping=rbac-defaults
            system:kube-controller-manager                                         4d        kubernetes.io/bootstrapping=rbac-defaults
            system:kube-dns                                                        4d        kubernetes.io/bootstrapping=rbac-defaults
            system:kube-scheduler                                                  4d        kubernetes.io/bootstrapping=rbac-defaults
            system:node                                                            4d        kubernetes.io/bootstrapping=rbac-defaults
            system:node-bootstrapper                                               4d        kubernetes.io/bootstrapping=rbac-defaults
            system:node-problem-detector                                           4d        kubernetes.io/bootstrapping=rbac-defaults
            system:node-proxier                                                    4d        kubernetes.io/bootstrapping=rbac-defaults
            system:persistent-volume-provisioner                                   4d        kubernetes.io/bootstrapping=rbac-defaults
            view                                                                   4d        kubernetes.io/bootstrapping=rbac-defaults

================
kubectl get clusters --all-namespaces --show-labels -o wide
================

the server doesn't have a resource type "clusters"

================
kubectl get componentstatuses --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                 STATUS    MESSAGE              ERROR     LABELS
            scheduler            Healthy   ok                             <none>
            controller-manager   Healthy   ok                             <none>
            etcd-0               Healthy   {"health": "true"}             <none>

================
kubectl get configmaps --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                 DATA      AGE       LABELS
default       ingress-controller-leader-nginx      0         4d        <none>
default       my-nginx-ingress-controller          1         50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       vip-configmap                        1         3h        <none>
kube-public   cluster-info                         1         4d        <none>
kube-system   calico-config                        2         4d        <none>
kube-system   extension-apiserver-authentication   6         4d        <none>
kube-system   fluentd-es-config-v0.1.1             5         4d        addonmanager.kubernetes.io/mode=Reconcile
kube-system   kube-proxy                           1         4d        app=kube-proxy
kube-system   kubeadm-config                       1         4d        <none>
kube-system   my-nginx-ingress.v1                  1         50m       MODIFIED_AT=1518558940,NAME=my-nginx-ingress,OWNER=TILLER,STATUS=DEPLOYED,VERSION=1

================
kubectl get controllerrevisions --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                               CONTROLLER                          REVISION   AGE       LABELS
default       ccphxvolume-8b677dc65              DaemonSet/ccphxvolume               1          4d        controller-revision-hash=462338721,name=ccphxvolume
default       kube-keepalived-vip-b5cc789d5      DaemonSet/kube-keepalived-vip       1          3h        controller-revision-hash=617734581,name=kube-keepalived-vip
kube-system   calico-node-76cc6dcd86             DaemonSet/calico-node               1          4d        controller-revision-hash=3277287842,k8s-app=calico-node
kube-system   elasticsearch-logging-7978c6964c   StatefulSet/elasticsearch-logging   1          4d        controller.kubernetes.io/hash=3534725207,k8s-app=elasticsearch-logging,version=v5.6.4
kube-system   fluentd-es-v2.0.2-55f788b445       DaemonSet/fluentd-es-v2.0.2         1          4d        controller-revision-hash=1193446001,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,version=v2.0.2
kube-system   kube-proxy-95856ccc5               DaemonSet/kube-proxy                1          4d        controller-revision-hash=514127771,k8s-app=kube-proxy

================
kubectl get cronjobs --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get customresourcedefinition --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                                          AGE       LABELS
            bgpconfigurations.crd.projectcalico.org       4d        <none>
            bgppeers.crd.projectcalico.org                4d        <none>
            clusterinformations.crd.projectcalico.org     4d        <none>
            felixconfigurations.crd.projectcalico.org     4d        <none>
            globalnetworkpolicies.crd.projectcalico.org   4d        <none>
            ippools.crd.projectcalico.org                 4d        <none>
            networkpolicies.crd.projectcalico.org         4d        <none>

================
kubectl get daemonsets --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                  DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE       CONTAINERS                IMAGES                                                                                                         SELECTOR                            LABELS
default       ccphxvolume           2         2         2         2            2           <none>          4d        hxvolume                  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge                                                name=ccphxvolume                    name=ccphxvolume
default       kube-keepalived-vip   1         1         1         1            1           <none>          3h        kube-keepalived-vip       k8s.gcr.io/kube-keepalived-vip:0.11                                                                            name=kube-keepalived-vip            name=kube-keepalived-vip
kube-system   calico-node           2         2         2         2            2           <none>          4d        calico-node,install-cni   registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master,registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master   k8s-app=calico-node                 k8s-app=calico-node
kube-system   fluentd-es-v2.0.2     1         1         1         1            1           <none>          4d        fluentd-es                registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2                              k8s-app=fluentd-es,version=v2.0.2   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,version=v2.0.2
kube-system   kube-proxy            2         2         2         2            2           <none>          4d        kube-proxy                gcr.io/google_containers/kube-proxy-amd64:v1.8.4                                                               k8s-app=kube-proxy                  k8s-app=kube-proxy

================
kubectl get deployments --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                               LABELS
default       my-nginx-ingress-controller        1         1         1            1           50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,release=my-nginx-ingress        app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       my-nginx-ingress-default-backend   1         1         1            1           50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,release=my-nginx-ingress   app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
kube-system   calico-typha                       0         0         0            0           4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha                                                   k8s-app=calico-typha
kube-system   kibana-logging                     1         1         1            1           4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging                                                 addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true
kube-system   kube-dns                           1         1         1            1           4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns                                                       k8s-app=kube-dns
kube-system   kubernetes-dashboard               1         1         1            1           4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard                                           k8s-app=kubernetes-dashboard
kube-system   tiller-deploy                      1         1         1            1           4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller                                                   app=helm,name=tiller

================
kubectl get endpoints --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                               ENDPOINTS                                         AGE       LABELS
default       kubernetes                         10.10.97.20:6443                                  4d        <none>
default       my-nginx-ingress-controller        192.168.2.19:443,192.168.2.19:80                  50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       my-nginx-ingress-default-backend   192.168.2.18:8080                                 50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
default       tea-svc                            192.168.1.52:80,192.168.2.14:80,192.168.2.15:80   1d        app=tea
kube-system   calico-typha                       <none>                                            4d        k8s-app=calico-typha
kube-system   elasticsearch-logging              192.168.1.4:9200,192.168.2.5:9200                 4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Elasticsearch
kube-system   kibana-logging                     192.168.2.4:5601                                  4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Kibana
kube-system   kube-controller-manager            <none>                                            4d        <none>
kube-system   kube-dns                           192.168.0.4:53,192.168.0.4:53                     4d        k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeDNS
kube-system   kube-scheduler                     <none>                                            4d        <none>
kube-system   kubernetes-dashboard               192.168.0.3:8443                                  4d        k8s-app=kubernetes-dashboard
kube-system   tiller-deploy                      192.168.2.6:44134                                 4d        app=helm,name=tiller

================
kubectl get events --all-namespaces --show-labels -o wide
================

NAMESPACE   LAST SEEN   FIRST SEEN   COUNT     NAME                                                                 KIND         SUBOBJECT                                        TYPE      REASON                  SOURCE                           MESSAGE                                                                                                              LABELS
default     49m         49m          1         cafe-ingress.15130151c58ea80b                                        Ingress                                                       Normal    CREATE                  nginx-ingress-controller         Ingress default/cafe-ingress                                                                                         <none>
default     49m         3h           4         kube-keepalived-vip-fph6b.1512f7ca5b38a1cc                           Pod          spec.containers{kube-keepalived-vip}             Normal    Pulling                 kubelet, vhosakot1-wc80d3e5ab6   pulling image "k8s.gcr.io/kube-keepalived-vip:0.11"                                                                  <none>
default     49m         3h           4         kube-keepalived-vip-fph6b.1512f7ca825390ad                           Pod          spec.containers{kube-keepalived-vip}             Normal    Pulled                  kubelet, vhosakot1-wc80d3e5ab6   Successfully pulled image "k8s.gcr.io/kube-keepalived-vip:0.11"                                                      <none>
default     49m         3h           4         kube-keepalived-vip-fph6b.1512f7ca85d1fe56                           Pod          spec.containers{kube-keepalived-vip}             Normal    Created                 kubelet, vhosakot1-wc80d3e5ab6   Created container                                                                                                    <none>
default     49m         3h           4         kube-keepalived-vip-fph6b.1512f7ca8bd6ddbd                           Pod          spec.containers{kube-keepalived-vip}             Normal    Started                 kubelet, vhosakot1-wc80d3e5ab6   Started container                                                                                                    <none>
default     49m         50m          3         kube-keepalived-vip-fph6b.1513014e24e93088                           Pod          spec.containers{kube-keepalived-vip}             Warning   BackOff                 kubelet, vhosakot1-wc80d3e5ab6   Back-off restarting failed container                                                                                 <none>
default     49m         50m          3         kube-keepalived-vip-fph6b.1513014e24eef2af                           Pod                                                           Warning   FailedSync              kubelet, vhosakot1-wc80d3e5ab6   Error syncing pod                                                                                                    <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-cs7mm.1513014bd41394f2         Pod          spec.containers{nginx-ingress-controller}        Warning   Unhealthy               kubelet, vhosakot1-wc80d3e5ab6   Liveness probe failed: HTTP probe failed with statuscode: 500                                                        <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-cs7mm.1513014c53ffe35a         Pod          spec.containers{nginx-ingress-controller}        Warning   Unhealthy               kubelet, vhosakot1-wc80d3e5ab6   Readiness probe failed: HTTP probe failed with statuscode: 500                                                       <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-cs7mm.1513014d53723904         Pod          spec.containers{nginx-ingress-controller}        Normal    Killing                 kubelet, vhosakot1-wc80d3e5ab6   Killing container with id docker://nginx-ingress-controller:Need to kill Pod                                         <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f669c2243         Pod                                                           Normal    Scheduled               default-scheduler                Successfully assigned my-nginx-ingress-controller-d95d4979d-kzgk7 to vhosakot1-wc80d3e5ab6                           <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f9ddacac3         Pod                                                           Normal    SuccessfulMountVolume   kubelet, vhosakot1-wc80d3e5ab6   MountVolume.SetUp succeeded for volume "my-nginx-ingress-token-9ffd9"                                                <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc229c270         Pod          spec.containers{nginx-ingress-controller}        Normal    Pulled                  kubelet, vhosakot1-wc80d3e5ab6   Container image "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2" already present on machine   <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc7026fae         Pod          spec.containers{nginx-ingress-controller}        Normal    Created                 kubelet, vhosakot1-wc80d3e5ab6   Created container                                                                                                    <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fceb03e13         Pod          spec.containers{nginx-ingress-controller}        Normal    Started                 kubelet, vhosakot1-wc80d3e5ab6   Started container                                                                                                    <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d.1513014a9836c301               ReplicaSet                                                    Normal    SuccessfulDelete        replicaset-controller            Deleted pod: my-nginx-ingress-controller-d95d4979d-cs7mm                                                             <none>
default     50m         50m          1         my-nginx-ingress-controller-d95d4979d.1513014f660b56e4               ReplicaSet                                                    Normal    SuccessfulCreate        replicaset-controller            Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7                                                             <none>
default     50m         50m          1         my-nginx-ingress-controller.1513014a979e01a5                         Deployment                                                    Normal    ScalingReplicaSet       deployment-controller            Scaled down replica set my-nginx-ingress-controller-d95d4979d to 0                                                   <none>
default     50m         50m          1         my-nginx-ingress-controller.1513014f64f57f7f                         Deployment                                                    Normal    ScalingReplicaSet       deployment-controller            Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1                                                     <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p495h.1513014b7ac09d99   Pod          spec.containers{nginx-ingress-default-backend}   Normal    Killing                 kubelet, vhosakot1-wc80d3e5ab6   Killing container with id docker://nginx-ingress-default-backend:Need to kill Pod                                    <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f67b5a669   Pod                                                           Normal    Scheduled               default-scheduler                Successfully assigned my-nginx-ingress-default-backend-855d89f775-p8jk6 to vhosakot1-wc80d3e5ab6                     <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f9dd3ee65   Pod                                                           Normal    SuccessfulMountVolume   kubelet, vhosakot1-wc80d3e5ab6   MountVolume.SetUp succeeded for volume "default-token-fc6vg"                                                         <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc0342d14   Pod          spec.containers{nginx-ingress-default-backend}   Normal    Pulled                  kubelet, vhosakot1-wc80d3e5ab6   Container image "k8s.gcr.io/defaultbackend:1.3" already present on machine                                           <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc5b17d44   Pod          spec.containers{nginx-ingress-default-backend}   Normal    Created                 kubelet, vhosakot1-wc80d3e5ab6   Created container                                                                                                    <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fccc1a9d7   Pod          spec.containers{nginx-ingress-default-backend}   Normal    Started                 kubelet, vhosakot1-wc80d3e5ab6   Started container                                                                                                    <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775.1513014b501f2edd         ReplicaSet                                                    Normal    SuccessfulDelete        replicaset-controller            Deleted pod: my-nginx-ingress-default-backend-855d89f775-p495h                                                       <none>
default     50m         50m          1         my-nginx-ingress-default-backend-855d89f775.1513014f670fda78         ReplicaSet                                                    Normal    SuccessfulCreate        replicaset-controller            Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6                                                       <none>
default     50m         50m          1         my-nginx-ingress-default-backend.1513014b4f9d7b8e                    Deployment                                                    Normal    ScalingReplicaSet       deployment-controller            Scaled down replica set my-nginx-ingress-default-backend-855d89f775 to 0                                             <none>
default     50m         50m          1         my-nginx-ingress-default-backend.1513014f656e8854                    Deployment                                                    Normal    ScalingReplicaSet       deployment-controller            Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1                                               <none>

================
kubectl get horizontalpodautoscalers --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get ingresses --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME           HOSTS              ADDRESS   PORTS     AGE       LABELS
default     cafe-ingress   cafe.example.com             80, 443   1d        <none>

================
kubectl get jobs --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get limitranges --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get namespaces --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME          STATUS    AGE       LABELS
            default       Active    4d        <none>
            kube-public   Active    4d        <none>
            kube-system   Active    4d        <none>

================
kubectl get networkpolicies --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get nodes --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME                    STATUS    ROLES     AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME   LABELS
            vhosakot1-m51b5b468be   Ready     master    4d        v1.8.4    <none>        Ubuntu 16.04.3 LTS   4.4.0-109-generic   docker://1.13.1     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=vhosakot1-m51b5b468be,node-role.kubernetes.io/master=
            vhosakot1-wc80d3e5ab6   Ready     <none>    4d        v1.8.4    <none>        Ubuntu 16.04.3 LTS   4.4.0-109-generic   docker://1.13.1     app=keepalived,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=vhosakot1-wc80d3e5ab6
            vhosakot1-we2d86faeb2   Ready     <none>    4d        v1.8.4    <none>        Ubuntu 16.04.3 LTS   4.4.0-109-generic   docker://1.13.1     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=vhosakot1-we2d86faeb2

================
kubectl get persistentvolumeclaims --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get persistentvolumes --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get poddisruptionbudgets --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get podpreset --all-namespaces --show-labels -o wide
================

the server doesn't have a resource type "podpreset"

================
kubectl get pods --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                                READY     STATUS    RESTARTS   AGE       IP             NODE                    LABELS
default       ccphxvolume-t972j                                   1/1       Running   0          4d        192.168.2.3    vhosakot1-wc80d3e5ab6   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       ccphxvolume-tgb9r                                   1/1       Running   0          4d        192.168.1.2    vhosakot1-we2d86faeb2   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       ccphxvolume-vlgmz                                   1/1       Running   0          4d        192.168.0.2    vhosakot1-m51b5b468be   controller-revision-hash=462338721,name=ccphxvolume,pod-template-generation=1
default       kube-keepalived-vip-fph6b                           1/1       Running   3          3h        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=617734581,name=kube-keepalived-vip,pod-template-generation=1
default       my-nginx-ingress-controller-d95d4979d-kzgk7         1/1       Running   0          50m       192.168.2.19   vhosakot1-wc80d3e5ab6   app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
default       my-nginx-ingress-default-backend-855d89f775-p8jk6   1/1       Running   0          50m       192.168.2.18   vhosakot1-wc80d3e5ab6   app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
default       tea-rc-96442                                        1/1       Running   0          1d        192.168.2.15   vhosakot1-wc80d3e5ab6   app=tea
default       tea-rc-fxj6x                                        1/1       Running   0          1d        192.168.2.14   vhosakot1-wc80d3e5ab6   app=tea
default       tea-rc-v8rxv                                        1/1       Running   0          1d        192.168.1.52   vhosakot1-we2d86faeb2   app=tea
kube-system   calico-node-2cpcv                                   2/2       Running   0          4d        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-g99qc                                   2/2       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   calico-node-kw9hm                                   2/2       Running   0          4d        10.10.97.46    vhosakot1-we2d86faeb2   controller-revision-hash=3277287842,k8s-app=calico-node,pod-template-generation=1
kube-system   elasticsearch-logging-0                             1/1       Running   0          4d        192.168.1.4    vhosakot1-we2d86faeb2   controller-revision-hash=elasticsearch-logging-7978c6964c,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4
kube-system   elasticsearch-logging-1                             1/1       Running   0          4d        192.168.2.5    vhosakot1-wc80d3e5ab6   controller-revision-hash=elasticsearch-logging-7978c6964c,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4
kube-system   etcd-vhosakot1-m51b5b468be                          1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=etcd,tier=control-plane
kube-system   fluentd-es-v2.0.2-9bmjw                             1/1       Running   0          4d        192.168.2.2    vhosakot1-wc80d3e5ab6   controller-revision-hash=1193446001,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,pod-template-generation=1,version=v2.0.2
kube-system   fluentd-es-v2.0.2-kmbqt                             1/1       Running   0          4d        192.168.1.3    vhosakot1-we2d86faeb2   controller-revision-hash=1193446001,k8s-app=fluentd-es,kubernetes.io/cluster-service=true,pod-template-generation=1,version=v2.0.2
kube-system   kibana-logging-767cf49759-f8zjt                     1/1       Running   0          4d        192.168.2.4    vhosakot1-wc80d3e5ab6   k8s-app=kibana-logging,pod-template-hash=3237905315
kube-system   kube-apiserver-vhosakot1-m51b5b468be                1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-apiserver,tier=control-plane
kube-system   kube-controller-manager-vhosakot1-m51b5b468be       1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-controller-manager,tier=control-plane
kube-system   kube-dns-545bc4bfd4-rzpz4                           3/3       Running   0          4d        192.168.0.4    vhosakot1-m51b5b468be   k8s-app=kube-dns,pod-template-hash=1016706980
kube-system   kube-proxy-8vlsv                                    1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   kube-proxy-nfkkf                                    1/1       Running   0          4d        10.10.97.46    vhosakot1-we2d86faeb2   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   kube-proxy-q8ng8                                    1/1       Running   0          4d        10.10.97.62    vhosakot1-wc80d3e5ab6   controller-revision-hash=514127771,k8s-app=kube-proxy,pod-template-generation=1
kube-system   kube-scheduler-vhosakot1-m51b5b468be                1/1       Running   0          4d        10.10.97.20    vhosakot1-m51b5b468be   component=kube-scheduler,tier=control-plane
kube-system   kubernetes-dashboard-7798c48646-rjmch               1/1       Running   0          4d        192.168.0.3    vhosakot1-m51b5b468be   k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
kube-system   tiller-deploy-546cf9696c-w4kq6                      1/1       Running   0          4d        192.168.2.6    vhosakot1-wc80d3e5ab6   app=helm,name=tiller,pod-template-hash=1027952527

================
kubectl get podsecuritypolicies --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get podtemplates --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get replicasets --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                          DESIRED   CURRENT   READY     AGE       CONTAINERS                      IMAGES                                                                                                                                                                     SELECTOR                                                                                            LABELS
default       my-nginx-ingress-controller-d95d4979d         1         1         1         50m       nginx-ingress-controller        quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2                                                                                                      app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress         app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
default       my-nginx-ingress-default-backend-855d89f775   1         1         1         50m       nginx-ingress-default-backend   k8s.gcr.io/defaultbackend:1.3                                                                                                                                              app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress   app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
kube-system   calico-typha-65b7467b56                       0         0         0         4d        calico-typha                    registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master                                                                                                                    k8s-app=calico-typha,pod-template-hash=2163023612                                                   k8s-app=calico-typha,pod-template-hash=2163023612
kube-system   kibana-logging-767cf49759                     1         1         1         4d        kibana-logging                  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4                                                                                                          k8s-app=kibana-logging,pod-template-hash=3237905315                                                 k8s-app=kibana-logging,pod-template-hash=3237905315
kube-system   kube-dns-545bc4bfd4                           1         1         1         4d        kubedns,dnsmasq,sidecar         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5,gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5,gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5   k8s-app=kube-dns,pod-template-hash=1016706980                                                       k8s-app=kube-dns,pod-template-hash=1016706980
kube-system   kubernetes-dashboard-7798c48646               1         1         1         4d        kubernetes-dashboard            k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1                                                                                                                               k8s-app=kubernetes-dashboard,pod-template-hash=3354704202                                           k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
kube-system   tiller-deploy-546cf9696c                      1         1         1         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=1027952527                                                   app=helm,name=tiller,pod-template-hash=1027952527
kube-system   tiller-deploy-5b9d65c7f                       0         0         0         4d        tiller                          gcr.io/kubernetes-helm/tiller:v2.7.2                                                                                                                                       app=helm,name=tiller,pod-template-hash=165821739                                                    app=helm,name=tiller,pod-template-hash=165821739

================
kubectl get replicationcontrollers --all-namespaces --show-labels -o wide
================

NAMESPACE   NAME      DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES             SELECTOR   LABELS
default     tea-rc    3         3         3         1d        tea          nginxdemos/hello   app=tea    app=tea

================
kubectl get resourcequotas --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get rolebindings --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                             AGE       ROLE                                                  USERS              GROUPS    SERVICEACCOUNTS                       LABELS
default       my-nginx-ingress                                 50m       Role/my-nginx-ingress                                                              default/my-nginx-ingress              app=nginx-ingress,chart=nginx-ingress-0.9.2,heritage=Tiller,release=my-nginx-ingress
kube-public   kubeadm:bootstrap-signer-clusterinfo             4d        Role/kubeadm:bootstrap-signer-clusterinfo             system:anonymous                                                   <none>
kube-public   system:controller:bootstrap-signer               4d        Role/system:controller:bootstrap-signer                                            kube-system/bootstrap-signer          kubernetes.io/bootstrapping=rbac-defaults
kube-system   kubernetes-dashboard-minimal                     4d        Role/kubernetes-dashboard-minimal                                                  kube-system/kubernetes-dashboard      <none>
kube-system   system::leader-locking-kube-controller-manager   4d        Role/system::leader-locking-kube-controller-manager                                kube-system/kube-controller-manager   kubernetes.io/bootstrapping=rbac-defaults
kube-system   system::leader-locking-kube-scheduler            4d        Role/system::leader-locking-kube-scheduler                                         kube-system/kube-scheduler            kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:bootstrap-signer               4d        Role/system:controller:bootstrap-signer                                            kube-system/bootstrap-signer          kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:cloud-provider                 4d        Role/system:controller:cloud-provider                                              kube-system/cloud-provider            kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:token-cleaner                  4d        Role/system:controller:token-cleaner                                               kube-system/token-cleaner             kubernetes.io/bootstrapping=rbac-defaults

================
kubectl get roles --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                             AGE       LABELS
default       my-nginx-ingress                                 50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,heritage=Tiller,release=my-nginx-ingress
kube-public   kubeadm:bootstrap-signer-clusterinfo             4d        <none>
kube-public   system:controller:bootstrap-signer               4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   extension-apiserver-authentication-reader        4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   kubernetes-dashboard-minimal                     4d        <none>
kube-system   system::leader-locking-kube-controller-manager   4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   system::leader-locking-kube-scheduler            4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:bootstrap-signer               4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:cloud-provider                 4d        kubernetes.io/bootstrapping=rbac-defaults
kube-system   system:controller:token-cleaner                  4d        kubernetes.io/bootstrapping=rbac-defaults

================
kubectl get secrets --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                                     TYPE                                  DATA      AGE       LABELS
default       cafe-secret                              Opaque                                2         1d        <none>
default       default-token-fc6vg                      kubernetes.io/service-account-token   3         4d        <none>
default       kube-keepalived-vip-token-2w47w          kubernetes.io/service-account-token   3         7h        <none>
default       my-nginx-ingress-token-9ffd9             kubernetes.io/service-account-token   3         50m       <none>
kube-public   default-token-7g442                      kubernetes.io/service-account-token   3         4d        <none>
kube-system   attachdetach-controller-token-pdmt4      kubernetes.io/service-account-token   3         4d        <none>
kube-system   bootstrap-signer-token-nhshd             kubernetes.io/service-account-token   3         4d        <none>
kube-system   calico-node-token-m5t54                  kubernetes.io/service-account-token   3         4d        <none>
kube-system   certificate-controller-token-sxs2h       kubernetes.io/service-account-token   3         4d        <none>
kube-system   cronjob-controller-token-4dlm7           kubernetes.io/service-account-token   3         4d        <none>
kube-system   daemon-set-controller-token-4jwtx        kubernetes.io/service-account-token   3         4d        <none>
kube-system   default-token-kxl84                      kubernetes.io/service-account-token   3         4d        <none>
kube-system   deployment-controller-token-8gz7n        kubernetes.io/service-account-token   3         4d        <none>
kube-system   disruption-controller-token-sndts        kubernetes.io/service-account-token   3         4d        <none>
kube-system   elasticsearch-logging-token-6cmks        kubernetes.io/service-account-token   3         4d        <none>
kube-system   endpoint-controller-token-6xqjn          kubernetes.io/service-account-token   3         4d        <none>
kube-system   fluentd-es-token-zw92g                   kubernetes.io/service-account-token   3         4d        <none>
kube-system   generic-garbage-collector-token-m25fs    kubernetes.io/service-account-token   3         4d        <none>
kube-system   horizontal-pod-autoscaler-token-jbgqq    kubernetes.io/service-account-token   3         4d        <none>
kube-system   job-controller-token-d769b               kubernetes.io/service-account-token   3         4d        <none>
kube-system   kube-dns-token-66tfx                     kubernetes.io/service-account-token   3         4d        <none>
kube-system   kube-proxy-token-ktcv9                   kubernetes.io/service-account-token   3         4d        <none>
kube-system   kubernetes-dashboard-certs               Opaque                                1         4d        k8s-app=kubernetes-dashboard
kube-system   kubernetes-dashboard-key-holder          Opaque                                2         4d        <none>
kube-system   kubernetes-dashboard-token-bjcwc         kubernetes.io/service-account-token   3         4d        <none>
kube-system   namespace-controller-token-w7zqj         kubernetes.io/service-account-token   3         4d        <none>
kube-system   node-controller-token-mg5m9              kubernetes.io/service-account-token   3         4d        <none>
kube-system   persistent-volume-binder-token-jn99w     kubernetes.io/service-account-token   3         4d        <none>
kube-system   pod-garbage-collector-token-ktn79        kubernetes.io/service-account-token   3         4d        <none>
kube-system   replicaset-controller-token-qgr8m        kubernetes.io/service-account-token   3         4d        <none>
kube-system   replication-controller-token-gw4vz       kubernetes.io/service-account-token   3         4d        <none>
kube-system   resourcequota-controller-token-fpsgt     kubernetes.io/service-account-token   3         4d        <none>
kube-system   service-account-controller-token-xqln5   kubernetes.io/service-account-token   3         4d        <none>
kube-system   service-controller-token-p75wq           kubernetes.io/service-account-token   3         4d        <none>
kube-system   statefulset-controller-token-k7k92       kubernetes.io/service-account-token   3         4d        <none>
kube-system   tiller-token-chfrl                       kubernetes.io/service-account-token   3         4d        <none>
kube-system   token-cleaner-token-9zl6n                kubernetes.io/service-account-token   3         4d        <none>
kube-system   ttl-controller-token-xgk8l               kubernetes.io/service-account-token   3         4d        <none>

================
kubectl get serviceaccounts --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                         SECRETS   AGE       LABELS
default       default                      1         4d        <none>
default       kube-keepalived-vip          1         7h        <none>
default       my-nginx-ingress             1         50m       app=nginx-ingress,chart=nginx-ingress-0.9.2,heritage=Tiller,release=my-nginx-ingress
kube-public   default                      1         4d        <none>
kube-system   attachdetach-controller      1         4d        <none>
kube-system   bootstrap-signer             1         4d        <none>
kube-system   calico-node                  1         4d        <none>
kube-system   certificate-controller       1         4d        <none>
kube-system   cronjob-controller           1         4d        <none>
kube-system   daemon-set-controller        1         4d        <none>
kube-system   default                      1         4d        <none>
kube-system   deployment-controller        1         4d        <none>
kube-system   disruption-controller        1         4d        <none>
kube-system   elasticsearch-logging        1         4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true
kube-system   endpoint-controller          1         4d        <none>
kube-system   fluentd-es                   1         4d        addonmanager.kubernetes.io/mode=Reconcile,k8s-app=fluentd-es,kubernetes.io/cluster-service=true
kube-system   generic-garbage-collector    1         4d        <none>
kube-system   horizontal-pod-autoscaler    1         4d        <none>
kube-system   job-controller               1         4d        <none>
kube-system   kube-dns                     1         4d        <none>
kube-system   kube-proxy                   1         4d        <none>
kube-system   kubernetes-dashboard         1         4d        k8s-app=kubernetes-dashboard
kube-system   namespace-controller         1         4d        <none>
kube-system   node-controller              1         4d        <none>
kube-system   persistent-volume-binder     1         4d        <none>
kube-system   pod-garbage-collector        1         4d        <none>
kube-system   replicaset-controller        1         4d        <none>
kube-system   replication-controller       1         4d        <none>
kube-system   resourcequota-controller     1         4d        <none>
kube-system   service-account-controller   1         4d        <none>
kube-system   service-controller           1         4d        <none>
kube-system   statefulset-controller       1         4d        <none>
kube-system   tiller                       1         4d        <none>
kube-system   token-cleaner                1         4d        <none>
kube-system   ttl-controller               1         4d        <none>

================
kubectl get services --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE       SELECTOR                                                               LABELS
default       kubernetes                         ClusterIP   10.96.0.1       <none>         443/TCP          4d        <none>                                                                 component=apiserver,provider=kubernetes
default       my-nginx-ingress-controller        ClusterIP   10.107.38.119   10.10.97.200   80/TCP,443/TCP   50m       app=nginx-ingress,component=controller,release=my-nginx-ingress        app=nginx-ingress,chart=nginx-ingress-0.9.2,component=controller,heritage=Tiller,release=my-nginx-ingress
default       my-nginx-ingress-default-backend   ClusterIP   10.96.69.91     <none>         80/TCP           50m       app=nginx-ingress,component=default-backend,release=my-nginx-ingress   app=nginx-ingress,chart=nginx-ingress-0.9.2,component=default-backend,heritage=Tiller,release=my-nginx-ingress
default       tea-svc                            ClusterIP   10.96.196.71    <none>         80/TCP           1d        app=tea                                                                app=tea
kube-system   calico-typha                       ClusterIP   10.99.51.177    <none>         5473/TCP         4d        k8s-app=calico-typha                                                   k8s-app=calico-typha
kube-system   elasticsearch-logging              ClusterIP   10.105.23.60    <none>         9200/TCP         4d        k8s-app=elasticsearch-logging                                          addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Elasticsearch
kube-system   kibana-logging                     NodePort    10.96.208.116   <none>         5601:30601/TCP   4d        k8s-app=kibana-logging                                                 addonmanager.kubernetes.io/mode=Reconcile,k8s-app=kibana-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Kibana
kube-system   kube-dns                           ClusterIP   10.96.0.10      <none>         53/UDP,53/TCP    4d        k8s-app=kube-dns                                                       k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeDNS
kube-system   kubernetes-dashboard               NodePort    10.106.31.65    <none>         443:31443/TCP    4d        k8s-app=kubernetes-dashboard                                           k8s-app=kubernetes-dashboard
kube-system   tiller-deploy                      ClusterIP   10.104.87.82    <none>         44134/TCP        4d        app=helm,name=tiller                                                   app=helm,name=tiller

================
kubectl get statefulsets --all-namespaces --show-labels -o wide
================

NAMESPACE     NAME                    DESIRED   CURRENT   AGE       CONTAINERS              IMAGES                                                                      LABELS
kube-system   elasticsearch-logging   2         2         4d        elasticsearch-logging   registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4   addonmanager.kubernetes.io/mode=Reconcile,k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,version=v5.6.4

================
kubectl get storageclasses --all-namespaces --show-labels -o wide
================

No resources found.

================
kubectl get kubectl --all-namespaces --show-labels -o wide && echo ================n && kubectl get kubectl --all-namespaces --show-labels -o wide

================
kubectl describe all --all-namespaces
================

Name:           ccphxvolume
Selector:       name=ccphxvolume
Node-Selector:  <none>
Labels:         name=ccphxvolume
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"de...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=ccphxvolume
  Init Containers:
   iscsi-initiator:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    Environment:  <none>
    Mounts:       <none>
   hxvolume-copy:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    Environment:  <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
  Containers:
   hxvolume:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
   iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
Events:    <none>


Name:           kube-keepalived-vip
Selector:       name=kube-keepalived-vip
Node-Selector:  <none>
Labels:         name=kube-keepalived-vip
Annotations:    <none>
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 0
Pods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           name=kube-keepalived-vip
  Service Account:  kube-keepalived-vip
  Containers:
   kube-keepalived-vip:
    Image:  k8s.gcr.io/kube-keepalived-vip:0.11
    Port:   <none>
    Args:
      --services-configmap=default/vip-configmap
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
  Volumes:
   modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   dev:
    Type:  HostPath (bare host directory volume)
    Path:  /dev
Events:    <none>


Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  <none>
Labels:         k8s-app=calico-node
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-node:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Port:   <none>
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
   install-cni:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Port:   <none>
    Command:
      /install-cni.sh
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
  Volumes:
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
   cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
   cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
Events:    <none>


Name:           fluentd-es-v2.0.2
Selector:       k8s-app=fluentd-es,version=v2.0.2
Node-Selector:  <none>
Labels:         addonmanager.kubernetes.io/mode=Reconcile
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                version=v2.0.2
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd...
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 1
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=fluentd-es
                    kubernetes.io/cluster-service=true
                    version=v2.0.2
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  fluentd-es
  Containers:
   fluentd-es:
    Image:  registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Port:   <none>
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
  Volumes:
   varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
   varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
   libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
Events:        <none>


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  <none>
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:  gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Port:   <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
Events:    <none>


Name:                   my-nginx-ingress-controller
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=controller
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=controller,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-controller-d95d4979d (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1


Name:                   my-nginx-ingress-default-backend
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=default-backend
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=default-backend,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-default-backend-855d89f775 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1


Name:                   calico-typha
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:45 +0000
Labels:                 k8s-app=calico-typha
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"ku...
Selector:               k8s-app=calico-typha
Replicas:               0 desired | 0 updated | 0 total | 0 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=calico-typha
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   calico-typha-65b7467b56 (0/0 replicas created)
Events:          <none>


Name:                   kibana-logging
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:47 +0000
Labels:                 addonmanager.kubernetes.io/mode=Reconcile
                        k8s-app=kibana-logging
                        kubernetes.io/cluster-service=true
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana...
Selector:               k8s-app=kibana-logging
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  k8s-app=kibana-logging
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kibana-logging-767cf49759 (1/1 replicas created)
Events:          <none>


Name:                   kube-dns
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:44 +0000
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision=1
Selector:               k8s-app=kube-dns
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 10% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kube-dns-545bc4bfd4 (1/1 replicas created)
Events:          <none>


Name:                   kubernetes-dashboard
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:48 +0000
Labels:                 k8s-app=kubernetes-dashboard
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard"...
Selector:               k8s-app=kubernetes-dashboard
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubernetes-dashboard-7798c48646 (1/1 replicas created)
Events:          <none>


Name:                   tiller-deploy
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:49:06 +0000
Labels:                 app=helm
                        name=tiller
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=helm,name=tiller
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=helm
                    name=tiller
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   tiller-deploy-546cf9696c (1/1 replicas created)
Events:          <none>


Name:           my-nginx-ingress-controller-d95d4979d
Namespace:      default
Selector:       app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=controller
                pod-template-hash=851805358
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-controller
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    pod-template-hash=851805358
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  50m   replicaset-controller  Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7


Name:           my-nginx-ingress-default-backend-855d89f775
Namespace:      default
Selector:       app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=default-backend
                pod-template-hash=4118459331
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-default-backend
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           pod-template-hash=4118459331
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  50m   replicaset-controller  Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6


Name:           calico-typha-65b7467b56
Namespace:      kube-system
Selector:       k8s-app=calico-typha,pod-template-hash=2163023612
Labels:         k8s-app=calico-typha
                pod-template-hash=2163023612
Annotations:    deployment.kubernetes.io/desired-replicas=0
                deployment.kubernetes.io/max-replicas=0
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/calico-typha
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-typha
                    pod-template-hash=2163023612
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Events:                                 <none>


Name:           kibana-logging-767cf49759
Namespace:      kube-system
Selector:       k8s-app=kibana-logging,pod-template-hash=3237905315
Labels:         k8s-app=kibana-logging
                pod-template-hash=3237905315
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kibana-logging
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  k8s-app=kibana-logging
           pod-template-hash=3237905315
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Events:                          <none>


Name:           kube-dns-545bc4bfd4
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=1016706980
Labels:         k8s-app=kube-dns
                pod-template-hash=1016706980
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kube-dns
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=1016706980
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           kubernetes-dashboard-7798c48646
Namespace:      kube-system
Selector:       k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
Labels:         k8s-app=kubernetes-dashboard
                pod-template-hash=3354704202
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kubernetes-dashboard
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
                    pod-template-hash=3354704202
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Events:      <none>


Name:           tiller-deploy-546cf9696c
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=1027952527
Labels:         app=helm
                name=tiller
                pod-template-hash=1027952527
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/tiller-deploy
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=helm
                    name=tiller
                    pod-template-hash=1027952527
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>


Name:           tiller-deploy-5b9d65c7f
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=165821739
Labels:         app=helm
                name=tiller
                pod-template-hash=165821739
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/tiller-deploy
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=helm
           name=tiller
           pod-template-hash=165821739
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>


Name:                   my-nginx-ingress-controller
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=controller
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=controller,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-controller-d95d4979d (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1


Name:                   my-nginx-ingress-default-backend
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=default-backend
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=default-backend,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-default-backend-855d89f775 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1


Name:                   calico-typha
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:45 +0000
Labels:                 k8s-app=calico-typha
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"ku...
Selector:               k8s-app=calico-typha
Replicas:               0 desired | 0 updated | 0 total | 0 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=calico-typha
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   calico-typha-65b7467b56 (0/0 replicas created)
Events:          <none>


Name:                   kibana-logging
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:47 +0000
Labels:                 addonmanager.kubernetes.io/mode=Reconcile
                        k8s-app=kibana-logging
                        kubernetes.io/cluster-service=true
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana...
Selector:               k8s-app=kibana-logging
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  k8s-app=kibana-logging
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kibana-logging-767cf49759 (1/1 replicas created)
Events:          <none>


Name:                   kube-dns
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:44 +0000
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision=1
Selector:               k8s-app=kube-dns
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 10% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kube-dns-545bc4bfd4 (1/1 replicas created)
Events:          <none>


Name:                   kubernetes-dashboard
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:48 +0000
Labels:                 k8s-app=kubernetes-dashboard
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard"...
Selector:               k8s-app=kubernetes-dashboard
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubernetes-dashboard-7798c48646 (1/1 replicas created)
Events:          <none>


Name:                   tiller-deploy
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:49:06 +0000
Labels:                 app=helm
                        name=tiller
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=helm,name=tiller
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=helm
                    name=tiller
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   tiller-deploy-546cf9696c (1/1 replicas created)
Events:          <none>


Name:               elasticsearch-logging
Namespace:          kube-system
CreationTimestamp:  Fri, 09 Feb 2018 19:46:46 +0000
Selector:           k8s-app=elasticsearch-logging,version=v5.6.4
Labels:             addonmanager.kubernetes.io/mode=Reconcile
                    k8s-app=elasticsearch-logging
                    kubernetes.io/cluster-service=true
                    version=v5.6.4
Annotations:        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elast...
Replicas:           2 desired | 2 total
Pods Status:        2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=elasticsearch-logging
                    kubernetes.io/cluster-service=true
                    version=v5.6.4
  Service Account:  elasticsearch-logging
  Init Containers:
   elasticsearch-logging-init:
    Image:  registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Port:   <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    Environment:  <none>
    Mounts:       <none>
  Containers:
   elasticsearch-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Ports:  9200/TCP, 9300/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
  Volumes:
   elasticsearch-logging:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
Volume Claims:  <none>
Events:         <none>


Name:           ccphxvolume
Selector:       name=ccphxvolume
Node-Selector:  <none>
Labels:         name=ccphxvolume
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"de...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=ccphxvolume
  Init Containers:
   iscsi-initiator:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    Environment:  <none>
    Mounts:       <none>
   hxvolume-copy:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    Environment:  <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
  Containers:
   hxvolume:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
   iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
Events:    <none>


Name:           kube-keepalived-vip
Selector:       name=kube-keepalived-vip
Node-Selector:  <none>
Labels:         name=kube-keepalived-vip
Annotations:    <none>
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 0
Pods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           name=kube-keepalived-vip
  Service Account:  kube-keepalived-vip
  Containers:
   kube-keepalived-vip:
    Image:  k8s.gcr.io/kube-keepalived-vip:0.11
    Port:   <none>
    Args:
      --services-configmap=default/vip-configmap
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
  Volumes:
   modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   dev:
    Type:  HostPath (bare host directory volume)
    Path:  /dev
Events:    <none>


Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  <none>
Labels:         k8s-app=calico-node
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-node:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Port:   <none>
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
   install-cni:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Port:   <none>
    Command:
      /install-cni.sh
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
  Volumes:
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
   cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
   cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
Events:    <none>


Name:           fluentd-es-v2.0.2
Selector:       k8s-app=fluentd-es,version=v2.0.2
Node-Selector:  <none>
Labels:         addonmanager.kubernetes.io/mode=Reconcile
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                version=v2.0.2
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd...
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 1
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=fluentd-es
                    kubernetes.io/cluster-service=true
                    version=v2.0.2
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  fluentd-es
  Containers:
   fluentd-es:
    Image:  registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Port:   <none>
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
  Volumes:
   varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
   varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
   libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
Events:        <none>


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  <none>
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:  gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Port:   <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
Events:    <none>


Name:           my-nginx-ingress-controller-d95d4979d
Namespace:      default
Selector:       app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=controller
                pod-template-hash=851805358
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-controller
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    pod-template-hash=851805358
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  50m   replicaset-controller  Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7


Name:           my-nginx-ingress-default-backend-855d89f775
Namespace:      default
Selector:       app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=default-backend
                pod-template-hash=4118459331
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-default-backend
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           pod-template-hash=4118459331
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  50m   replicaset-controller  Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6


Name:           calico-typha-65b7467b56
Namespace:      kube-system
Selector:       k8s-app=calico-typha,pod-template-hash=2163023612
Labels:         k8s-app=calico-typha
                pod-template-hash=2163023612
Annotations:    deployment.kubernetes.io/desired-replicas=0
                deployment.kubernetes.io/max-replicas=0
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/calico-typha
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-typha
                    pod-template-hash=2163023612
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Events:                                 <none>


Name:           kibana-logging-767cf49759
Namespace:      kube-system
Selector:       k8s-app=kibana-logging,pod-template-hash=3237905315
Labels:         k8s-app=kibana-logging
                pod-template-hash=3237905315
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kibana-logging
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  k8s-app=kibana-logging
           pod-template-hash=3237905315
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Events:                          <none>


Name:           kube-dns-545bc4bfd4
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=1016706980
Labels:         k8s-app=kube-dns
                pod-template-hash=1016706980
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kube-dns
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=1016706980
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           kubernetes-dashboard-7798c48646
Namespace:      kube-system
Selector:       k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
Labels:         k8s-app=kubernetes-dashboard
                pod-template-hash=3354704202
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kubernetes-dashboard
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
                    pod-template-hash=3354704202
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Events:      <none>


Name:           tiller-deploy-546cf9696c
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=1027952527
Labels:         app=helm
                name=tiller
                pod-template-hash=1027952527
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/tiller-deploy
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=helm
                    name=tiller
                    pod-template-hash=1027952527
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>


Name:           tiller-deploy-5b9d65c7f
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=165821739
Labels:         app=helm
                name=tiller
                pod-template-hash=165821739
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/tiller-deploy
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=helm
           name=tiller
           pod-template-hash=165821739
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>


Name:           ccphxvolume-t972j
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.2.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.2.3
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
      Finished:     Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://133d74a2f67c41276c95642a7ee2cc640806f1e710e38d528b615833a7cc6832
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           ccphxvolume-tgb9r
Namespace:      default
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.1.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.1.2
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:37 +0000
      Finished:     Fri, 09 Feb 2018 19:47:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://9f4d367ff709fa795b1d3aa31b2a97474c57c8235735c1240f14aa1142059e4f
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           ccphxvolume-vlgmz
Namespace:      default
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:58 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.0.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.0.2
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://db0ba7b2436c0809e058838a7510f8150b381d0c473ce93a46fa90d77c77ed55
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-keepalived-vip-fph6b
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 19:01:12 +0000
Labels:         controller-revision-hash=617734581
                name=kube-keepalived-vip
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"kube-keepalived-vip","uid":"41d5383d-10f0-11e8-871b-005056af9e97"...
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/kube-keepalived-vip
Controlled By:  DaemonSet/kube-keepalived-vip
Containers:
  kube-keepalived-vip:
    Container ID:  docker://00aa555e894c867aef4c94268145997e0d4e0fb8e2bc01194d34c2ee495a4137
    Image:         k8s.gcr.io/kube-keepalived-vip:0.11
    Image ID:      docker-pullable://k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
    Port:          <none>
    Args:
      --services-configmap=default/vip-configmap
    State:          Running
      Started:      Tue, 13 Feb 2018 21:56:25 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 13 Feb 2018 21:55:47 +0000
      Finished:     Tue, 13 Feb 2018 21:55:53 +0000
    Ready:          True
    Restart Count:  3
    Environment:
      POD_NAME:       kube-keepalived-vip-fph6b (v1:metadata.name)
      POD_NAMESPACE:  default (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-keepalived-vip-token-2w47w (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  dev:
    Type:  HostPath (bare host directory volume)
    Path:  /dev
  kube-keepalived-vip-token-2w47w:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-keepalived-vip-token-2w47w
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type     Reason      Age                From                            Message
  ----     ------      ----               ----                            -------
  Warning  BackOff     49m (x3 over 50m)  kubelet, vhosakot1-wc80d3e5ab6  Back-off restarting failed container
  Warning  FailedSync  49m (x3 over 50m)  kubelet, vhosakot1-wc80d3e5ab6  Error syncing pod
  Normal   Pulling     49m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  pulling image "k8s.gcr.io/kube-keepalived-vip:0.11"
  Normal   Pulled      49m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Successfully pulled image "k8s.gcr.io/kube-keepalived-vip:0.11"
  Normal   Created     49m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal   Started     49m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           my-nginx-ingress-controller-d95d4979d-kzgk7
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 21:55:40 +0000
Labels:         app=nginx-ingress
                component=controller
                pod-template-hash=851805358
                release=my-nginx-ingress
Annotations:    checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
                cni.projectcalico.org/podIP=192.168.2.19/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-controller-d95d4979d","uid":"a14fe756-1108-11e8...
Status:         Running
IP:             192.168.2.19
Created By:     ReplicaSet/my-nginx-ingress-controller-d95d4979d
Controlled By:  ReplicaSet/my-nginx-ingress-controller-d95d4979d
Containers:
  nginx-ingress-controller:
    Container ID:  docker://b839ab72ad639b16185c8238762de9213cf04afa7c2aa93a80b6a23d87f36faa
    Image:         quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Image ID:      docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
    Ports:         80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    State:          Running
      Started:      Tue, 13 Feb 2018 21:55:41 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:       my-nginx-ingress-controller-d95d4979d-kzgk7 (v1:metadata.name)
      POD_NAMESPACE:  default (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from my-nginx-ingress-token-9ffd9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  my-nginx-ingress-token-9ffd9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  my-nginx-ingress-token-9ffd9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              50m   default-scheduler               Successfully assigned my-nginx-ingress-controller-d95d4979d-kzgk7 to vhosakot1-wc80d3e5ab6
  Normal  SuccessfulMountVolume  50m   kubelet, vhosakot1-wc80d3e5ab6  MountVolume.SetUp succeeded for volume "my-nginx-ingress-token-9ffd9"
  Normal  Pulled                 50m   kubelet, vhosakot1-wc80d3e5ab6  Container image "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2" already present on machine
  Normal  Created                50m   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal  Started                50m   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           my-nginx-ingress-default-backend-855d89f775-p8jk6
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 21:55:40 +0000
Labels:         app=nginx-ingress
                component=default-backend
                pod-template-hash=4118459331
                release=my-nginx-ingress
Annotations:    cni.projectcalico.org/podIP=192.168.2.18/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-default-backend-855d89f775","uid":"a151592a-110...
Status:         Running
IP:             192.168.2.18
Created By:     ReplicaSet/my-nginx-ingress-default-backend-855d89f775
Controlled By:  ReplicaSet/my-nginx-ingress-default-backend-855d89f775
Containers:
  nginx-ingress-default-backend:
    Container ID:   docker://59a4fe54449af40c3801c5057d82fe3e17c6e7ea7e4b5b2020e4bb82fe98ff5c
    Image:          k8s.gcr.io/defaultbackend:1.3
    Image ID:       docker-pullable://k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
    Port:           8080/TCP
    State:          Running
      Started:      Tue, 13 Feb 2018 21:55:41 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              50m   default-scheduler               Successfully assigned my-nginx-ingress-default-backend-855d89f775-p8jk6 to vhosakot1-wc80d3e5ab6
  Normal  SuccessfulMountVolume  50m   kubelet, vhosakot1-wc80d3e5ab6  MountVolume.SetUp succeeded for volume "default-token-fc6vg"
  Normal  Pulled                 50m   kubelet, vhosakot1-wc80d3e5ab6  Container image "k8s.gcr.io/defaultbackend:1.3" already present on machine
  Normal  Created                50m   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal  Started                50m   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           tea-rc-96442
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.2.15/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.2.15
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://1d0b5fdc1a237a0d73e3fb736d0730125d76e145a5926d2c77210aa174e73355
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:15 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tea-rc-fxj6x
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.2.14/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.2.14
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://353edef9a7f5eb68513f5a9b463dd7535ebc9abd1a7eec03941101351d84694b
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:14 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tea-rc-v8rxv
Namespace:      default
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.1.52/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.1.52
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://8f455e857f54947f253ab4a470b00b34f55086b42dd75e1a40ef0fc9d0a5ba11
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:14 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           calico-node-2cpcv
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://b9ba3c46ec16b030166cc9085a61ea34f8d39462ee880c5b088ab51332d36a53
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:24 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://bcd4205803fe0b6f9d0d06b6f7d43e23f0bfa48a56a24efe5a118873f4ed6e55
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-g99qc
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:57 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.20
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://80863955dfa346126c501bfde786831975a65e4188f4697348220b10f902b15f
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:58 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://3e78bce5869ac82e5283dd30c852640141f5914d3d2459534e1c94be1ed53470
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:59 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-kw9hm
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.46
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://153accc108eab50e06c45a0aae1788230477384976ef825a64b55f6114d85856
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://185c93be449c0f711ce22e774a8ec43d4fd993cbca35bd4445cb286d9941814f
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           elasticsearch-logging-0
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:28 +0000
Labels:         controller-revision-hash=elasticsearch-logging-7978c6964c
                k8s-app=elasticsearch-logging
                kubernetes.io/cluster-service=true
                version=v5.6.4
Annotations:    cni.projectcalico.org/podIP=192.168.1.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-00505...
Status:         Running
IP:             192.168.1.4
Created By:     StatefulSet/elasticsearch-logging
Controlled By:  StatefulSet/elasticsearch-logging
Init Containers:
  elasticsearch-logging-init:
    Container ID:  docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
    Image:         registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
    Port:          <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:43 +0000
      Finished:     Fri, 09 Feb 2018 19:47:43 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Containers:
  elasticsearch-logging:
    Container ID:   docker://26e38830a98fcd14580f1349b2e0643db39093b17e7f086e29cce5c07fe75d6a
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
    Ports:          9200/TCP, 9300/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  elasticsearch-logging:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  elasticsearch-logging-token-6cmks:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-logging-token-6cmks
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           elasticsearch-logging-1
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:44 +0000
Labels:         controller-revision-hash=elasticsearch-logging-7978c6964c
                k8s-app=elasticsearch-logging
                kubernetes.io/cluster-service=true
                version=v5.6.4
Annotations:    cni.projectcalico.org/podIP=192.168.2.5/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-00505...
Status:         Running
IP:             192.168.2.5
Created By:     StatefulSet/elasticsearch-logging
Controlled By:  StatefulSet/elasticsearch-logging
Init Containers:
  elasticsearch-logging-init:
    Container ID:  docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
    Image:         registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
    Port:          <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:46 +0000
      Finished:     Fri, 09 Feb 2018 19:47:46 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Containers:
  elasticsearch-logging:
    Container ID:   docker://598eb6f9ef563829dcef2355b141ea06eb162e11e8d429236c9426580c22620b
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
    Ports:          9200/TCP, 9300/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  elasticsearch-logging:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  elasticsearch-logging-token-6cmks:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-logging-token-6cmks
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         etcd-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=etcd
              tier=control-plane
Annotations:  kubernetes.io/config.hash=d76e26fba3bf2bfd215eb29011d55250
              kubernetes.io/config.mirror=d76e26fba3bf2bfd215eb29011d55250
              kubernetes.io/config.seen=2018-02-09T19:46:22.807466634Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  etcd:
    Container ID:  docker://11eb72aba33cbf28f02623a3e01c996da3542f375262a338c769dc91417a8865
    Image:         gcr.io/google_containers/etcd-amd64:3.0.17
    Image ID:      docker-pullable://gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
    Port:          <none>
    Command:
      etcd
      --listen-client-urls=http://127.0.0.1:2379
      --advertise-client-urls=http://127.0.0.1:2379
      --data-dir=/var/lib/etcd
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://127.0.0.1:2379/health delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:    <none>
    Mounts:
      /var/lib/etcd from etcd (rw)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  etcd:
    Type:        HostPath (bare host directory volume)
    Path:        /var/lib/etcd
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           fluentd-es-v2.0.2-9bmjw
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=1193446001
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                pod-template-generation=1
                version=v2.0.2
Annotations:    cni.projectcalico.org/podIP=192.168.2.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e9...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             192.168.2.2
Created By:     DaemonSet/fluentd-es-v2.0.2
Controlled By:  DaemonSet/fluentd-es-v2.0.2
Containers:
  fluentd-es:
    Container ID:   docker://5a66378b3e72ce2fd8df7b7181818302c86b7cb85ce53c1ac81410c171169e70
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from fluentd-es-token-zw92g (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
  varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
  libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
  fluentd-es-token-zw92g:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  fluentd-es-token-zw92g
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           fluentd-es-v2.0.2-kmbqt
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=1193446001
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                pod-template-generation=1
                version=v2.0.2
Annotations:    cni.projectcalico.org/podIP=192.168.1.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e9...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             192.168.1.3
Created By:     DaemonSet/fluentd-es-v2.0.2
Controlled By:  DaemonSet/fluentd-es-v2.0.2
Containers:
  fluentd-es:
    Container ID:   docker://c650008103084454c1d238468843c301b568d1919e3ae02c60338e1303189954
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from fluentd-es-token-zw92g (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
  varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
  libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
  fluentd-es-token-zw92g:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  fluentd-es-token-zw92g
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kibana-logging-767cf49759-f8zjt
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:28 +0000
Labels:         k8s-app=kibana-logging
                pod-template-hash=3237905315
Annotations:    cni.projectcalico.org/podIP=192.168.2.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kibana-logging-767cf49759","uid":"fcf34643-0dd1-11e8-871b-00...
Status:         Running
IP:             192.168.2.4
Created By:     ReplicaSet/kibana-logging-767cf49759
Controlled By:  ReplicaSet/kibana-logging-767cf49759
Containers:
  kibana-logging:
    Container ID:   docker://32e30c8b573b980690ec999dbb4f981dec9caadfb001356bd428b26187c1a451
    Image:          registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
    Port:           5601/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kxl84 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-kxl84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-kxl84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         kube-apiserver-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-apiserver
              tier=control-plane
Annotations:  kubernetes.io/config.hash=3f93d95099662b6d548520f6873d8454
              kubernetes.io/config.mirror=3f93d95099662b6d548520f6873d8454
              kubernetes.io/config.seen=2018-02-09T19:46:22.807479364Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-apiserver:
    Container ID:  docker://a04e800ebb80645cd9826489e7d56e778b23f495984e5d32822831c1178e0915
    Image:         gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
    Port:          <none>
    Command:
      kube-apiserver
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --allow-privileged=true
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --requestheader-username-headers=X-Remote-User
      --requestheader-group-headers=X-Remote-Group
      --service-cluster-ip-range=10.96.0.0/12
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --insecure-port=0
      --requestheader-allowed-names=front-proxy-client
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      --secure-port=6443
      --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --advertise-address=10.10.97.20
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --enable-bootstrap-token-auth=true
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --authorization-mode=Node,RBAC
      --etcd-servers=http://127.0.0.1:2379
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://127.0.0.1:6443/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  k8s-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/pki
  ca-certs:
    Type:        HostPath (bare host directory volume)
    Path:        /etc/ssl/certs
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:         kube-controller-manager-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-controller-manager
              tier=control-plane
Annotations:  kubernetes.io/config.hash=ab7c88cd37d9d92c34bb35c6a377a96f
              kubernetes.io/config.mirror=ab7c88cd37d9d92c34bb35c6a377a96f
              kubernetes.io/config.seen=2018-02-09T19:46:22.807482481Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-controller-manager:
    Container ID:  docker://952f4f63dea04d6c2c7ed47c7c6d4a720b7eea4f69f43d3daaf0113ab685163c
    Image:         gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
    Port:          <none>
    Command:
      kube-controller-manager
      --address=127.0.0.1
      --controllers=*,bootstrapsigner,tokencleaner
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --leader-elect=true
      --use-service-account-credentials=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --allocate-node-cidrs=true
      --cluster-cidr=192.168.0.0/16
      --node-cidr-mask-size=24
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get http://127.0.0.1:10252/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  k8s-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/pki
  ca-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/ssl/certs
  kubeconfig:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/controller-manager.conf
  flexvolume-dir:
    Type:        HostPath (bare host directory volume)
    Path:        /usr/libexec/kubernetes/kubelet-plugins/volume/exec
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           kube-dns-545bc4bfd4-rzpz4
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:47:13 +0000
Labels:         k8s-app=kube-dns
                pod-template-hash=1016706980
Annotations:    cni.projectcalico.org/podIP=192.168.0.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kube-dns-545bc4bfd4","uid":"fcf396ae-0dd1-11e8-871b-005056af...
Status:         Running
IP:             192.168.0.4
Created By:     ReplicaSet/kube-dns-545bc4bfd4
Controlled By:  ReplicaSet/kube-dns-545bc4bfd4
Containers:
  kubedns:
    Container ID:  docker://585a3048cfcdc2498ba543ef02124e15c7c7e8ea79f84f5df37d87e46b6cfb19
    Image:         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
    Ports:         10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
  dnsmasq:
    Container ID:  docker://ead0625a9610ac4f66bee25ee8ed4d6b5418bc31d7ac8b7e6eb3d38b339b3e7e
    Image:         gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
    Ports:         53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
  sidecar:
    Container ID:  docker://313b3d5c54c63059e63c3bc2b8d23d2902e8033a007ebd78f015ae22556f9603
    Image:         gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
    Port:          10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:46 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
  kube-dns-token-66tfx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-dns-token-66tfx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           kube-proxy-8vlsv
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:57 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.20
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://bf6367debcebe328805c54a362b8767b8e8810c327094c53b98ea7a59c4e5514
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:58 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-proxy-nfkkf
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.46
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://d47a6b44a292ed53cc15d6e72546c50f08a3037a306a847e63f39660af3b8a4a
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-proxy-q8ng8
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3ac1e13578a22c5fe5e2f3c8209af657906680224d8faf34cf95880e1a320243
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:         kube-scheduler-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-scheduler
              tier=control-plane
Annotations:  kubernetes.io/config.hash=e554495c6f8701f21accd04866090b05
              kubernetes.io/config.mirror=e554495c6f8701f21accd04866090b05
              kubernetes.io/config.seen=2018-02-09T19:46:22.807485209Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-scheduler:
    Container ID:  docker://dc6d33ae2ff92d6b18190f6edd6d943f352b37dabb80d6785d1e40238ec1530d
    Image:         gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
    Port:          <none>
    Command:
      kube-scheduler
      --address=127.0.0.1
      --leader-elect=true
      --kubeconfig=/etc/kubernetes/scheduler.conf
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get http://127.0.0.1:10251/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kubeconfig:
    Type:        HostPath (bare host directory volume)
    Path:        /etc/kubernetes/scheduler.conf
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           kubernetes-dashboard-7798c48646-rjmch
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:47:12 +0000
Labels:         k8s-app=kubernetes-dashboard
                pod-template-hash=3354704202
Annotations:    cni.projectcalico.org/podIP=192.168.0.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kubernetes-dashboard-7798c48646","uid":"fcf3720c-0dd1-11e8-8...
Status:         Running
IP:             192.168.0.3
Created By:     ReplicaSet/kubernetes-dashboard-7798c48646
Controlled By:  ReplicaSet/kubernetes-dashboard-7798c48646
Containers:
  kubernetes-dashboard:
    Container ID:  docker://15d41739dac144360b50543a9b2ed5269720107fc11c5e19c2c9a61716c4b626
    Image:         k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Image ID:      docker-pullable://k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
    Port:          8443/TCP
    Args:
      --auto-generate-certificates
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:42 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kubernetes-dashboard-token-bjcwc (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
  tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  kubernetes-dashboard-token-bjcwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-token-bjcwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tiller-deploy-546cf9696c-w4kq6
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:49:23 +0000
Labels:         app=helm
                name=tiller
                pod-template-hash=1027952527
Annotations:    cni.projectcalico.org/podIP=192.168.2.6/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"tiller-deploy-546cf9696c","uid":"54347a31-0dd2-11e8-871b-005...
Status:         Running
IP:             192.168.2.6
Created By:     ReplicaSet/tiller-deploy-546cf9696c
Controlled By:  ReplicaSet/tiller-deploy-546cf9696c
Containers:
  tiller:
    Container ID:   docker://9aa05a99a4fe19195842dbbe5f406912fdd380339d496b401a690d15d6033379
    Image:          gcr.io/kubernetes-helm/tiller:v2.7.2
    Image ID:       docker-pullable://gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
    Port:           44134/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:49:27 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from tiller-token-chfrl (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  tiller-token-chfrl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tiller-token-chfrl
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         tea-rc
Namespace:    default
Selector:     app=tea
Labels:       app=tea
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=tea
  Containers:
   tea:
    Image:        nginxdemos/hello
    Port:         80/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         10.10.97.20:6443
Session Affinity:  ClientIP
Events:            <none>


Name:              my-nginx-ingress-controller
Namespace:         default
Labels:            app=nginx-ingress
                   chart=nginx-ingress-0.9.2
                   component=controller
                   heritage=Tiller
                   release=my-nginx-ingress
Annotations:       <none>
Selector:          app=nginx-ingress,component=controller,release=my-nginx-ingress
Type:              ClusterIP
IP:                10.107.38.119
External IPs:      10.10.97.200
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         192.168.2.19:80
Port:              https  443/TCP
TargetPort:        443/TCP
Endpoints:         192.168.2.19:443
Session Affinity:  None
Events:            <none>


Name:              my-nginx-ingress-default-backend
Namespace:         default
Labels:            app=nginx-ingress
                   chart=nginx-ingress-0.9.2
                   component=default-backend
                   heritage=Tiller
                   release=my-nginx-ingress
Annotations:       <none>
Selector:          app=nginx-ingress,component=default-backend,release=my-nginx-ingress
Type:              ClusterIP
IP:                10.96.69.91
Port:              <unset>  80/TCP
TargetPort:        8080/TCP
Endpoints:         192.168.2.18:8080
Session Affinity:  None
Events:            <none>


Name:              tea-svc
Namespace:         default
Labels:            app=tea
Annotations:       <none>
Selector:          app=tea
Type:              ClusterIP
IP:                10.96.196.71
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         192.168.1.52:80,192.168.2.14:80,192.168.2.15:80
Session Affinity:  None
Events:            <none>


Name:              calico-typha
Namespace:         kube-system
Labels:            k8s-app=calico-typha
Annotations:       kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"...
Selector:          k8s-app=calico-typha
Type:              ClusterIP
IP:                10.99.51.177
Port:              calico-typha  5473/TCP
TargetPort:        %!d(string=calico-typha)/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:              elasticsearch-logging
Namespace:         kube-system
Labels:            addonmanager.kubernetes.io/mode=Reconcile
                   k8s-app=elasticsearch-logging
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=Elasticsearch
Annotations:       kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-loggi...
Selector:          k8s-app=elasticsearch-logging
Type:              ClusterIP
IP:                10.105.23.60
Port:              <unset>  9200/TCP
TargetPort:        %!d(string=db)/TCP
Endpoints:         192.168.1.4:9200,192.168.2.5:9200
Session Affinity:  None
Events:            <none>


Name:                     kibana-logging
Namespace:                kube-system
Labels:                   addonmanager.kubernetes.io/mode=Reconcile
                          k8s-app=kibana-logging
                          kubernetes.io/cluster-service=true
                          kubernetes.io/name=Kibana
Annotations:              kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","ku...
Selector:                 k8s-app=kibana-logging
Type:                     NodePort
IP:                       10.96.208.116
Port:                     <unset>  5601/TCP
TargetPort:               %!d(string=ui)/TCP
NodePort:                 <unset>  30601/TCP
Endpoints:                192.168.2.4:5601
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=KubeDNS
Annotations:       <none>
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP:                10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         192.168.0.4:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         192.168.0.4:53
Session Affinity:  None
Events:            <none>


Name:                     kubernetes-dashboard
Namespace:                kube-system
Labels:                   k8s-app=kubernetes-dashboard
Annotations:              kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":...
Selector:                 k8s-app=kubernetes-dashboard
Type:                     NodePort
IP:                       10.106.31.65
Port:                     <unset>  443/TCP
TargetPort:               8443/TCP
NodePort:                 <unset>  31443/TCP
Endpoints:                192.168.0.3:8443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


Name:              tiller-deploy
Namespace:         kube-system
Labels:            app=helm
                   name=tiller
Annotations:       <none>
Selector:          app=helm,name=tiller
Type:              ClusterIP
IP:                10.104.87.82
Port:              tiller  44134/TCP
TargetPort:        %!d(string=tiller)/TCP
Endpoints:         192.168.2.6:44134
Session Affinity:  None
Events:            <none>

================
kubectl describe certificatesigningrequests --all-namespaces
================

Name:               node-csr-8Aa5zv54iOqpJoBvflvn2nWYyB8c7zTe1Ptsk8YFepY
Labels:             <none>
Annotations:        <none>
CreationTimestamp:  Fri, 09 Feb 2018 19:47:18 +0000
Requesting User:    system:bootstrap:e06da1
Status:             Approved,Issued
Subject:
         Common Name:    system:node:vhosakot1-wc80d3e5ab6
         Serial Number:  
         Organization:   system:nodes
Events:  <none>


Name:               node-csr-rU0zAAXNDV-3D7DihpcOemzwANHaDyeaLGBYVz8lO1U
Labels:             <none>
Annotations:        <none>
CreationTimestamp:  Fri, 09 Feb 2018 19:47:18 +0000
Requesting User:    system:bootstrap:e06da1
Status:             Approved,Issued
Subject:
         Common Name:    system:node:vhosakot1-we2d86faeb2
         Serial Number:  
         Organization:   system:nodes
Events:  <none>

================
kubectl describe clusterrolebindings --all-namespaces
================

Name:         calico-node
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"name":"calico-node","namespace":""},"roleRe...
Role:
  Kind:  ClusterRole
  Name:  calico-node
Subjects:
  Kind            Name         Namespace
  ----            ----         ---------
  ServiceAccount  calico-node  kube-system


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name            Namespace
  ----   ----            ---------
  Group  system:masters  


Name:         elasticsearch-logging
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=elasticsearch-logging
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reco...
Role:
  Kind:  ClusterRole
  Name:  elasticsearch-logging
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  elasticsearch-logging  kube-system


Name:         fluentd-es
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=fluentd-es
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reco...
Role:
  Kind:  ClusterRole
  Name:  fluentd-es
Subjects:
  Kind            Name        Namespace
  ----            ----        ---------
  ServiceAccount  fluentd-es  kube-system


Name:         kube-keepalived-vip
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  kube-keepalived-vip
Subjects:
  Kind            Name                 Namespace
  ----            ----                 ---------
  ServiceAccount  kube-keepalived-vip  default


Name:         kubeadm:kubelet-bootstrap
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  system:node-bootstrapper
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token  


Name:         kubeadm:node-autoapprove-bootstrap
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  system:certificates.k8s.io:certificatesigningrequests:nodeclient
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token  


Name:         kubeadm:node-autoapprove-certificate-rotation
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
Subjects:
  Kind   Name          Namespace
  ----   ----          ---------
  Group  system:nodes  


Name:         kubeadm:node-proxier
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  system:node-proxier
Subjects:
  Kind            Name        Namespace
  ----            ----        ---------
  ServiceAccount  kube-proxy  kube-system


Name:         kubernetes-dashboard
Labels:       k8s-app=kubernetes-dashboard
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},...
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind            Name                  Namespace
  ----            ----                  ---------
  ServiceAccount  kubernetes-dashboard  kube-system


Name:         my-nginx-ingress
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  my-nginx-ingress
Subjects:
  Kind            Name              Namespace
  ----            ----              ---------
  ServiceAccount  my-nginx-ingress  default


Name:         system:basic-user
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:basic-user
Subjects:
  Kind   Name                    Namespace
  ----   ----                    ---------
  Group  system:authenticated    
  Group  system:unauthenticated  


Name:         system:controller:attachdetach-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:attachdetach-controller
Subjects:
  Kind            Name                     Namespace
  ----            ----                     ---------
  ServiceAccount  attachdetach-controller  kube-system


Name:         system:controller:certificate-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:certificate-controller
Subjects:
  Kind            Name                    Namespace
  ----            ----                    ---------
  ServiceAccount  certificate-controller  kube-system


Name:         system:controller:cronjob-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:cronjob-controller
Subjects:
  Kind            Name                Namespace
  ----            ----                ---------
  ServiceAccount  cronjob-controller  kube-system


Name:         system:controller:daemon-set-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:daemon-set-controller
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  daemon-set-controller  kube-system


Name:         system:controller:deployment-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:deployment-controller
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  deployment-controller  kube-system


Name:         system:controller:disruption-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:disruption-controller
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  disruption-controller  kube-system


Name:         system:controller:endpoint-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:endpoint-controller
Subjects:
  Kind            Name                 Namespace
  ----            ----                 ---------
  ServiceAccount  endpoint-controller  kube-system


Name:         system:controller:generic-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:generic-garbage-collector
Subjects:
  Kind            Name                       Namespace
  ----            ----                       ---------
  ServiceAccount  generic-garbage-collector  kube-system


Name:         system:controller:horizontal-pod-autoscaler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:horizontal-pod-autoscaler
Subjects:
  Kind            Name                       Namespace
  ----            ----                       ---------
  ServiceAccount  horizontal-pod-autoscaler  kube-system


Name:         system:controller:job-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:job-controller
Subjects:
  Kind            Name            Namespace
  ----            ----            ---------
  ServiceAccount  job-controller  kube-system


Name:         system:controller:namespace-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:namespace-controller
Subjects:
  Kind            Name                  Namespace
  ----            ----                  ---------
  ServiceAccount  namespace-controller  kube-system


Name:         system:controller:node-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:node-controller
Subjects:
  Kind            Name             Namespace
  ----            ----             ---------
  ServiceAccount  node-controller  kube-system


Name:         system:controller:persistent-volume-binder
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:persistent-volume-binder
Subjects:
  Kind            Name                      Namespace
  ----            ----                      ---------
  ServiceAccount  persistent-volume-binder  kube-system


Name:         system:controller:pod-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:pod-garbage-collector
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  pod-garbage-collector  kube-system


Name:         system:controller:replicaset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:replicaset-controller
Subjects:
  Kind            Name                   Namespace
  ----            ----                   ---------
  ServiceAccount  replicaset-controller  kube-system


Name:         system:controller:replication-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:replication-controller
Subjects:
  Kind            Name                    Namespace
  ----            ----                    ---------
  ServiceAccount  replication-controller  kube-system


Name:         system:controller:resourcequota-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:resourcequota-controller
Subjects:
  Kind            Name                      Namespace
  ----            ----                      ---------
  ServiceAccount  resourcequota-controller  kube-system


Name:         system:controller:route-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:route-controller
Subjects:
  Kind            Name              Namespace
  ----            ----              ---------
  ServiceAccount  route-controller  kube-system


Name:         system:controller:service-account-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:service-account-controller
Subjects:
  Kind            Name                        Namespace
  ----            ----                        ---------
  ServiceAccount  service-account-controller  kube-system


Name:         system:controller:service-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:service-controller
Subjects:
  Kind            Name                Namespace
  ----            ----                ---------
  ServiceAccount  service-controller  kube-system


Name:         system:controller:statefulset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:statefulset-controller
Subjects:
  Kind            Name                    Namespace
  ----            ----                    ---------
  ServiceAccount  statefulset-controller  kube-system


Name:         system:controller:ttl-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:controller:ttl-controller
Subjects:
  Kind            Name            Namespace
  ----            ----            ---------
  ServiceAccount  ttl-controller  kube-system


Name:         system:discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:discovery
Subjects:
  Kind   Name                    Namespace
  ----   ----                    ---------
  Group  system:authenticated    
  Group  system:unauthenticated  


Name:         system:kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:kube-controller-manager
Subjects:
  Kind  Name                            Namespace
  ----  ----                            ---------
  User  system:kube-controller-manager  


Name:         system:kube-dns
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:kube-dns
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  kube-dns  kube-system


Name:         system:kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:kube-scheduler
Subjects:
  Kind  Name                   Namespace
  ----  ----                   ---------
  User  system:kube-scheduler  


Name:         system:node
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:node
Subjects:
  Kind  Name  Namespace
  ----  ----  ---------


Name:         system:node-proxier
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:node-proxier
Subjects:
  Kind  Name               Namespace
  ----  ----               ---------
  User  system:kube-proxy  


Name:         tiller-cluster-rule
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind            Name    Namespace
  ----            ----    ---------
  ServiceAccount  tiller  kube-system

================
kubectl describe clusterroles --all-namespaces
================

Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  bindings                                        []                 []              [get list watch]
  configmaps                                      []                 []              [create delete deletecollection get list patch update watch]
  cronjobs.batch                                  []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.apps                                 []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.extensions                           []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps                                []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions                          []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/rollback                       []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/rollback                 []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/scale                          []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/scale                    []                 []              [create delete deletecollection get list patch update watch]
  endpoints                                       []                 []              [create delete deletecollection get list patch update watch]
  events                                          []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling            []                 []              [create delete deletecollection get list patch update watch]
  ingresses.extensions                            []                 []              [create delete deletecollection get list patch update watch]
  jobs.batch                                      []                 []              [create delete deletecollection get list patch update watch]
  limitranges                                     []                 []              [get list watch]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  namespaces                                      []                 []              [get list watch]
  namespaces/status                               []                 []              [get list watch]
  persistentvolumeclaims                          []                 []              [create delete deletecollection get list patch update watch]
  pods                                            []                 []              [create delete deletecollection get list patch update watch]
  pods/attach                                     []                 []              [create delete deletecollection get list patch update watch]
  pods/exec                                       []                 []              [create delete deletecollection get list patch update watch]
  pods/log                                        []                 []              [get list watch]
  pods/portforward                                []                 []              [create delete deletecollection get list patch update watch]
  pods/proxy                                      []                 []              [create delete deletecollection get list patch update watch]
  pods/status                                     []                 []              [get list watch]
  replicasets.apps                                []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions                          []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps/scale                          []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions/scale                    []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers                          []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/scale                    []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers.extensions/scale         []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/status                   []                 []              [get list watch]
  resourcequotas                                  []                 []              [get list watch]
  resourcequotas/status                           []                 []              [get list watch]
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  secrets                                         []                 []              [create delete deletecollection get list patch update watch]
  serviceaccounts                                 []                 []              [create delete deletecollection get list patch update watch impersonate]
  services                                        []                 []              [create delete deletecollection get list patch update watch]
  services/proxy                                  []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps                               []                 []              [create delete deletecollection get list patch update watch]


Name:         calico-node
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRole","metadata":{"annotations":{},"name":"calico-node","namespace":""},"rules":[{"api...
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bgpconfigurations.crd.projectcalico.org      []                 []              [create get list update watch]
  bgppeers.crd.projectcalico.org               []                 []              [create get list update watch]
  clusterinformations.crd.projectcalico.org    []                 []              [create get list update watch]
  endpoints                                    []                 []              [get]
  felixconfigurations.crd.projectcalico.org    []                 []              [create get list update watch]
  globalbgpconfigs.crd.projectcalico.org       []                 []              [create get list update watch]
  globalfelixconfigs.crd.projectcalico.org     []                 []              [create get list update watch]
  globalnetworkpolicies.crd.projectcalico.org  []                 []              [create get list update watch]
  ippools.crd.projectcalico.org                []                 []              [create get list update watch]
  namespaces                                   []                 []              [get list watch]
  networkpolicies.crd.projectcalico.org        []                 []              [create get list update watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  nodes                                        []                 []              [get list update watch]
  pods                                         []                 []              [get list watch patch]
  pods/status                                  []                 []              [update]
  services                                     []                 []              [get]


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [*]                []              [*]
  *.*        []                 []              [*]


Name:         edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  bindings                                 []                 []              [get list watch]
  configmaps                               []                 []              [create delete deletecollection get list patch update watch]
  cronjobs.batch                           []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.apps                          []                 []              [create delete deletecollection get list patch update watch]
  daemonsets.extensions                    []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps                         []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions                   []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/rollback                []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/rollback          []                 []              [create delete deletecollection get list patch update watch]
  deployments.apps/scale                   []                 []              [create delete deletecollection get list patch update watch]
  deployments.extensions/scale             []                 []              [create delete deletecollection get list patch update watch]
  endpoints                                []                 []              [create delete deletecollection get list patch update watch]
  events                                   []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling     []                 []              [create delete deletecollection get list patch update watch]
  ingresses.extensions                     []                 []              [create delete deletecollection get list patch update watch]
  jobs.batch                               []                 []              [create delete deletecollection get list patch update watch]
  limitranges                              []                 []              [get list watch]
  namespaces                               []                 []              [get list watch]
  namespaces/status                        []                 []              [get list watch]
  persistentvolumeclaims                   []                 []              [create delete deletecollection get list patch update watch]
  pods                                     []                 []              [create delete deletecollection get list patch update watch]
  pods/attach                              []                 []              [create delete deletecollection get list patch update watch]
  pods/exec                                []                 []              [create delete deletecollection get list patch update watch]
  pods/log                                 []                 []              [get list watch]
  pods/portforward                         []                 []              [create delete deletecollection get list patch update watch]
  pods/proxy                               []                 []              [create delete deletecollection get list patch update watch]
  pods/status                              []                 []              [get list watch]
  replicasets.apps                         []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions                   []                 []              [create delete deletecollection get list patch update watch]
  replicasets.apps/scale                   []                 []              [create delete deletecollection get list patch update watch]
  replicasets.extensions/scale             []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers                   []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/scale             []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers.extensions/scale  []                 []              [create delete deletecollection get list patch update watch]
  replicationcontrollers/status            []                 []              [get list watch]
  resourcequotas                           []                 []              [get list watch]
  resourcequotas/status                    []                 []              [get list watch]
  secrets                                  []                 []              [create delete deletecollection get list patch update watch]
  serviceaccounts                          []                 []              [create delete deletecollection get list patch update watch impersonate]
  services                                 []                 []              [create delete deletecollection get list patch update watch]
  services/proxy                           []                 []              [create delete deletecollection get list patch update watch]
  statefulsets.apps                        []                 []              [create delete deletecollection get list patch update watch]


Name:         elasticsearch-logging
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=elasticsearch-logging
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile",...
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  endpoints   []                 []              [get]
  namespaces  []                 []              [get]
  services    []                 []              [get]


Name:         fluentd-es
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=fluentd-es
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile",...
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  namespaces  []                 []              [get watch list]
  pods        []                 []              [get watch list]


Name:         kube-keepalived-vip
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"name":"kube-keepalived-vip","namespace":""},"rules":[{"...
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 []              [get list watch]
  endpoints   []                 []              [get list watch]
  nodes       []                 []              [get list watch]
  pods        []                 []              [get list watch]
  services    []                 []              [get list watch]


Name:         my-nginx-ingress
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names  Verbs
  ---------                    -----------------  --------------  -----
  configmaps                   []                 []              [list watch]
  endpoints                    []                 []              [list watch]
  events                       []                 []              [create patch]
  ingresses.extensions         []                 []              [get list watch]
  ingresses.extensions/status  []                 []              [update]
  nodes                        []                 []              [list watch get]
  pods                         []                 []              [list watch]
  secrets                      []                 []              [list watch]
  services                     []                 []              [get list update watch]


Name:         system:auth-delegator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                  -----------------  --------------  -----
  subjectaccessreviews.authorization.k8s.io  []                 []              [create]
  tokenreviews.authentication.k8s.io         []                 []              [create]


Name:         system:basic-user
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                      -----------------  --------------  -----
  selfsubjectaccessreviews.authorization.k8s.io  []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:nodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                                  -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/nodeclient  []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                                      -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/selfnodeclient  []                 []              [create]


Name:         system:controller:attachdetach-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [create patch update]
  nodes                   []                 []              [get list watch]
  nodes/status            []                 []              [patch update]
  persistentvolumeclaims  []                 []              [list watch]
  persistentvolumes       []                 []              [list watch]
  pods                    []                 []              [list watch]


Name:         system:controller:certificate-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                                Non-Resource URLs  Resource Names  Verbs
  ---------                                                -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io           []                 []              [get list watch]
  certificatesigningrequests.certificates.k8s.io/approval  []                 []              [update]
  certificatesigningrequests.certificates.k8s.io/status    []                 []              [update]
  events                                                   []                 []              [create patch update]
  subjectaccessreviews.authorization.k8s.io                []                 []              [create]


Name:         system:controller:cronjob-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                  Non-Resource URLs  Resource Names  Verbs
  ---------                  -----------------  --------------  -----
  cronjobs.batch             []                 []              [get list update watch]
  cronjobs.batch/finalizers  []                 []              [update]
  cronjobs.batch/status      []                 []              [update]
  events                     []                 []              [create patch update]
  jobs.batch                 []                 []              [create delete get list patch update watch]
  pods                       []                 []              [delete list]


Name:         system:controller:daemon-set-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                         Non-Resource URLs  Resource Names  Verbs
  ---------                         -----------------  --------------  -----
  controllerrevisions.apps          []                 []              [create delete list patch update watch]
  daemonsets.apps                   []                 []              [get list watch]
  daemonsets.extensions             []                 []              [get list watch]
  daemonsets.apps/finalizers        []                 []              [update]
  daemonsets.extensions/finalizers  []                 []              [update]
  daemonsets.apps/status            []                 []              [update]
  daemonsets.extensions/status      []                 []              [update]
  events                            []                 []              [create patch update]
  nodes                             []                 []              [list watch]
  pods                              []                 []              [create delete list patch watch]
  pods/binding                      []                 []              [create]


Name:         system:controller:deployment-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  deployments.apps                   []                 []              [get list update watch]
  deployments.extensions             []                 []              [get list update watch]
  deployments.apps/finalizers        []                 []              [update]
  deployments.extensions/finalizers  []                 []              [update]
  deployments.apps/status            []                 []              [update]
  deployments.extensions/status      []                 []              [update]
  events                             []                 []              [create patch update]
  pods                               []                 []              [get list update watch]
  replicasets.apps                   []                 []              [create delete get list patch update watch]
  replicasets.extensions             []                 []              [create delete get list patch update watch]


Name:         system:controller:disruption-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                           Non-Resource URLs  Resource Names  Verbs
  ---------                           -----------------  --------------  -----
  deployments.apps                    []                 []              [get list watch]
  deployments.extensions              []                 []              [get list watch]
  events                              []                 []              [create patch update]
  poddisruptionbudgets.policy         []                 []              [get list watch]
  poddisruptionbudgets.policy/status  []                 []              [update]
  replicasets.apps                    []                 []              [get list watch]
  replicasets.extensions              []                 []              [get list watch]
  replicationcontrollers              []                 []              [get list watch]
  statefulsets.apps                   []                 []              [get list watch]


Name:         system:controller:endpoint-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  endpoints             []                 []              [create delete get list update]
  endpoints/restricted  []                 []              [create]
  events                []                 []              [create patch update]
  pods                  []                 []              [get list watch]
  services              []                 []              [get list watch]


Name:         system:controller:generic-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [delete get list patch update watch]
  events     []                 []              [create patch update]


Name:         system:controller:horizontal-pod-autoscaler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names     Verbs
  ---------                                    -----------------  --------------     -----
  *.custom.metrics.k8s.io                      []                 []                 [get list]
  deployments.apps/scale                       []                 []                 [get update]
  deployments.extensions/scale                 []                 []                 [get update]
  events                                       []                 []                 [create patch update]
  horizontalpodautoscalers.autoscaling         []                 []                 [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []                 [update]
  pods                                         []                 []                 [list]
  pods.metrics.k8s.io                          []                 []                 [list]
  replicasets.apps/scale                       []                 []                 [get update]
  replicasets.extensions/scale                 []                 []                 [get update]
  replicationcontrollers/scale                 []                 []                 [get update]
  replicationcontrollers.extensions/scale      []                 []                 [get update]
  services/proxy                               []                 [http:heapster:]   [get]
  services/proxy                               []                 [https:heapster:]  [get]


Name:         system:controller:job-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  events                 []                 []              [create patch update]
  jobs.batch             []                 []              [get list update watch]
  jobs.batch/finalizers  []                 []              [update]
  jobs.batch/status      []                 []              [update]
  pods                   []                 []              [create delete list patch watch]


Name:         system:controller:namespace-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources            Non-Resource URLs  Resource Names  Verbs
  ---------            -----------------  --------------  -----
  *.*                  []                 []              [delete deletecollection get list]
  namespaces           []                 []              [delete get list watch]
  namespaces/finalize  []                 []              [update]
  namespaces/status    []                 []              [update]


Name:         system:controller:node-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources     Non-Resource URLs  Resource Names  Verbs
  ---------     -----------------  --------------  -----
  events        []                 []              [create patch update]
  nodes         []                 []              [delete get list patch update]
  nodes/status  []                 []              [patch update]
  pods          []                 []              [delete list]
  pods/status   []                 []              [update]


Name:         system:controller:persistent-volume-binder
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  endpoints                      []                 []              [create delete get]
  events                         []                 []              [watch create patch update]
  nodes                          []                 []              [get list]
  persistentvolumeclaims         []                 []              [get list update watch]
  persistentvolumeclaims/status  []                 []              [update]
  persistentvolumes              []                 []              [create delete get list update watch]
  persistentvolumes/status       []                 []              [update]
  pods                           []                 []              [create delete get list watch]
  secrets                        []                 []              [get]
  services                       []                 []              [create delete get]
  storageclasses.storage.k8s.io  []                 []              [get list watch]


Name:         system:controller:pod-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  nodes      []                 []              [list]
  pods       []                 []              [delete list watch]


Name:         system:controller:replicaset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  events                             []                 []              [create patch update]
  pods                               []                 []              [create delete list patch watch]
  replicasets.apps                   []                 []              [get list update watch]
  replicasets.extensions             []                 []              [get list update watch]
  replicasets.apps/finalizers        []                 []              [update]
  replicasets.extensions/finalizers  []                 []              [update]
  replicasets.apps/status            []                 []              [update]
  replicasets.extensions/status      []                 []              [update]


Name:         system:controller:replication-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  events                             []                 []              [create patch update]
  pods                               []                 []              [create delete list patch watch]
  replicationcontrollers             []                 []              [get list update watch]
  replicationcontrollers/finalizers  []                 []              [update]
  replicationcontrollers/status      []                 []              [update]


Name:         system:controller:resourcequota-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  *.*                    []                 []              [list watch]
  events                 []                 []              [create patch update]
  resourcequotas/status  []                 []              [update]


Name:         system:controller:route-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources     Non-Resource URLs  Resource Names  Verbs
  ---------     -----------------  --------------  -----
  events        []                 []              [create patch update]
  nodes         []                 []              [list watch]
  nodes/status  []                 []              [patch]


Name:         system:controller:service-account-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources        Non-Resource URLs  Resource Names  Verbs
  ---------        -----------------  --------------  -----
  events           []                 []              [create patch update]
  serviceaccounts  []                 []              [create]


Name:         system:controller:service-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources        Non-Resource URLs  Resource Names  Verbs
  ---------        -----------------  --------------  -----
  events           []                 []              [create patch update]
  nodes            []                 []              [list watch]
  services         []                 []              [get list watch]
  services/status  []                 []              [update]


Name:         system:controller:statefulset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                     Non-Resource URLs  Resource Names  Verbs
  ---------                     -----------------  --------------  -----
  controllerrevisions.apps      []                 []              [create delete get list patch update watch]
  events                        []                 []              [create patch update]
  persistentvolumeclaims        []                 []              [create get]
  pods                          []                 []              [list watch create delete get patch update]
  statefulsets.apps             []                 []              [get list watch]
  statefulsets.apps/finalizers  []                 []              [update]
  statefulsets.apps/status      []                 []              [update]


Name:         system:controller:ttl-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  events     []                 []              [create patch update]
  nodes      []                 []              [list patch update watch]


Name:         system:discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs       Resource Names  Verbs
  ---------  -----------------       --------------  -----
             [/api]                  []              [get]
             [/api/*]                []              [get]
             [/apis]                 []              [get]
             [/apis/*]               []              [get]
             [/healthz]              []              [get]
             [/swagger-2.0.0.pb-v1]  []              [get]
             [/swagger.json]         []              [get]
             [/swaggerapi]           []              [get]
             [/swaggerapi/*]         []              [get]
             [/version]              []              [get]


Name:         system:heapster
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  deployments.extensions  []                 []              [get list watch]
  events                  []                 []              [get list watch]
  namespaces              []                 []              [get list watch]
  nodes                   []                 []              [get list watch]
  pods                    []                 []              [get list watch]


Name:         system:kube-aggregator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [get list watch]
  services   []                 []              [get list watch]


Name:         system:kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                           Non-Resource URLs  Resource Names  Verbs
  ---------                           -----------------  --------------  -----
  *.*                                 []                 []              [list watch]
  endpoints                           []                 []              [create get update]
  events                              []                 []              [create patch update]
  namespaces                          []                 []              [get]
  secrets                             []                 []              [create delete get update]
  serviceaccounts                     []                 []              [create get update]
  tokenreviews.authentication.k8s.io  []                 []              [create]


Name:         system:kube-dns
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [list watch]
  services   []                 []              [list watch]


Name:         system:kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names    Verbs
  ---------               -----------------  --------------    -----
  bindings                []                 []                [create]
  endpoints               []                 []                [create]
  endpoints               []                 [kube-scheduler]  [delete get patch update]
  events                  []                 []                [create patch update]
  nodes                   []                 []                [get list watch]
  persistentvolumeclaims  []                 []                [get list watch]
  persistentvolumes       []                 []                [get list watch]
  pods                    []                 []                [delete get list watch]
  pods/binding            []                 []                [create]
  pods/status             []                 []                [update]
  replicasets.apps        []                 []                [get list watch]
  replicasets.extensions  []                 []                [get list watch]
  replicationcontrollers  []                 []                [get list watch]
  services                []                 []                [get list watch]
  statefulsets.apps       []                 []                [get list watch]


Name:         system:node
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]
  configmaps                                      []                 []              [get]
  endpoints                                       []                 []              [get]
  events                                          []                 []              [create patch update]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  nodes                                           []                 []              [create get list watch delete patch update]
  nodes/status                                    []                 []              [patch update]
  persistentvolumeclaims                          []                 []              [get]
  persistentvolumes                               []                 []              [get]
  pods                                            []                 []              [get list watch create delete]
  pods/eviction                                   []                 []              [create]
  pods/status                                     []                 []              [update]
  secrets                                         []                 []              [get]
  services                                        []                 []              [get list watch]
  subjectaccessreviews.authorization.k8s.io       []                 []              [create]
  tokenreviews.authentication.k8s.io              []                 []              [create]


Name:         system:node-bootstrapper
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]


Name:         system:node-problem-detector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources     Non-Resource URLs  Resource Names  Verbs
  ---------     -----------------  --------------  -----
  events        []                 []              [create patch update]
  nodes         []                 []              [get]
  nodes/status  []                 []              [patch]


Name:         system:node-proxier
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [list watch]
  events     []                 []              [create patch update]
  nodes      []                 []              [get]
  services   []                 []              [list watch]


Name:         system:persistent-volume-provisioner
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  events                         []                 []              [watch create patch update]
  persistentvolumeclaims         []                 []              [get list update watch]
  persistentvolumes              []                 []              [create delete get list watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]


Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  bindings                                 []                 []              [get list watch]
  configmaps                               []                 []              [get list watch]
  cronjobs.batch                           []                 []              [get list watch]
  daemonsets.apps                          []                 []              [get list watch]
  daemonsets.extensions                    []                 []              [get list watch]
  deployments.apps                         []                 []              [get list watch]
  deployments.extensions                   []                 []              [get list watch]
  deployments.apps/scale                   []                 []              [get list watch]
  deployments.extensions/scale             []                 []              [get list watch]
  endpoints                                []                 []              [get list watch]
  events                                   []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling     []                 []              [get list watch]
  ingresses.extensions                     []                 []              [get list watch]
  jobs.batch                               []                 []              [get list watch]
  limitranges                              []                 []              [get list watch]
  namespaces                               []                 []              [get list watch]
  namespaces/status                        []                 []              [get list watch]
  persistentvolumeclaims                   []                 []              [get list watch]
  pods                                     []                 []              [get list watch]
  pods/log                                 []                 []              [get list watch]
  pods/status                              []                 []              [get list watch]
  replicasets.apps                         []                 []              [get list watch]
  replicasets.extensions                   []                 []              [get list watch]
  replicasets.apps/scale                   []                 []              [get list watch]
  replicasets.extensions/scale             []                 []              [get list watch]
  replicationcontrollers                   []                 []              [get list watch]
  replicationcontrollers/scale             []                 []              [get list watch]
  replicationcontrollers.extensions/scale  []                 []              [get list watch]
  replicationcontrollers/status            []                 []              [get list watch]
  resourcequotas                           []                 []              [get list watch]
  resourcequotas/status                    []                 []              [get list watch]
  serviceaccounts                          []                 []              [get list watch]
  services                                 []                 []              [get list watch]
  statefulsets.apps                        []                 []              [get list watch]

================
kubectl describe clusters --all-namespaces
================

the server doesn't have a resource type "clusters"

================
kubectl describe componentstatuses --all-namespaces
================

Name:         scheduler
Namespace:    
Labels:       <none>
Annotations:  <none>
API Version:  v1
Conditions:
  Message:  ok
  Status:   True
  Type:     Healthy
Kind:       ComponentStatus
Metadata:
  Creation Timestamp:  <nil>
  Self Link:           /api/v1/componentstatuses/scheduler
Events:                <none>


Name:         controller-manager
Namespace:    
Labels:       <none>
Annotations:  <none>
API Version:  v1
Conditions:
  Message:  ok
  Status:   True
  Type:     Healthy
Kind:       ComponentStatus
Metadata:
  Creation Timestamp:  <nil>
  Self Link:           /api/v1/componentstatuses/controller-manager
Events:                <none>


Name:         etcd-0
Namespace:    
Labels:       <none>
Annotations:  <none>
API Version:  v1
Conditions:
  Message:  {"health": "true"}
  Status:   True
  Type:     Healthy
Kind:       ComponentStatus
Metadata:
  Creation Timestamp:  <nil>
  Self Link:           /api/v1/componentstatuses/etcd-0
Events:                <none>

================
kubectl describe configmaps --all-namespaces
================

Name:         ingress-controller-leader-nginx
Namespace:    default
Labels:       <none>
Annotations:  control-plane.alpha.kubernetes.io/leader={"holderIdentity":"my-nginx-ingress-controller-d95d4979d-kzgk7","leaseDurationSeconds":30,"acquireTime":"2018-02-13T21:56:23Z","renewTime":"2018-02-13T22:46:25...

Data
====
Events:  <none>


Name:         my-nginx-ingress-controller
Namespace:    default
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              component=controller
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>

Data
====
enable-vts-status:
----
false
Events:  <none>


Name:         vip-configmap
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
10.10.97.200:
----
default/my-nginx-ingress-controller
Events:  <none>


Name:         cluster-info
Namespace:    kube-public
Labels:       <none>
Annotations:  <none>

Data
====
kubeconfig:
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://10.10.97.20:6443
  name: ""
contexts: []
current-context: ""
kind: Config
preferences: {}
users: []

Events:  <none>


Name:         calico-config
Namespace:    kube-system
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"cni_network_config":"{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.3.0\",\n  \"plugins\": [\n    {\n      \"ty...

Data
====
cni_network_config:
----
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.0",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "datastore_type": "kubernetes",
      "nodename": "__KUBERNETES_NODE_NAME__",
      "mtu": 1500,
      "ipam": {
        "type": "host-local",
        "subnet": "usePodCidr"
      },
      "policy": {
        "type": "k8s",
        "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
      },
      "kubernetes": {
        "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
        "kubeconfig": "__KUBECONFIG_FILEPATH__"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    }
  ]
}
typha_service_name:
----
none
Events:  <none>


Name:         extension-apiserver-authentication
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
requestheader-extra-headers-prefix:
----
["X-Remote-Extra-"]
requestheader-group-headers:
----
["X-Remote-Group"]
requestheader-username-headers:
----
["X-Remote-User"]
client-ca-file:
----
-----BEGIN CERTIFICATE-----
MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
cm5ldGVzMB4XDTE4MDIwOTE5NDYxNFoXDTI4MDIwNzE5NDYxNFowFTETMBEGA1UE
AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALqu
arSh9xo+PRq4bT+a/D/2eFxNxetj7jcjcwB1o8MLzPCvpu94voQLXag44mJuuDEG
TpEde2SJ+1PYv/XwPsqafpxNSvf92U9UVBAUXqhqBesfS+SKy+laauYu4Ein/U4I
1C2Ndoav2uEkkasaOAX7mrSCo09pysSKR+ybEdn8q0wWLZlOg/Nx9YcaNn9Tvsiy
p6zPA9oDdYQ7cruEApwao+NC8+4ANv/ZlUlhcBFtohuIi2EbDGIFGR1a1ca0PS9c
X/m3z6LDtVZZY1W44Djk+1W34v9Hsz3f2e70Ct7lxtgjdkay89Reez2IU8p1XjrO
jWWNkO54CBNeT++zGO0CAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKzzPmtWofZezXbhiVd0vDFBHaLb
RIwBeI7Yi1C83ZeD/GZ5tBFGE6Xc9deaqqRIFYRf7Y9migo8veBD4tDn4MGFqPd9
wRZMFXslzzRX7aiR8hPAxIkEzwAaFoYRms67aTnqvDKb/fnL/LCZQ76EB+AiFiRs
415kaJss/11Yb1zRItZY0dHxBu+mOqv0uvZ1yC7nARp3QbHxVsz/xH0kFia12BUg
eYydStwfEzplXG0H9zOp5PkDn70I714ie/RpCr2AEIelmBYyf1sm2+SxDtuV/skm
4UHJGW6bTRes/jEBWSUnXvRW4ImH97sqR6MNYXoiWhchMzrtmp3G1tcfCRQ=
-----END CERTIFICATE-----

requestheader-allowed-names:
----
["front-proxy-client"]
requestheader-client-ca-file:
----
-----BEGIN CERTIFICATE-----
MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
cm5ldGVzMB4XDTE4MDIwOTE5NDYxNVoXDTI4MDIwNzE5NDYxNVowFTETMBEGA1UE
AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKVr
QPZjj+JvGnpt5fEBZ3vn3Mj/TKC5eYQnAKT6Up+KiqB6CLsubnmqQX1LGCOR0uZZ
4jekqxDT2In7gk8+gb5/EchQ+za1tRIYZlkkqDIrE257FvMU+8AKU9f9ynfT9J1F
FXOIwzMZW7seHDh6p98W7P3e/mlJjywSwJTNVb/iYB+voYzFIq/y+3BVG1TzKvlM
Qc/bq9/D6UUS+YNgbDgwSNNTD3p/V6J0ATJAEYfHrzyP3QBOlbHA1cyLGDCsqgay
A2dyasXNQRA39eyixBLgHEPJYNbmBH1kQWrfCFQk0K5phLpM6Ow0/XXkLROq8sYO
G6yFErv9fs/mQ8Geq+cCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAIvY+VVHOKBo9KHV8iIug1jq31RZ
qeYNJNZ5g80koGxJ0g46rXhFlJ8CXz7PAGDaKHFCjuzbeOmVQ5q5PDAg15BfizvR
Rx/q/ugJJRitxZR0zIuCSNQ29EwokyBE7k5j54wYIX61aY/yHe4GEfv9DTZIztP6
E6z2gW7RfZbcKV+komATaN8bQCM9W3T/FvYTei1tZ89gzbJbkVk/Y7AxyDQ9Ym2O
8H/paYAiMy0npNqviC6eyMEcM9m+G1Sxye4cOkFLkkjlxWFSuXuUqSnUbqTGlhSt
C1DtrSg1rnzGqnsp7bdRmgevNluy2h7STk0fO1zB42MtQq5JNI/1TyeQeGM=
-----END CERTIFICATE-----

Events:  <none>


Name:         fluentd-es-config-v0.1.1
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"containers.input.conf":"# This configuration file for Fluentd / td-agent is used\n# to watch changes to Docker log files. T...

Data
====
monitoring.conf:
----
# Prometheus Exporter Plugin
# input plugin that exports metrics
<source>
  @type prometheus
</source>

<source>
  @type monitor_agent
</source>

# input plugin that collects metrics from MonitorAgent
<source>
  @type prometheus_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

# input plugin that collects metrics for output plugin
<source>
  @type prometheus_output_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

# input plugin that collects metrics for in_tail plugin
<source>
  @type prometheus_tail_monitor
  <labels>
    host ${hostname}
  </labels>
</source>
output.conf:
----
# Enriches records with Kubernetes metadata
<filter kubernetes.**>
  type kubernetes_metadata
</filter>

<match **>
   type elasticsearch
   log_level info
   include_tag_key true
   host elasticsearch-logging
   port 9200
   logstash_format true
   # Set the chunk limits.
   buffer_chunk_limit 2M
   buffer_queue_limit 8
   flush_interval 5s
   # Never wait longer than 5 minutes between retries.
   max_retry_wait 30
   # Disable the limit on the number of retries (retry forever).
   disable_retry_limit
   # Use multiple threads for processing.
   num_threads 2
</match>
system.input.conf:
----
# Example:
# 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
<source>
  type tail
  format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
  time_format %Y-%m-%d %H:%M:%S
  path /var/log/salt/minion
  pos_file /var/log/es-salt.pos
  tag salt
</source>

# Example:
# Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
<source>
  type tail
  format syslog
  path /var/log/startupscript.log
  pos_file /var/log/es-startupscript.log.pos
  tag startupscript
</source>

# Examples:
# time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
# time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
<source>
  type tail
  format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
  path /var/log/docker.log
  pos_file /var/log/es-docker.log.pos
  tag docker
</source>

# Example:
# 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
<source>
  type tail
  # Not parsing this, because it doesn't have anything particularly useful to
  # parse out of it (like severities).
  format none
  path /var/log/etcd.log
  pos_file /var/log/es-etcd.log.pos
  tag etcd
</source>

# Multi-line parsing is required for all the kube logs because very large log
# statements, such as those that include entire object bodies, get split into
# multiple lines by glog.

# Example:
# I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kubelet.log
  pos_file /var/log/es-kubelet.log.pos
  tag kubelet
</source>

# Example:
# I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-proxy.log
  pos_file /var/log/es-kube-proxy.log.pos
  tag kube-proxy
</source>

# Example:
# I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-apiserver.log
  pos_file /var/log/es-kube-apiserver.log.pos
  tag kube-apiserver
</source>

# Example:
# I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-controller-manager.log
  pos_file /var/log/es-kube-controller-manager.log.pos
  tag kube-controller-manager
</source>

# Example:
# W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-scheduler.log
  pos_file /var/log/es-kube-scheduler.log.pos
  tag kube-scheduler
</source>

# Example:
# I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/rescheduler.log
  pos_file /var/log/es-rescheduler.log.pos
  tag rescheduler
</source>

# Example:
# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/glbc.log
  pos_file /var/log/es-glbc.log.pos
  tag glbc
</source>

# Example:
# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/cluster-autoscaler.log
  pos_file /var/log/es-cluster-autoscaler.log.pos
  tag cluster-autoscaler
</source>

# Logs from systemd-journal for interesting services.
<source>
  type systemd
  filters [{ "_SYSTEMD_UNIT": "docker.service" }]
  pos_file /var/log/gcp-journald-docker.pos
  read_from_head true
  tag docker
</source>

<source>
  type systemd
  filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
  pos_file /var/log/gcp-journald-kubelet.pos
  read_from_head true
  tag kubelet
</source>

<source>
  type systemd
  filters [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
  pos_file /var/log/gcp-journald-node-problem-detector.pos
  read_from_head true
  tag node-problem-detector
</source>
containers.input.conf:
----
# This configuration file for Fluentd / td-agent is used
# to watch changes to Docker log files. The kubelet creates symlinks that
# capture the pod name, namespace, container name & Docker container ID
# to the docker logs for pods in the /var/log/containers directory on the host.
# If running this fluentd configuration in a Docker container, the /var/log
# directory should be mounted in the container.
#
# These logs are then submitted to Elasticsearch which assumes the
# installation of the fluent-plugin-elasticsearch & the
# fluent-plugin-kubernetes_metadata_filter plugins.
# See https://github.com/uken/fluent-plugin-elasticsearch &
# https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
# more information about the plugins.
#
# Example
# =======
# A line in the Docker log file might look like this JSON:
#
# {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
#  "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z"}
#
# The time_format specification below makes sure we properly
# parse the time format produced by Docker. This will be
# submitted to Elasticsearch and should appear like:
# $ curl 'http://elasticsearch-logging:9200/_search?pretty'
# ...
# {
#      "_index" : "logstash-2014.09.25",
#      "_type" : "fluentd",
#      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
#      "_score" : 1.0,
#      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
#                 "stream":"stderr","tag":"docker.container.all",
#                 "@timestamp":"2014-09-25T22:45:50+00:00"}
#    },
# ...
#
# The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
# record & add labels to the log record if properly configured. This enables users
# to filter & search logs on any metadata.
# For example a Docker container's logs might be in the directory:
#
#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
#
# and in the file:
#
#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# where 997599971ee6... is the Docker ID of the running container.
# The Kubernetes kubelet makes a symbolic link to this file on the host machine
# in the /var/log/containers directory which includes the pod name and the Kubernetes
# container name:
#
#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#    ->
#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# The /var/log directory on the host is mapped to the /var/log directory in the container
# running this instance of Fluentd and we end up collecting the file:
#
#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# This results in the tag:
#
#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
# which are added to the log message as a kubernetes field object & the Docker container ID
# is also added under the docker field object.
# The final tag is:
#
#   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# And the final log record look like:
#
# {
#   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
#   "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z",
#   "kubernetes": {
#     "namespace": "default",
#     "pod_name": "synthetic-logger-0.25lps-pod",
#     "container_name": "synth-lgr"
#   },
#   "docker": {
#     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
#   }
# }
#
# This makes it easier for users to search for logs by pod name or by
# the name of the Kubernetes container regardless of how many times the
# Kubernetes pod has been restarted (resulting in a several Docker container IDs).

# Json Log Example:
# {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
# CRI Log Example:
# 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
<source>
  type tail
  path /var/log/containers/*.log
  pos_file /var/log/es-containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S.%NZ
  tag kubernetes.*
  read_from_head true
  format multi_format
  <pattern>
    format json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </pattern>
  <pattern>
    format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
    time_format %Y-%m-%dT%H:%M:%S.%N%:z
  </pattern>
</source>
forward.input.conf:
----
# Takes the messages sent over TCP
<source>
  type forward
</source>
Events:  <none>


Name:         kube-proxy
Namespace:    kube-system
Labels:       app=kube-proxy
Annotations:  <none>

Data
====
kubeconfig.conf:
----
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    server: https://10.10.97.20:6443
  name: default
contexts:
- context:
    cluster: default
    namespace: default
    user: default
  name: default
current-context: default
users:
- name: default
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token

Events:  <none>


Name:         kubeadm-config
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
MasterConfiguration:
----
api:
  advertiseAddress: 10.10.97.20
  bindPort: 6443
authorizationModes:
- Node
- RBAC
certificatesDir: /etc/kubernetes/pki
cloudProvider: ""
etcd:
  caFile: ""
  certFile: ""
  dataDir: /var/lib/etcd
  endpoints: null
  image: ""
  keyFile: ""
imageRepository: gcr.io/google_containers
kubernetesVersion: v1.8.4
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
nodeName: vhosakot1-m51b5b468be
token: ""
tokenTTL: 24h0m0s
unifiedControlPlaneImage: ""

Events:  <none>


Name:         my-nginx-ingress.v1
Namespace:    kube-system
Labels:       MODIFIED_AT=1518558940
              NAME=my-nginx-ingress
              OWNER=TILLER
              STATUS=DEPLOYED
              VERSION=1
Annotations:  <none>

Data
====
release:
----
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
Events:  <none>

================
kubectl describe controllerrevisions --all-namespaces
================

Name:         ccphxvolume-8b677dc65
Namespace:    default
Labels:       controller-revision-hash=462338721
              name=ccphxvolume
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"de...
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          Name:  ccphxvolume
      Spec:
        Containers:
          Command:
            /bin/sh
            -c
            while true; do sleep 2; done
          Image:              registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          Image Pull Policy:  IfNotPresent
          Name:               hxvolume
          Resources:
          Security Context:
            Privileged:                true
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
        Dns Policy:                    ClusterFirst
        Init Containers:
          Command:
            sh
            -c
            SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
          Image:              registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          Image Pull Policy:  IfNotPresent
          Name:               iscsi-initiator
          Resources:
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Command:
            sh
            -c
            cp /hxcache/hxvolume /hxhostmount/
          Image:              registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          Image Pull Policy:  IfNotPresent
          Name:               hxvolume-copy
          Resources:
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /hxhostmount
            Name:        hxvolume-mount
            Mount Path:  /etc/iscsi
            Name:        iscsi-volume
        Restart Policy:  Always
        Scheduler Name:  default-scheduler
        Security Context:
        Termination Grace Period Seconds:  30
        Tolerations:
          Effect:  NoSchedule
          Key:     node-role.kubernetes.io/master
        Volumes:
          Host Path:
            Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
            Type:  
          Name:    hxvolume-mount
          Host Path:
            Path:  /etc/iscsi
            Type:  
          Name:    iscsi-volume
Kind:              ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-09T19:46:58Z
  Owner References:
    API Version:           extensions/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  DaemonSet
    Name:                  ccphxvolume
    UID:                   f73cde15-0dd1-11e8-871b-005056af9e97
  Resource Version:        375
  Self Link:               /apis/apps/v1beta1/namespaces/default/controllerrevisions/ccphxvolume-8b677dc65
  UID:                     fd184e83-0dd1-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>


Name:         kube-keepalived-vip-b5cc789d5
Namespace:    default
Labels:       controller-revision-hash=617734581
              name=kube-keepalived-vip
Annotations:  <none>
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          Name:  kube-keepalived-vip
      Spec:
        Containers:
          Args:
            --services-configmap=default/vip-configmap
          Env:
            Name:  POD_NAME
            Value From:
              Field Ref:
                API Version:  v1
                Field Path:   metadata.name
            Name:             POD_NAMESPACE
            Value From:
              Field Ref:
                API Version:  v1
                Field Path:   metadata.namespace
          Image:              k8s.gcr.io/kube-keepalived-vip:0.11
          Image Pull Policy:  Always
          Name:               kube-keepalived-vip
          Resources:
          Security Context:
            Privileged:                true
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /lib/modules
            Name:        modules
            Read Only:   true
            Mount Path:  /dev
            Name:        dev
        Dns Policy:      ClusterFirst
        Host Network:    true
        Restart Policy:  Always
        Scheduler Name:  default-scheduler
        Security Context:
        Service Account:                   kube-keepalived-vip
        Service Account Name:              kube-keepalived-vip
        Termination Grace Period Seconds:  30
        Volumes:
          Host Path:
            Path:  /lib/modules
            Type:  
          Name:    modules
          Host Path:
            Path:  /dev
            Type:  
          Name:    dev
Kind:              ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-13T19:01:11Z
  Owner References:
    API Version:           extensions/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  DaemonSet
    Name:                  kube-keepalived-vip
    UID:                   41d5383d-10f0-11e8-871b-005056af9e97
  Resource Version:        458291
  Self Link:               /apis/apps/v1beta1/namespaces/default/controllerrevisions/kube-keepalived-vip-b5cc789d5
  UID:                     41d629a4-10f0-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>


Name:         calico-node-76cc6dcd86
Namespace:    kube-system
Labels:       controller-revision-hash=3277287842
              k8s-app=calico-node
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Annotations:
          Scheduler . Alpha . Kubernetes . Io / Critical - Pod:  
        Creation Timestamp:                                      <nil>
        Labels:
          K 8 S - App:  calico-node
      Spec:
        Containers:
          Env:
            Name:   DATASTORE_TYPE
            Value:  kubernetes
            Name:   FELIX_LOGSEVERITYSCREEN
            Value:  info
            Name:   CLUSTER_TYPE
            Value:  k8s,bgp
            Name:   CALICO_DISABLE_FILE_LOGGING
            Value:  true
            Name:   FELIX_DEFAULTENDPOINTTOHOSTACTION
            Value:  ACCEPT
            Name:   FELIX_IPV6SUPPORT
            Value:  false
            Name:   FELIX_IPINIPMTU
            Value:  1440
            Name:   WAIT_FOR_DATASTORE
            Value:  true
            Name:   CALICO_IPV4POOL_CIDR
            Value:  192.168.0.0/16
            Name:   CALICO_IPV4POOL_IPIP
            Value:  Always
            Name:   FELIX_IPINIPENABLED
            Value:  true
            Name:   FELIX_TYPHAK8SSERVICENAME
            Value From:
              Config Map Key Ref:
                Key:   typha_service_name
                Name:  calico-config
            Name:      NODENAME
            Value From:
              Field Ref:
                API Version:  v1
                Field Path:   spec.nodeName
            Name:             IP
            Name:             FELIX_HEALTHENABLED
            Value:            true
          Image:              registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
          Image Pull Policy:  IfNotPresent
          Liveness Probe:
            Failure Threshold:  6
            Http Get:
              Path:                 /liveness
              Port:                 9099
              Scheme:               HTTP
            Initial Delay Seconds:  10
            Period Seconds:         10
            Success Threshold:      1
            Timeout Seconds:        1
          Name:                     calico-node
          Readiness Probe:
            Failure Threshold:  3
            Http Get:
              Path:             /readiness
              Port:             9099
              Scheme:           HTTP
            Period Seconds:     10
            Success Threshold:  1
            Timeout Seconds:    1
          Resources:
            Requests:
              Cpu:  250m
          Security Context:
            Privileged:                true
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /lib/modules
            Name:        lib-modules
            Read Only:   true
            Mount Path:  /var/run/calico
            Name:        var-run-calico
          Command:
            /install-cni.sh
          Env:
            Name:   CNI_CONF_NAME
            Value:  10-calico.conflist
            Name:   CNI_NETWORK_CONFIG
            Value From:
              Config Map Key Ref:
                Key:   cni_network_config
                Name:  calico-config
            Name:      KUBERNETES_NODE_NAME
            Value From:
              Field Ref:
                API Version:  v1
                Field Path:   spec.nodeName
          Image:              registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
          Image Pull Policy:  IfNotPresent
          Name:               install-cni
          Resources:
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /host/opt/cni/bin
            Name:        cni-bin-dir
            Mount Path:  /host/etc/cni/net.d
            Name:        cni-net-dir
        Dns Policy:      ClusterFirst
        Host Network:    true
        Restart Policy:  Always
        Scheduler Name:  default-scheduler
        Security Context:
        Service Account:                   calico-node
        Service Account Name:              calico-node
        Termination Grace Period Seconds:  0
        Tolerations:
          Effect:    NoSchedule
          Key:       node-role.kubernetes.io/master
          Key:       CriticalAddonsOnly
          Operator:  Exists
        Volumes:
          Host Path:
            Path:  /lib/modules
            Type:  
          Name:    lib-modules
          Host Path:
            Path:  /var/run/calico
            Type:  
          Name:    var-run-calico
          Host Path:
            Path:  /opt/cni/bin
            Type:  
          Name:    cni-bin-dir
          Host Path:
            Path:  /etc/cni/net.d
            Type:  
          Name:    cni-net-dir
Kind:              ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-09T19:46:57Z
  Owner References:
    API Version:           extensions/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  DaemonSet
    Name:                  calico-node
    UID:                   f590af99-0dd1-11e8-871b-005056af9e97
  Resource Version:        329
  Self Link:               /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/calico-node-76cc6dcd86
  UID:                     fcf5befc-0dd1-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>


Name:         elasticsearch-logging-7978c6964c
Namespace:    kube-system
Labels:       controller.kubernetes.io/hash=3534725207
              k8s-app=elasticsearch-logging
              version=v5.6.4
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elast...
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          K 8 S - App:                          elasticsearch-logging
          Kubernetes . Io / Cluster - Service:  true
          Version:                              v5.6.4
      Spec:
        Containers:
          Env:
            Name:  NAMESPACE
            Value From:
              Field Ref:
                API Version:  v1
                Field Path:   metadata.namespace
          Image:              registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
          Image Pull Policy:  IfNotPresent
          Name:               elasticsearch-logging
          Ports:
            Container Port:  9200
            Name:            db
            Protocol:        TCP
            Container Port:  9300
            Name:            transport
            Protocol:        TCP
          Resources:
            Limits:
              Cpu:  1
            Requests:
              Cpu:                     100m
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /data
            Name:        elasticsearch-logging
        Dns Policy:      ClusterFirst
        Init Containers:
          Command:
            /sbin/sysctl
            -w
            vm.max_map_count=262144
          Image:              registry.ci.dfj.io/cpsg_ccp/alpine:3.6
          Image Pull Policy:  IfNotPresent
          Name:               elasticsearch-logging-init
          Resources:
          Security Context:
            Privileged:                true
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
        Restart Policy:                Always
        Scheduler Name:                default-scheduler
        Security Context:
        Service Account:                   elasticsearch-logging
        Service Account Name:              elasticsearch-logging
        Termination Grace Period Seconds:  30
        Volumes:
          Empty Dir:
          Name:  elasticsearch-logging
Kind:            ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-09T19:46:57Z
  Owner References:
    API Version:           apps/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  StatefulSet
    Name:                  elasticsearch-logging
    UID:                   f66f6df9-0dd1-11e8-871b-005056af9e97
  Resource Version:        342
  Self Link:               /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/elasticsearch-logging-7978c6964c
  UID:                     fcf7ed0a-0dd1-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>


Name:         fluentd-es-v2.0.2-55f788b445
Namespace:    kube-system
Labels:       controller-revision-hash=1193446001
              k8s-app=fluentd-es
              kubernetes.io/cluster-service=true
              version=v2.0.2
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd...
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Annotations:
          Scheduler . Alpha . Kubernetes . Io / Critical - Pod:  
        Creation Timestamp:                                      <nil>
        Labels:
          K 8 S - App:                          fluentd-es
          Kubernetes . Io / Cluster - Service:  true
          Version:                              v2.0.2
      Spec:
        Containers:
          Env:
            Name:             FLUENTD_ARGS
            Value:            --no-supervisor -q
          Image:              registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
          Image Pull Policy:  IfNotPresent
          Name:               fluentd-es
          Resources:
            Limits:
              Memory:  500Mi
            Requests:
              Cpu:                     100m
              Memory:                  200Mi
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /var/log
            Name:        varlog
            Mount Path:  /var/lib/docker/containers
            Name:        varlibdockercontainers
            Read Only:   true
            Mount Path:  /host/lib
            Name:        libsystemddir
            Read Only:   true
            Mount Path:  /etc/fluent/config.d
            Name:        config-volume
        Dns Policy:      ClusterFirst
        Restart Policy:  Always
        Scheduler Name:  default-scheduler
        Security Context:
        Service Account:                   fluentd-es
        Service Account Name:              fluentd-es
        Termination Grace Period Seconds:  30
        Volumes:
          Host Path:
            Path:  /var/log
            Type:  
          Name:    varlog
          Host Path:
            Path:  /var/lib/docker/containers
            Type:  
          Name:    varlibdockercontainers
          Host Path:
            Path:  /usr/lib64
            Type:  
          Name:    libsystemddir
          Config Map:
            Default Mode:  420
            Name:          fluentd-es-config-v0.1.1
          Name:            config-volume
Kind:                      ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-09T19:46:58Z
  Owner References:
    API Version:           extensions/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  DaemonSet
    Name:                  fluentd-es-v2.0.2
    UID:                   f626704d-0dd1-11e8-871b-005056af9e97
  Resource Version:        376
  Self Link:               /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/fluentd-es-v2.0.2-55f788b445
  UID:                     fd1718b9-0dd1-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>


Name:         kube-proxy-95856ccc5
Namespace:    kube-system
Labels:       controller-revision-hash=514127771
              k8s-app=kube-proxy
Annotations:  <none>
API Version:  apps/v1beta1
Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          K 8 S - App:  kube-proxy
      Spec:
        Containers:
          Command:
            /usr/local/bin/kube-proxy
            --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
            --cluster-cidr=192.168.0.0/16
          Image:              gcr.io/google_containers/kube-proxy-amd64:v1.8.4
          Image Pull Policy:  IfNotPresent
          Name:               kube-proxy
          Resources:
          Security Context:
            Privileged:                true
          Termination Message Path:    /dev/termination-log
          Termination Message Policy:  File
          Volume Mounts:
            Mount Path:  /var/lib/kube-proxy
            Name:        kube-proxy
            Mount Path:  /run/xtables.lock
            Name:        xtables-lock
            Mount Path:  /lib/modules
            Name:        lib-modules
            Read Only:   true
        Dns Policy:      ClusterFirst
        Host Network:    true
        Restart Policy:  Always
        Scheduler Name:  default-scheduler
        Security Context:
        Service Account:                   kube-proxy
        Service Account Name:              kube-proxy
        Termination Grace Period Seconds:  30
        Tolerations:
          Effect:  NoSchedule
          Key:     node-role.kubernetes.io/master
          Effect:  NoSchedule
          Key:     node.cloudprovider.kubernetes.io/uninitialized
          Value:   true
        Volumes:
          Config Map:
            Default Mode:  420
            Name:          kube-proxy
          Name:            kube-proxy
          Host Path:
            Path:  /run/xtables.lock
            Type:  FileOrCreate
          Name:    xtables-lock
          Host Path:
            Path:  /lib/modules
            Type:  
          Name:    lib-modules
Kind:              ControllerRevision
Metadata:
  Creation Timestamp:  2018-02-09T19:46:57Z
  Owner References:
    API Version:           extensions/v1beta1
    Block Owner Deletion:  true
    Controller:            true
    Kind:                  DaemonSet
    Name:                  kube-proxy
    UID:                   f4eaff3c-0dd1-11e8-871b-005056af9e97
  Resource Version:        330
  Self Link:               /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/kube-proxy-95856ccc5
  UID:                     fcf56484-0dd1-11e8-871b-005056af9e97
Revision:                  1
Events:                    <none>

================
kubectl describe cronjobs --all-namespaces
================


================
kubectl describe customresourcedefinition --all-namespaces
================

Name:         bgpconfigurations.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico BGP Configuration","kind":"CustomResourceDefinition","metadata":{"annotations":{},"n...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    216
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/bgpconfigurations.crd.projectcalico.org
  UID:                 f59ee823-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       BGPConfiguration
    List Kind:  BGPConfigurationList
    Plural:     bgpconfigurations
    Singular:   bgpconfiguration
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       BGPConfiguration
    List Kind:  BGPConfigurationList
    Plural:     bgpconfigurations
    Singular:   bgpconfiguration
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         bgppeers.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico BGP Peers","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"bg...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    213
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/bgppeers.crd.projectcalico.org
  UID:                 f59b343d-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       BGPPeer
    List Kind:  BGPPeerList
    Plural:     bgppeers
    Singular:   bgppeer
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       BGPPeer
    List Kind:  BGPPeerList
    Plural:     bgppeers
    Singular:   bgppeer
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         clusterinformations.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Cluster Information","kind":"CustomResourceDefinition","metadata":{"annotations":{},...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    220
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/clusterinformations.crd.projectcalico.org
  UID:                 f5a6010a-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       ClusterInformation
    List Kind:  ClusterInformationList
    Plural:     clusterinformations
    Singular:   clusterinformation
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       ClusterInformation
    List Kind:  ClusterInformationList
    Plural:     clusterinformations
    Singular:   clusterinformation
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         felixconfigurations.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Felix Configuration","kind":"CustomResourceDefinition","metadata":{"annotations":{},...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    210
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/felixconfigurations.crd.projectcalico.org
  UID:                 f595a15e-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       FelixConfiguration
    List Kind:  FelixConfigurationList
    Plural:     felixconfigurations
    Singular:   felixconfiguration
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       FelixConfiguration
    List Kind:  FelixConfigurationList
    Plural:     felixconfigurations
    Singular:   felixconfiguration
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         globalnetworkpolicies.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Global Network Policies","kind":"CustomResourceDefinition","metadata":{"annotations"...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    222
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/globalnetworkpolicies.crd.projectcalico.org
  UID:                 f5a99650-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       GlobalNetworkPolicy
    List Kind:  GlobalNetworkPolicyList
    Plural:     globalnetworkpolicies
    Singular:   globalnetworkpolicy
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       GlobalNetworkPolicy
    List Kind:  GlobalNetworkPolicyList
    Plural:     globalnetworkpolicies
    Singular:   globalnetworkpolicy
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         ippools.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico IP Pools","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"ipp...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    218
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/ippools.crd.projectcalico.org
  UID:                 f5a2bed4-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       IPPool
    List Kind:  IPPoolList
    Plural:     ippools
    Singular:   ippool
  Scope:        Cluster
  Version:      v1
Status:
  Accepted Names:
    Kind:       IPPool
    List Kind:  IPPoolList
    Plural:     ippools
    Singular:   ippool
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>


Name:         networkpolicies.crd.projectcalico.org
Namespace:    
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Network Policies","kind":"CustomResourceDefinition","metadata":{"annotations":{},"na...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2018-02-09T19:46:45Z
  Generation:          1
  Resource Version:    224
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/networkpolicies.crd.projectcalico.org
  UID:                 f5ad84b6-0dd1-11e8-871b-005056af9e97
Spec:
  Group:  crd.projectcalico.org
  Names:
    Kind:       NetworkPolicy
    List Kind:  NetworkPolicyList
    Plural:     networkpolicies
    Singular:   networkpolicy
  Scope:        Namespaced
  Version:      v1
Status:
  Accepted Names:
    Kind:       NetworkPolicy
    List Kind:  NetworkPolicyList
    Plural:     networkpolicies
    Singular:   networkpolicy
  Conditions:
    Last Transition Time:  <nil>
    Message:               no conflicts found
    Reason:                NoConflicts
    Status:                True
    Type:                  NamesAccepted
    Last Transition Time:  2018-02-09T19:46:45Z
    Message:               the initial names have been accepted
    Reason:                InitialNamesAccepted
    Status:                True
    Type:                  Established
Events:                    <none>

================
kubectl describe daemonsets --all-namespaces
================

Name:           ccphxvolume
Selector:       name=ccphxvolume
Node-Selector:  <none>
Labels:         name=ccphxvolume
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"de...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=ccphxvolume
  Init Containers:
   iscsi-initiator:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    Environment:  <none>
    Mounts:       <none>
   hxvolume-copy:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    Environment:  <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
  Containers:
   hxvolume:
    Image:  registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Port:   <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
   iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
Events:    <none>


Name:           kube-keepalived-vip
Selector:       name=kube-keepalived-vip
Node-Selector:  <none>
Labels:         name=kube-keepalived-vip
Annotations:    <none>
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 0
Pods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           name=kube-keepalived-vip
  Service Account:  kube-keepalived-vip
  Containers:
   kube-keepalived-vip:
    Image:  k8s.gcr.io/kube-keepalived-vip:0.11
    Port:   <none>
    Args:
      --services-configmap=default/vip-configmap
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
  Volumes:
   modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   dev:
    Type:  HostPath (bare host directory volume)
    Path:  /dev
Events:    <none>


Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  <none>
Labels:         k8s-app=calico-node
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-node:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Port:   <none>
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
   install-cni:
    Image:  registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Port:   <none>
    Command:
      /install-cni.sh
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
  Volumes:
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
   cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
   cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
Events:    <none>


Name:           fluentd-es-v2.0.2
Selector:       k8s-app=fluentd-es,version=v2.0.2
Node-Selector:  <none>
Labels:         addonmanager.kubernetes.io/mode=Reconcile
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                version=v2.0.2
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd...
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 1
Number of Nodes Misscheduled: 1
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=fluentd-es
                    kubernetes.io/cluster-service=true
                    version=v2.0.2
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  fluentd-es
  Containers:
   fluentd-es:
    Image:  registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Port:   <none>
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
  Volumes:
   varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
   varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
   libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
Events:        <none>


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  <none>
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 1
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:  gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Port:   <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
Events:    <none>

================
kubectl describe deployments --all-namespaces
================

Name:                   my-nginx-ingress-controller
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=controller
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=controller,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-controller-d95d4979d (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1


Name:                   my-nginx-ingress-default-backend
Namespace:              default
CreationTimestamp:      Tue, 13 Feb 2018 21:55:39 +0000
Labels:                 app=nginx-ingress
                        chart=nginx-ingress-0.9.2
                        component=default-backend
                        heritage=Tiller
                        release=my-nginx-ingress
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx-ingress,component=default-backend,release=my-nginx-ingress
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   my-nginx-ingress-default-backend-855d89f775 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1


Name:                   calico-typha
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:45 +0000
Labels:                 k8s-app=calico-typha
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"ku...
Selector:               k8s-app=calico-typha
Replicas:               0 desired | 0 updated | 0 total | 0 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=calico-typha
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   calico-typha-65b7467b56 (0/0 replicas created)
Events:          <none>


Name:                   kibana-logging
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:47 +0000
Labels:                 addonmanager.kubernetes.io/mode=Reconcile
                        k8s-app=kibana-logging
                        kubernetes.io/cluster-service=true
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana...
Selector:               k8s-app=kibana-logging
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  k8s-app=kibana-logging
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kibana-logging-767cf49759 (1/1 replicas created)
Events:          <none>


Name:                   kube-dns
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:44 +0000
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision=1
Selector:               k8s-app=kube-dns
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 10% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kube-dns-545bc4bfd4 (1/1 replicas created)
Events:          <none>


Name:                   kubernetes-dashboard
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:46:48 +0000
Labels:                 k8s-app=kubernetes-dashboard
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard"...
Selector:               k8s-app=kubernetes-dashboard
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubernetes-dashboard-7798c48646 (1/1 replicas created)
Events:          <none>


Name:                   tiller-deploy
Namespace:              kube-system
CreationTimestamp:      Fri, 09 Feb 2018 19:49:06 +0000
Labels:                 app=helm
                        name=tiller
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=helm,name=tiller
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=helm
                    name=tiller
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   tiller-deploy-546cf9696c (1/1 replicas created)
Events:          <none>

================
kubectl describe endpoints --all-namespaces
================

Name:         kubernetes
Namespace:    default
Labels:       <none>
Annotations:  <none>
Subsets:
  Addresses:          10.10.97.20
  NotReadyAddresses:  <none>
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    https  6443  TCP

Events:  <none>


Name:         my-nginx-ingress-controller
Namespace:    default
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              component=controller
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
Subsets:
  Addresses:          192.168.2.19
  NotReadyAddresses:  <none>
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    https  443   TCP
    http   80    TCP

Events:  <none>


Name:         my-nginx-ingress-default-backend
Namespace:    default
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              component=default-backend
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
Subsets:
  Addresses:          192.168.2.18
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  8080  TCP

Events:  <none>


Name:         tea-svc
Namespace:    default
Labels:       app=tea
Annotations:  <none>
Subsets:
  Addresses:          192.168.1.52,192.168.2.14,192.168.2.15
  NotReadyAddresses:  <none>
  Ports:
    Name  Port  Protocol
    ----  ----  --------
    http  80    TCP

Events:  <none>


Name:         calico-typha
Namespace:    kube-system
Labels:       k8s-app=calico-typha
Annotations:  <none>
Subsets:
Events:  <none>


Name:         elasticsearch-logging
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=elasticsearch-logging
              kubernetes.io/cluster-service=true
              kubernetes.io/name=Elasticsearch
Annotations:  <none>
Subsets:
  Addresses:          192.168.1.4,192.168.2.5
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  9200  TCP

Events:  <none>


Name:         kibana-logging
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=kibana-logging
              kubernetes.io/cluster-service=true
              kubernetes.io/name=Kibana
Annotations:  <none>
Subsets:
  Addresses:          192.168.2.4
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  5601  TCP

Events:  <none>


Name:         kube-controller-manager
Namespace:    kube-system
Labels:       <none>
Annotations:  control-plane.alpha.kubernetes.io/leader={"holderIdentity":"vhosakot1-m51b5b468be","leaseDurationSeconds":15,"acquireTime":"2018-02-09T19:46:43Z","renewTime":"2018-02-13T22:46:34Z","leaderTransitions"...
Subsets:
Events:  <none>


Name:         kube-dns
Namespace:    kube-system
Labels:       k8s-app=kube-dns
              kubernetes.io/cluster-service=true
              kubernetes.io/name=KubeDNS
Annotations:  <none>
Subsets:
  Addresses:          192.168.0.4
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    dns      53    UDP
    dns-tcp  53    TCP

Events:  <none>


Name:         kube-scheduler
Namespace:    kube-system
Labels:       <none>
Annotations:  control-plane.alpha.kubernetes.io/leader={"holderIdentity":"vhosakot1-m51b5b468be","leaseDurationSeconds":15,"acquireTime":"2018-02-09T19:46:42Z","renewTime":"2018-02-13T22:46:35Z","leaderTransitions"...
Subsets:
Events:  <none>


Name:         kubernetes-dashboard
Namespace:    kube-system
Labels:       k8s-app=kubernetes-dashboard
Annotations:  <none>
Subsets:
  Addresses:          192.168.0.3
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  8443  TCP

Events:  <none>


Name:         tiller-deploy
Namespace:    kube-system
Labels:       app=helm
              name=tiller
Annotations:  <none>
Subsets:
  Addresses:          192.168.2.6
  NotReadyAddresses:  <none>
  Ports:
    Name    Port   Protocol
    ----    ----   --------
    tiller  44134  TCP

Events:  <none>

================
kubectl describe events --all-namespaces
================

Name:             cafe-ingress.15130151c58ea80b
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:50Z
Involved Object:
  API Version:       extensions
  Kind:              Ingress
  Name:              cafe-ingress
  Namespace:         default
  Resource Version:  447311
  UID:               21048cc8-100e-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:50Z
Message:             Ingress default/cafe-ingress
Metadata:
  Creation Timestamp:  2018-02-13T21:55:49Z
  Resource Version:    473474
  Self Link:           /api/v1/namespaces/default/events/cafe-ingress.15130151c58ea80b
  UID:                 a7029d48-1108-11e8-871b-005056af9e97
Reason:                CREATE
Source:
  Component:  nginx-ingress-controller
Type:         Normal
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1512f7ca5b38a1cc
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            4
First Timestamp:  2018-02-13T19:01:12Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{kube-keepalived-vip}
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:24Z
Message:             pulling image "k8s.gcr.io/kube-keepalived-vip:0.11"
Metadata:
  Creation Timestamp:  2018-02-13T21:55:25Z
  Resource Version:    473531
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca5b38a1cc
  UID:                 98d7b162-1108-11e8-871b-005056af9e97
Reason:                Pulling
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1512f7ca825390ad
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            4
First Timestamp:  2018-02-13T19:01:13Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{kube-keepalived-vip}
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:25Z
Message:             Successfully pulled image "k8s.gcr.io/kube-keepalived-vip:0.11"
Metadata:
  Creation Timestamp:  2018-02-13T21:55:26Z
  Resource Version:    473534
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca825390ad
  UID:                 993b5c48-1108-11e8-871b-005056af9e97
Reason:                Pulled
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1512f7ca85d1fe56
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            4
First Timestamp:  2018-02-13T19:01:13Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{kube-keepalived-vip}
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:25Z
Message:             Created container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:26Z
  Resource Version:    473535
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca85d1fe56
  UID:                 9944fa54-1108-11e8-871b-005056af9e97
Reason:                Created
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1512f7ca8bd6ddbd
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            4
First Timestamp:  2018-02-13T19:01:13Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{kube-keepalived-vip}
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:25Z
Message:             Started container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:26Z
  Resource Version:    473536
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca8bd6ddbd
  UID:                 9956cda9-1108-11e8-871b-005056af9e97
Reason:                Started
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1513014e24e93088
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            3
First Timestamp:  2018-02-13T21:55:34Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{kube-keepalived-vip}
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:09Z
Message:             Back-off restarting failed container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:33Z
  Resource Version:    473509
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1513014e24e93088
  UID:                 9db957a0-1108-11e8-871b-005056af9e97
Reason:                BackOff
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Warning
Events:       <none>


Name:             kube-keepalived-vip-fph6b.1513014e24eef2af
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            3
First Timestamp:  2018-02-13T21:55:34Z
Involved Object:
  API Version:       v1
  Kind:              Pod
  Name:              kube-keepalived-vip-fph6b
  Namespace:         default
  Resource Version:  458292
  UID:               41d733d7-10f0-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:56:09Z
Message:             Error syncing pod
Metadata:
  Creation Timestamp:  2018-02-13T21:55:33Z
  Resource Version:    473510
  Self Link:           /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1513014e24eef2af
  UID:                 9db9c435-1108-11e8-871b-005056af9e97
Reason:                FailedSync
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Warning
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-cs7mm.1513014bd41394f2
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:24Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-cs7mm
  Namespace:         default
  Resource Version:  447142
  UID:               7cd083ae-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:24Z
Message:             Liveness probe failed: HTTP probe failed with statuscode: 500
Metadata:
  Creation Timestamp:  2018-02-13T21:55:23Z
  Resource Version:    473346
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014bd41394f2
  UID:                 97cbac64-1108-11e8-871b-005056af9e97
Reason:                Unhealthy
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Warning
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-cs7mm.1513014c53ffe35a
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:26Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-cs7mm
  Namespace:         default
  Resource Version:  447142
  UID:               7cd083ae-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:26Z
Message:             Readiness probe failed: HTTP probe failed with statuscode: 500
Metadata:
  Creation Timestamp:  2018-02-13T21:55:26Z
  Resource Version:    473364
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014c53ffe35a
  UID:                 99132654-1108-11e8-871b-005056af9e97
Reason:                Unhealthy
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Warning
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-cs7mm.1513014d53723904
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:30Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-cs7mm
  Namespace:         default
  Resource Version:  447142
  UID:               7cd083ae-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:30Z
Message:             Killing container with id docker://nginx-ingress-controller:Need to kill Pod
Metadata:
  Creation Timestamp:  2018-02-13T21:55:30Z
  Resource Version:    473377
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014d53723904
  UID:                 9ba11c75-1108-11e8-871b-005056af9e97
Reason:                Killing
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f669c2243
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       v1
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-kzgk7
  Namespace:         default
  Resource Version:  473420
  UID:               a1522304-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Successfully assigned my-nginx-ingress-controller-d95d4979d-kzgk7 to vhosakot1-wc80d3e5ab6
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473429
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f669c2243
  UID:                 a154e90d-1108-11e8-871b-005056af9e97
Reason:                Scheduled
Source:
  Component:  default-scheduler
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f9ddacac3
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:40Z
Involved Object:
  API Version:       v1
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-kzgk7
  Namespace:         default
  Resource Version:  473425
  UID:               a1522304-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:40Z
Message:             MountVolume.SetUp succeeded for volume "my-nginx-ingress-token-9ffd9" 
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473441
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f9ddacac3
  UID:                 a17ea26a-1108-11e8-871b-005056af9e97
Reason:                SuccessfulMountVolume
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc229c270
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-kzgk7
  Namespace:         default
  Resource Version:  473425
  UID:               a1522304-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Container image "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2" already present on machine
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473445
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc229c270
  UID:                 a1db4ee9-1108-11e8-871b-005056af9e97
Reason:                Pulled
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc7026fae
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-kzgk7
  Namespace:         default
  Resource Version:  473425
  UID:               a1522304-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Created container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473447
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc7026fae
  UID:                 a1e7a1a6-1108-11e8-871b-005056af9e97
Reason:                Created
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fceb03e13
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-controller}
  Kind:              Pod
  Name:              my-nginx-ingress-controller-d95d4979d-kzgk7
  Namespace:         default
  Resource Version:  473425
  UID:               a1522304-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Started container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473450
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fceb03e13
  UID:                 a1fb50b5-1108-11e8-871b-005056af9e97
Reason:                Started
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d.1513014a9836c301
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:19Z
Involved Object:
  API Version:       extensions
  Kind:              ReplicaSet
  Name:              my-nginx-ingress-controller-d95d4979d
  Namespace:         default
  Resource Version:  473318
  UID:               7cce3e08-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:19Z
Message:             Deleted pod: my-nginx-ingress-controller-d95d4979d-cs7mm
Metadata:
  Creation Timestamp:  2018-02-13T21:55:19Z
  Resource Version:    473324
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d.1513014a9836c301
  UID:                 95071c41-1108-11e8-871b-005056af9e97
Reason:                SuccessfulDelete
Source:
  Component:  replicaset-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller-d95d4979d.1513014f660b56e4
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       extensions
  Kind:              ReplicaSet
  Name:              my-nginx-ingress-controller-d95d4979d
  Namespace:         default
  Resource Version:  473413
  UID:               a14fe756-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473427
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d.1513014f660b56e4
  UID:                 a1539e36-1108-11e8-871b-005056af9e97
Reason:                SuccessfulCreate
Source:
  Component:  replicaset-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller.1513014a979e01a5
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:19Z
Involved Object:
  API Version:       extensions
  Kind:              Deployment
  Name:              my-nginx-ingress-controller
  Namespace:         default
  Resource Version:  473317
  UID:               7ccd026e-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:19Z
Message:             Scaled down replica set my-nginx-ingress-controller-d95d4979d to 0
Metadata:
  Creation Timestamp:  2018-02-13T21:55:19Z
  Resource Version:    473321
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller.1513014a979e01a5
  UID:                 9505af83-1108-11e8-871b-005056af9e97
Reason:                ScalingReplicaSet
Source:
  Component:  deployment-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-controller.1513014f64f57f7f
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       extensions
  Kind:              Deployment
  Name:              my-nginx-ingress-controller
  Namespace:         default
  Resource Version:  473412
  UID:               a14f1a1b-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473416
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-controller.1513014f64f57f7f
  UID:                 a150ea6c-1108-11e8-871b-005056af9e97
Reason:                ScalingReplicaSet
Source:
  Component:  deployment-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p495h.1513014b7ac09d99
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:23Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-default-backend}
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p495h
  Namespace:         default
  Resource Version:  447149
  UID:               7cd07bec-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:23Z
Message:             Killing container with id docker://nginx-ingress-default-backend:Need to kill Pod
Metadata:
  Creation Timestamp:  2018-02-13T21:55:22Z
  Resource Version:    473341
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p495h.1513014b7ac09d99
  UID:                 96e7015b-1108-11e8-871b-005056af9e97
Reason:                Killing
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f67b5a669
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       v1
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p8jk6
  Namespace:         default
  Resource Version:  473426
  UID:               a155085d-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Successfully assigned my-nginx-ingress-default-backend-855d89f775-p8jk6 to vhosakot1-wc80d3e5ab6
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473436
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f67b5a669
  UID:                 a157ae5f-1108-11e8-871b-005056af9e97
Reason:                Scheduled
Source:
  Component:  default-scheduler
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f9dd3ee65
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:40Z
Involved Object:
  API Version:       v1
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p8jk6
  Namespace:         default
  Resource Version:  473430
  UID:               a155085d-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:40Z
Message:             MountVolume.SetUp succeeded for volume "default-token-fc6vg" 
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473440
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f9dd3ee65
  UID:                 a17e3076-1108-11e8-871b-005056af9e97
Reason:                SuccessfulMountVolume
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc0342d14
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-default-backend}
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p8jk6
  Namespace:         default
  Resource Version:  473430
  UID:               a155085d-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Container image "k8s.gcr.io/defaultbackend:1.3" already present on machine
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473444
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc0342d14
  UID:                 a1d6572d-1108-11e8-871b-005056af9e97
Reason:                Pulled
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc5b17d44
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-default-backend}
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p8jk6
  Namespace:         default
  Resource Version:  473430
  UID:               a155085d-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Created container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473446
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc5b17d44
  UID:                 a1e445ca-1108-11e8-871b-005056af9e97
Reason:                Created
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fccc1a9d7
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:41Z
Involved Object:
  API Version:       v1
  Field Path:        spec.containers{nginx-ingress-default-backend}
  Kind:              Pod
  Name:              my-nginx-ingress-default-backend-855d89f775-p8jk6
  Namespace:         default
  Resource Version:  473430
  UID:               a155085d-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:41Z
Message:             Started container
Metadata:
  Creation Timestamp:  2018-02-13T21:55:40Z
  Resource Version:    473449
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fccc1a9d7
  UID:                 a1f675ef-1108-11e8-871b-005056af9e97
Reason:                Started
Source:
  Component:  kubelet
  Host:       vhosakot1-wc80d3e5ab6
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775.1513014b501f2edd
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:22Z
Involved Object:
  API Version:       extensions
  Kind:              ReplicaSet
  Name:              my-nginx-ingress-default-backend-855d89f775
  Namespace:         default
  Resource Version:  473333
  UID:               7ccea98b-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:22Z
Message:             Deleted pod: my-nginx-ingress-default-backend-855d89f775-p495h
Metadata:
  Creation Timestamp:  2018-02-13T21:55:22Z
  Resource Version:    473339
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775.1513014b501f2edd
  UID:                 96ddfd5f-1108-11e8-871b-005056af9e97
Reason:                SuccessfulDelete
Source:
  Component:  replicaset-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend-855d89f775.1513014f670fda78
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       extensions
  Kind:              ReplicaSet
  Name:              my-nginx-ingress-default-backend-855d89f775
  Namespace:         default
  Resource Version:  473418
  UID:               a151592a-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473433
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775.1513014f670fda78
  UID:                 a1565d8c-1108-11e8-871b-005056af9e97
Reason:                SuccessfulCreate
Source:
  Component:  replicaset-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend.1513014b4f9d7b8e
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:22Z
Involved Object:
  API Version:       extensions
  Kind:              Deployment
  Name:              my-nginx-ingress-default-backend
  Namespace:         default
  Resource Version:  473332
  UID:               7ccdfe1f-10de-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:22Z
Message:             Scaled down replica set my-nginx-ingress-default-backend-855d89f775 to 0
Metadata:
  Creation Timestamp:  2018-02-13T21:55:22Z
  Resource Version:    473335
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend.1513014b4f9d7b8e
  UID:                 96dc8aa8-1108-11e8-871b-005056af9e97
Reason:                ScalingReplicaSet
Source:
  Component:  deployment-controller
Type:         Normal
Events:       <none>


Name:             my-nginx-ingress-default-backend.1513014f656e8854
Namespace:        default
Labels:           <none>
Annotations:      <none>
API Version:      v1
Count:            1
First Timestamp:  2018-02-13T21:55:39Z
Involved Object:
  API Version:       extensions
  Kind:              Deployment
  Name:              my-nginx-ingress-default-backend
  Namespace:         default
  Resource Version:  473414
  UID:               a14f9e60-1108-11e8-871b-005056af9e97
Kind:                Event
Last Timestamp:      2018-02-13T21:55:39Z
Message:             Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1
Metadata:
  Creation Timestamp:  2018-02-13T21:55:39Z
  Resource Version:    473422
  Self Link:           /api/v1/namespaces/default/events/my-nginx-ingress-default-backend.1513014f656e8854
  UID:                 a1520e3d-1108-11e8-871b-005056af9e97
Reason:                ScalingReplicaSet
Source:
  Component:  deployment-controller
Type:         Normal
Events:       <none>

================
kubectl describe horizontalpodautoscalers --all-namespaces
================


================
kubectl describe ingresses --all-namespaces
================

Name:             cafe-ingress
Namespace:        default
Address:          
Default backend:  default-http-backend:80 (<none>)
TLS:
  cafe-secret terminates cafe.example.com
Rules:
  Host              Path  Backends
  ----              ----  --------
  cafe.example.com  
                    /tea      tea-svc:80 (<none>)
                    /coffee   coffee-svc:80 (<none>)
Annotations:
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  50m   nginx-ingress-controller  Ingress default/cafe-ingress

================
kubectl describe jobs --all-namespaces
================


================
kubectl describe limitranges --all-namespaces
================


================
kubectl describe namespaces --all-namespaces
================

Name:         default
Labels:       <none>
Annotations:  <none>
Status:       Active

No resource quota.

No resource limits.


Name:         kube-public
Labels:       <none>
Annotations:  <none>
Status:       Active

No resource quota.

No resource limits.


Name:         kube-system
Labels:       <none>
Annotations:  <none>
Status:       Active

No resource quota.

No resource limits.

================
kubectl describe networkpolicies --all-namespaces
================


================
kubectl describe nodes --all-namespaces
================

Name:               vhosakot1-m51b5b468be
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=vhosakot1-m51b5b468be
                    node-role.kubernetes.io/master=
Annotations:        node.alpha.kubernetes.io/ttl=0
                    projectcalico.org/IPv4Address=10.10.97.20/22
                    volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             node-role.kubernetes.io/master:NoSchedule
CreationTimestamp:  Fri, 09 Feb 2018 19:46:40 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Tue, 13 Feb 2018 22:46:38 +0000   Fri, 09 Feb 2018 19:46:36 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Tue, 13 Feb 2018 22:46:38 +0000   Fri, 09 Feb 2018 19:46:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 13 Feb 2018 22:46:38 +0000   Fri, 09 Feb 2018 19:46:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Tue, 13 Feb 2018 22:46:38 +0000   Fri, 09 Feb 2018 19:47:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  10.10.97.20
  Hostname:    vhosakot1-m51b5b468be
Capacity:
 cpu:     2
 memory:  16432780Ki
 pods:    110
Allocatable:
 cpu:     2
 memory:  16330380Ki
 pods:    110
System Info:
 Machine ID:                 9ebc80503a934280b09c657524f9dcb4
 System UUID:                422F6C6E-40D4-4689-47A1-347776818F58
 Boot ID:                    7787ad21-ad5f-4249-9c27-6fe98b4b2745
 Kernel Version:             4.4.0-109-generic
 OS Image:                   Ubuntu 16.04.3 LTS
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://1.13.1
 Kubelet Version:            v1.8.4
 Kube-Proxy Version:         v1.8.4
PodCIDR:                     192.168.0.0/24
ExternalID:                  vhosakot1-m51b5b468be
Non-terminated Pods:         (9 in total)
  Namespace                  Name                                             CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                             ------------  ----------  ---------------  -------------
  default                    ccphxvolume-vlgmz                                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                calico-node-g99qc                                250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                etcd-vhosakot1-m51b5b468be                       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-apiserver-vhosakot1-m51b5b468be             250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-controller-manager-vhosakot1-m51b5b468be    200m (10%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-dns-545bc4bfd4-rzpz4                        260m (13%)    0 (0%)      110Mi (0%)       170Mi (1%)
  kube-system                kube-proxy-8vlsv                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-scheduler-vhosakot1-m51b5b468be             100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system                kubernetes-dashboard-7798c48646-rjmch            0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  1060m (53%)   0 (0%)      110Mi (0%)       170Mi (1%)
Events:         <none>


Name:               vhosakot1-wc80d3e5ab6
Roles:              <none>
Labels:             app=keepalived
                    beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=vhosakot1-wc80d3e5ab6
Annotations:        node.alpha.kubernetes.io/ttl=0
                    projectcalico.org/IPv4Address=10.10.97.62/22
                    volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Fri, 09 Feb 2018 19:47:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Tue, 13 Feb 2018 22:46:37 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Tue, 13 Feb 2018 22:46:37 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 13 Feb 2018 22:46:37 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Tue, 13 Feb 2018 22:46:37 +0000   Fri, 09 Feb 2018 19:47:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  10.10.97.62
  Hostname:    vhosakot1-wc80d3e5ab6
Capacity:
 cpu:     2
 memory:  16432780Ki
 pods:    110
Allocatable:
 cpu:     2
 memory:  16330380Ki
 pods:    110
System Info:
 Machine ID:                 9ebc80503a934280b09c657524f9dcb4
 System UUID:                422F02B5-096A-AAD9-2560-FDBAB5F059B8
 Boot ID:                    156c77a6-1219-4d3f-bbfb-8b08e2227f82
 Kernel Version:             4.4.0-109-generic
 OS Image:                   Ubuntu 16.04.3 LTS
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://1.13.1
 Kubelet Version:            v1.8.4
 Kube-Proxy Version:         v1.8.4
PodCIDR:                     192.168.2.0/24
ExternalID:                  vhosakot1-wc80d3e5ab6
Non-terminated Pods:         (12 in total)
  Namespace                  Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                                 ------------  ----------  ---------------  -------------
  default                    ccphxvolume-t972j                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    kube-keepalived-vip-fph6b                            0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    my-nginx-ingress-controller-d95d4979d-kzgk7          0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    my-nginx-ingress-default-backend-855d89f775-p8jk6    0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    tea-rc-96442                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    tea-rc-fxj6x                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                calico-node-2cpcv                                    250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                elasticsearch-logging-1                              100m (5%)     1 (50%)     0 (0%)           0 (0%)
  kube-system                fluentd-es-v2.0.2-9bmjw                              100m (5%)     0 (0%)      200Mi (1%)       500Mi (3%)
  kube-system                kibana-logging-767cf49759-f8zjt                      100m (5%)     1 (50%)     0 (0%)           0 (0%)
  kube-system                kube-proxy-q8ng8                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                tiller-deploy-546cf9696c-w4kq6                       0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  550m (27%)    2 (100%)    200Mi (1%)       500Mi (3%)
Events:         <none>


Name:               vhosakot1-we2d86faeb2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=vhosakot1-we2d86faeb2
Annotations:        node.alpha.kubernetes.io/ttl=0
                    projectcalico.org/IPv4Address=10.10.97.46/22
                    volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             key=value:NoSchedule
CreationTimestamp:  Fri, 09 Feb 2018 19:47:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Tue, 13 Feb 2018 22:46:32 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Tue, 13 Feb 2018 22:46:32 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 13 Feb 2018 22:46:32 +0000   Fri, 09 Feb 2018 19:47:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Tue, 13 Feb 2018 22:46:32 +0000   Fri, 09 Feb 2018 19:47:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  10.10.97.46
  Hostname:    vhosakot1-we2d86faeb2
Capacity:
 cpu:     2
 memory:  16432780Ki
 pods:    110
Allocatable:
 cpu:     2
 memory:  16330380Ki
 pods:    110
System Info:
 Machine ID:                 9ebc80503a934280b09c657524f9dcb4
 System UUID:                422FA6EA-8678-E963-2F60-4AE55CA628FD
 Boot ID:                    3ad61f99-6f47-4ae6-8a7a-e2c88cc6f2a5
 Kernel Version:             4.4.0-109-generic
 OS Image:                   Ubuntu 16.04.3 LTS
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://1.13.1
 Kubelet Version:            v1.8.4
 Kube-Proxy Version:         v1.8.4
PodCIDR:                     192.168.1.0/24
ExternalID:                  vhosakot1-we2d86faeb2
Non-terminated Pods:         (6 in total)
  Namespace                  Name                       CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                       ------------  ----------  ---------------  -------------
  default                    ccphxvolume-tgb9r          0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                    tea-rc-v8rxv               0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                calico-node-kw9hm          250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                elasticsearch-logging-0    100m (5%)     1 (50%)     0 (0%)           0 (0%)
  kube-system                fluentd-es-v2.0.2-kmbqt    100m (5%)     0 (0%)      200Mi (1%)       500Mi (3%)
  kube-system                kube-proxy-nfkkf           0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  450m (22%)    1 (50%)     200Mi (1%)       500Mi (3%)
Events:         <none>

================
kubectl describe persistentvolumeclaims --all-namespaces
================


================
kubectl describe persistentvolumes --all-namespaces
================


================
kubectl describe poddisruptionbudgets --all-namespaces
================


================
kubectl describe podpreset --all-namespaces
================

the server doesn't have a resource type "podpreset"

================
kubectl describe pods --all-namespaces
================

Name:           ccphxvolume-t972j
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.2.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.2.3
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
      Finished:     Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://133d74a2f67c41276c95642a7ee2cc640806f1e710e38d528b615833a7cc6832
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           ccphxvolume-tgb9r
Namespace:      default
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.1.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.1.2
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:37 +0000
      Finished:     Fri, 09 Feb 2018 19:47:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://9f4d367ff709fa795b1d3aa31b2a97474c57c8235735c1240f14aa1142059e4f
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           ccphxvolume-vlgmz
Namespace:      default
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:58 +0000
Labels:         controller-revision-hash=462338721
                name=ccphxvolume
                pod-template-generation=1
Annotations:    cni.projectcalico.org/podIP=192.168.0.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVer...
Status:         Running
IP:             192.168.0.2
Created By:     DaemonSet/ccphxvolume
Controlled By:  DaemonSet/ccphxvolume
Init Containers:
  iscsi-initiator:
    Container ID:  docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
  hxvolume-copy:
    Container ID:  docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      sh
      -c
      cp /hxcache/hxvolume /hxhostmount/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:38 +0000
      Finished:     Fri, 09 Feb 2018 19:47:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/iscsi from iscsi-volume (rw)
      /hxhostmount from hxvolume-mount (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Containers:
  hxvolume:
    Container ID:  docker://db0ba7b2436c0809e058838a7510f8150b381d0c473ce93a46fa90d77c77ed55
    Image:         registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
    Port:          <none>
    Command:
      /bin/sh
      -c
      while true; do sleep 2; done
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  hxvolume-mount:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
  iscsi-volume:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/iscsi
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-keepalived-vip-fph6b
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 19:01:12 +0000
Labels:         controller-revision-hash=617734581
                name=kube-keepalived-vip
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"kube-keepalived-vip","uid":"41d5383d-10f0-11e8-871b-005056af9e97"...
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/kube-keepalived-vip
Controlled By:  DaemonSet/kube-keepalived-vip
Containers:
  kube-keepalived-vip:
    Container ID:  docker://00aa555e894c867aef4c94268145997e0d4e0fb8e2bc01194d34c2ee495a4137
    Image:         k8s.gcr.io/kube-keepalived-vip:0.11
    Image ID:      docker-pullable://k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
    Port:          <none>
    Args:
      --services-configmap=default/vip-configmap
    State:          Running
      Started:      Tue, 13 Feb 2018 21:56:25 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 13 Feb 2018 21:55:47 +0000
      Finished:     Tue, 13 Feb 2018 21:55:53 +0000
    Ready:          True
    Restart Count:  3
    Environment:
      POD_NAME:       kube-keepalived-vip-fph6b (v1:metadata.name)
      POD_NAMESPACE:  default (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-keepalived-vip-token-2w47w (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  dev:
    Type:  HostPath (bare host directory volume)
    Path:  /dev
  kube-keepalived-vip-token-2w47w:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-keepalived-vip-token-2w47w
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type     Reason      Age                From                            Message
  ----     ------      ----               ----                            -------
  Warning  BackOff     50m (x3 over 51m)  kubelet, vhosakot1-wc80d3e5ab6  Back-off restarting failed container
  Warning  FailedSync  50m (x3 over 51m)  kubelet, vhosakot1-wc80d3e5ab6  Error syncing pod
  Normal   Pulling     50m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  pulling image "k8s.gcr.io/kube-keepalived-vip:0.11"
  Normal   Pulled      50m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Successfully pulled image "k8s.gcr.io/kube-keepalived-vip:0.11"
  Normal   Created     50m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal   Started     50m (x4 over 3h)   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           my-nginx-ingress-controller-d95d4979d-kzgk7
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 21:55:40 +0000
Labels:         app=nginx-ingress
                component=controller
                pod-template-hash=851805358
                release=my-nginx-ingress
Annotations:    checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
                cni.projectcalico.org/podIP=192.168.2.19/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-controller-d95d4979d","uid":"a14fe756-1108-11e8...
Status:         Running
IP:             192.168.2.19
Created By:     ReplicaSet/my-nginx-ingress-controller-d95d4979d
Controlled By:  ReplicaSet/my-nginx-ingress-controller-d95d4979d
Containers:
  nginx-ingress-controller:
    Container ID:  docker://b839ab72ad639b16185c8238762de9213cf04afa7c2aa93a80b6a23d87f36faa
    Image:         quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Image ID:      docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
    Ports:         80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    State:          Running
      Started:      Tue, 13 Feb 2018 21:55:41 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:       my-nginx-ingress-controller-d95d4979d-kzgk7 (v1:metadata.name)
      POD_NAMESPACE:  default (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from my-nginx-ingress-token-9ffd9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  my-nginx-ingress-token-9ffd9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  my-nginx-ingress-token-9ffd9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              51m   default-scheduler               Successfully assigned my-nginx-ingress-controller-d95d4979d-kzgk7 to vhosakot1-wc80d3e5ab6
  Normal  SuccessfulMountVolume  51m   kubelet, vhosakot1-wc80d3e5ab6  MountVolume.SetUp succeeded for volume "my-nginx-ingress-token-9ffd9"
  Normal  Pulled                 50m   kubelet, vhosakot1-wc80d3e5ab6  Container image "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2" already present on machine
  Normal  Created                50m   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal  Started                50m   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           my-nginx-ingress-default-backend-855d89f775-p8jk6
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Tue, 13 Feb 2018 21:55:40 +0000
Labels:         app=nginx-ingress
                component=default-backend
                pod-template-hash=4118459331
                release=my-nginx-ingress
Annotations:    cni.projectcalico.org/podIP=192.168.2.18/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-default-backend-855d89f775","uid":"a151592a-110...
Status:         Running
IP:             192.168.2.18
Created By:     ReplicaSet/my-nginx-ingress-default-backend-855d89f775
Controlled By:  ReplicaSet/my-nginx-ingress-default-backend-855d89f775
Containers:
  nginx-ingress-default-backend:
    Container ID:   docker://59a4fe54449af40c3801c5057d82fe3e17c6e7ea7e4b5b2020e4bb82fe98ff5c
    Image:          k8s.gcr.io/defaultbackend:1.3
    Image ID:       docker-pullable://k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
    Port:           8080/TCP
    State:          Running
      Started:      Tue, 13 Feb 2018 21:55:41 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              51m   default-scheduler               Successfully assigned my-nginx-ingress-default-backend-855d89f775-p8jk6 to vhosakot1-wc80d3e5ab6
  Normal  SuccessfulMountVolume  51m   kubelet, vhosakot1-wc80d3e5ab6  MountVolume.SetUp succeeded for volume "default-token-fc6vg"
  Normal  Pulled                 50m   kubelet, vhosakot1-wc80d3e5ab6  Container image "k8s.gcr.io/defaultbackend:1.3" already present on machine
  Normal  Created                50m   kubelet, vhosakot1-wc80d3e5ab6  Created container
  Normal  Started                50m   kubelet, vhosakot1-wc80d3e5ab6  Started container


Name:           tea-rc-96442
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.2.15/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.2.15
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://1d0b5fdc1a237a0d73e3fb736d0730125d76e145a5926d2c77210aa174e73355
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:15 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tea-rc-fxj6x
Namespace:      default
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.2.14/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.2.14
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://353edef9a7f5eb68513f5a9b463dd7535ebc9abd1a7eec03941101351d84694b
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:14 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tea-rc-v8rxv
Namespace:      default
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Mon, 12 Feb 2018 16:02:12 +0000
Labels:         app=tea
Annotations:    cni.projectcalico.org/podIP=192.168.1.52/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97",...
Status:         Running
IP:             192.168.1.52
Created By:     ReplicationController/tea-rc
Controlled By:  ReplicationController/tea-rc
Containers:
  tea:
    Container ID:   docker://8f455e857f54947f253ab4a470b00b34f55086b42dd75e1a40ef0fc9d0a5ba11
    Image:          nginxdemos/hello
    Image ID:       docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
    Port:           80/TCP
    State:          Running
      Started:      Mon, 12 Feb 2018 16:02:14 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fc6vg (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-fc6vg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fc6vg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           calico-node-2cpcv
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://b9ba3c46ec16b030166cc9085a61ea34f8d39462ee880c5b088ab51332d36a53
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:24 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://bcd4205803fe0b6f9d0d06b6f7d43e23f0bfa48a56a24efe5a118873f4ed6e55
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-g99qc
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:57 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.20
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://80863955dfa346126c501bfde786831975a65e4188f4697348220b10f902b15f
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:58 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://3e78bce5869ac82e5283dd30c852640141f5914d3d2459534e1c94be1ed53470
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:59 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-kw9hm
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=3277287842
                k8s-app=calico-node
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             10.10.97.46
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://153accc108eab50e06c45a0aae1788230477384976ef825a64b55f6114d85856
    Image:          registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      FELIX_LOGSEVERITYSCREEN:            info
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_IPINIPMTU:                    1440
      WAIT_FOR_DATASTORE:                 true
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPENABLED:                true
      FELIX_TYPHAK8SSERVICENAME:          <set to the key 'typha_service_name' of config map 'calico-config'>  Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                 
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
  install-cni:
    Container ID:  docker://185c93be449c0f711ce22e774a8ec43d4fd993cbca35bd4445cb286d9941814f
    Image:         registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-m5t54 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  calico-node-token-m5t54:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-m5t54
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           elasticsearch-logging-0
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:28 +0000
Labels:         controller-revision-hash=elasticsearch-logging-7978c6964c
                k8s-app=elasticsearch-logging
                kubernetes.io/cluster-service=true
                version=v5.6.4
Annotations:    cni.projectcalico.org/podIP=192.168.1.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-00505...
Status:         Running
IP:             192.168.1.4
Created By:     StatefulSet/elasticsearch-logging
Controlled By:  StatefulSet/elasticsearch-logging
Init Containers:
  elasticsearch-logging-init:
    Container ID:  docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
    Image:         registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
    Port:          <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:43 +0000
      Finished:     Fri, 09 Feb 2018 19:47:43 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Containers:
  elasticsearch-logging:
    Container ID:   docker://26e38830a98fcd14580f1349b2e0643db39093b17e7f086e29cce5c07fe75d6a
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
    Ports:          9200/TCP, 9300/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  elasticsearch-logging:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  elasticsearch-logging-token-6cmks:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-logging-token-6cmks
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           elasticsearch-logging-1
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:44 +0000
Labels:         controller-revision-hash=elasticsearch-logging-7978c6964c
                k8s-app=elasticsearch-logging
                kubernetes.io/cluster-service=true
                version=v5.6.4
Annotations:    cni.projectcalico.org/podIP=192.168.2.5/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-00505...
Status:         Running
IP:             192.168.2.5
Created By:     StatefulSet/elasticsearch-logging
Controlled By:  StatefulSet/elasticsearch-logging
Init Containers:
  elasticsearch-logging-init:
    Container ID:  docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
    Image:         registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Image ID:      docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
    Port:          <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 09 Feb 2018 19:47:46 +0000
      Finished:     Fri, 09 Feb 2018 19:47:46 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Containers:
  elasticsearch-logging:
    Container ID:   docker://598eb6f9ef563829dcef2355b141ea06eb162e11e8d429236c9426580c22620b
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
    Ports:          9200/TCP, 9300/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:47 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from elasticsearch-logging-token-6cmks (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  elasticsearch-logging:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  elasticsearch-logging-token-6cmks:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-logging-token-6cmks
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         etcd-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=etcd
              tier=control-plane
Annotations:  kubernetes.io/config.hash=d76e26fba3bf2bfd215eb29011d55250
              kubernetes.io/config.mirror=d76e26fba3bf2bfd215eb29011d55250
              kubernetes.io/config.seen=2018-02-09T19:46:22.807466634Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  etcd:
    Container ID:  docker://11eb72aba33cbf28f02623a3e01c996da3542f375262a338c769dc91417a8865
    Image:         gcr.io/google_containers/etcd-amd64:3.0.17
    Image ID:      docker-pullable://gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
    Port:          <none>
    Command:
      etcd
      --listen-client-urls=http://127.0.0.1:2379
      --advertise-client-urls=http://127.0.0.1:2379
      --data-dir=/var/lib/etcd
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://127.0.0.1:2379/health delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:    <none>
    Mounts:
      /var/lib/etcd from etcd (rw)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  etcd:
    Type:        HostPath (bare host directory volume)
    Path:        /var/lib/etcd
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           fluentd-es-v2.0.2-9bmjw
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=1193446001
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                pod-template-generation=1
                version=v2.0.2
Annotations:    cni.projectcalico.org/podIP=192.168.2.2/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e9...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             192.168.2.2
Created By:     DaemonSet/fluentd-es-v2.0.2
Controlled By:  DaemonSet/fluentd-es-v2.0.2
Containers:
  fluentd-es:
    Container ID:   docker://5a66378b3e72ce2fd8df7b7181818302c86b7cb85ce53c1ac81410c171169e70
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:36 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from fluentd-es-token-zw92g (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
  varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
  libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
  fluentd-es-token-zw92g:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  fluentd-es-token-zw92g
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           fluentd-es-v2.0.2-kmbqt
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=1193446001
                k8s-app=fluentd-es
                kubernetes.io/cluster-service=true
                pod-template-generation=1
                version=v2.0.2
Annotations:    cni.projectcalico.org/podIP=192.168.1.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e9...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             192.168.1.3
Created By:     DaemonSet/fluentd-es-v2.0.2
Controlled By:  DaemonSet/fluentd-es-v2.0.2
Containers:
  fluentd-es:
    Container ID:   docker://c650008103084454c1d238468843c301b568d1919e3ae02c60338e1303189954
    Image:          registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
    Port:           <none>
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:39 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  500Mi
    Requests:
      cpu:     100m
      memory:  200Mi
    Environment:
      FLUENTD_ARGS:  --no-supervisor -q
    Mounts:
      /etc/fluent/config.d from config-volume (rw)
      /host/lib from libsystemddir (ro)
      /var/lib/docker/containers from varlibdockercontainers (ro)
      /var/log from varlog (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from fluentd-es-token-zw92g (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  varlog:
    Type:  HostPath (bare host directory volume)
    Path:  /var/log
  varlibdockercontainers:
    Type:  HostPath (bare host directory volume)
    Path:  /var/lib/docker/containers
  libsystemddir:
    Type:  HostPath (bare host directory volume)
    Path:  /usr/lib64
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      fluentd-es-config-v0.1.1
    Optional:  false
  fluentd-es-token-zw92g:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  fluentd-es-token-zw92g
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kibana-logging-767cf49759-f8zjt
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:28 +0000
Labels:         k8s-app=kibana-logging
                pod-template-hash=3237905315
Annotations:    cni.projectcalico.org/podIP=192.168.2.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kibana-logging-767cf49759","uid":"fcf34643-0dd1-11e8-871b-00...
Status:         Running
IP:             192.168.2.4
Created By:     ReplicaSet/kibana-logging-767cf49759
Controlled By:  ReplicaSet/kibana-logging-767cf49759
Containers:
  kibana-logging:
    Container ID:   docker://32e30c8b573b980690ec999dbb4f981dec9caadfb001356bd428b26187c1a451
    Image:          registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Image ID:       docker-pullable://registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
    Port:           5601/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:44 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kxl84 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-kxl84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-kxl84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         kube-apiserver-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-apiserver
              tier=control-plane
Annotations:  kubernetes.io/config.hash=3f93d95099662b6d548520f6873d8454
              kubernetes.io/config.mirror=3f93d95099662b6d548520f6873d8454
              kubernetes.io/config.seen=2018-02-09T19:46:22.807479364Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-apiserver:
    Container ID:  docker://a04e800ebb80645cd9826489e7d56e778b23f495984e5d32822831c1178e0915
    Image:         gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
    Port:          <none>
    Command:
      kube-apiserver
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --allow-privileged=true
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --requestheader-username-headers=X-Remote-User
      --requestheader-group-headers=X-Remote-Group
      --service-cluster-ip-range=10.96.0.0/12
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --insecure-port=0
      --requestheader-allowed-names=front-proxy-client
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      --secure-port=6443
      --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --advertise-address=10.10.97.20
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --enable-bootstrap-token-auth=true
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --authorization-mode=Node,RBAC
      --etcd-servers=http://127.0.0.1:2379
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://127.0.0.1:6443/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  k8s-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/pki
  ca-certs:
    Type:        HostPath (bare host directory volume)
    Path:        /etc/ssl/certs
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:         kube-controller-manager-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-controller-manager
              tier=control-plane
Annotations:  kubernetes.io/config.hash=ab7c88cd37d9d92c34bb35c6a377a96f
              kubernetes.io/config.mirror=ab7c88cd37d9d92c34bb35c6a377a96f
              kubernetes.io/config.seen=2018-02-09T19:46:22.807482481Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-controller-manager:
    Container ID:  docker://952f4f63dea04d6c2c7ed47c7c6d4a720b7eea4f69f43d3daaf0113ab685163c
    Image:         gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
    Port:          <none>
    Command:
      kube-controller-manager
      --address=127.0.0.1
      --controllers=*,bootstrapsigner,tokencleaner
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --leader-elect=true
      --use-service-account-credentials=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --allocate-node-cidrs=true
      --cluster-cidr=192.168.0.0/16
      --node-cidr-mask-size=24
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get http://127.0.0.1:10252/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  k8s-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/pki
  ca-certs:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/ssl/certs
  kubeconfig:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/kubernetes/controller-manager.conf
  flexvolume-dir:
    Type:        HostPath (bare host directory volume)
    Path:        /usr/libexec/kubernetes/kubelet-plugins/volume/exec
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           kube-dns-545bc4bfd4-rzpz4
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:47:13 +0000
Labels:         k8s-app=kube-dns
                pod-template-hash=1016706980
Annotations:    cni.projectcalico.org/podIP=192.168.0.4/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kube-dns-545bc4bfd4","uid":"fcf396ae-0dd1-11e8-871b-005056af...
Status:         Running
IP:             192.168.0.4
Created By:     ReplicaSet/kube-dns-545bc4bfd4
Controlled By:  ReplicaSet/kube-dns-545bc4bfd4
Containers:
  kubedns:
    Container ID:  docker://585a3048cfcdc2498ba543ef02124e15c7c7e8ea79f84f5df37d87e46b6cfb19
    Image:         gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
    Ports:         10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
  dnsmasq:
    Container ID:  docker://ead0625a9610ac4f66bee25ee8ed4d6b5418bc31d7ac8b7e6eb3d38b339b3e7e
    Image:         gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
    Ports:         53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:45 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
  sidecar:
    Container ID:  docker://313b3d5c54c63059e63c3bc2b8d23d2902e8033a007ebd78f015ae22556f9603
    Image:         gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Image ID:      docker-pullable://gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
    Port:          10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:46 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-66tfx (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
  kube-dns-token-66tfx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-dns-token-66tfx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           kube-proxy-8vlsv
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:46:57 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.20
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://bf6367debcebe328805c54a362b8767b8e8810c327094c53b98ea7a59c4e5514
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:58 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-proxy-nfkkf
Namespace:      kube-system
Node:           vhosakot1-we2d86faeb2/10.10.97.46
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.46
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://d47a6b44a292ed53cc15d6e72546c50f08a3037a306a847e63f39660af3b8a4a
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           kube-proxy-q8ng8
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:47:23 +0000
Labels:         controller-revision-hash=514127771
                k8s-app=kube-proxy
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","api...
Status:         Running
IP:             10.10.97.62
Created By:     DaemonSet/kube-proxy
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3ac1e13578a22c5fe5e2f3c8209af657906680224d8faf34cf95880e1a320243
    Image:         gcr.io/google_containers/kube-proxy-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
    Port:          <none>
    Command:
      /usr/local/bin/kube-proxy
      --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      --cluster-cidr=192.168.0.0/16
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-ktcv9 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:  HostPath (bare host directory volume)
    Path:  /run/xtables.lock
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  kube-proxy-token-ktcv9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-ktcv9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:         kube-scheduler-vhosakot1-m51b5b468be
Namespace:    kube-system
Node:         vhosakot1-m51b5b468be/10.10.97.20
Start Time:   Fri, 09 Feb 2018 19:46:27 +0000
Labels:       component=kube-scheduler
              tier=control-plane
Annotations:  kubernetes.io/config.hash=e554495c6f8701f21accd04866090b05
              kubernetes.io/config.mirror=e554495c6f8701f21accd04866090b05
              kubernetes.io/config.seen=2018-02-09T19:46:22.807485209Z
              kubernetes.io/config.source=file
              scheduler.alpha.kubernetes.io/critical-pod=
Status:       Running
IP:           10.10.97.20
Containers:
  kube-scheduler:
    Container ID:  docker://dc6d33ae2ff92d6b18190f6edd6d943f352b37dabb80d6785d1e40238ec1530d
    Image:         gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
    Image ID:      docker-pullable://gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
    Port:          <none>
    Command:
      kube-scheduler
      --address=127.0.0.1
      --leader-elect=true
      --kubeconfig=/etc/kubernetes/scheduler.conf
    State:          Running
      Started:      Fri, 09 Feb 2018 19:46:28 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get http://127.0.0.1:10251/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kubeconfig:
    Type:        HostPath (bare host directory volume)
    Path:        /etc/kubernetes/scheduler.conf
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>


Name:           kubernetes-dashboard-7798c48646-rjmch
Namespace:      kube-system
Node:           vhosakot1-m51b5b468be/10.10.97.20
Start Time:     Fri, 09 Feb 2018 19:47:12 +0000
Labels:         k8s-app=kubernetes-dashboard
                pod-template-hash=3354704202
Annotations:    cni.projectcalico.org/podIP=192.168.0.3/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kubernetes-dashboard-7798c48646","uid":"fcf3720c-0dd1-11e8-8...
Status:         Running
IP:             192.168.0.3
Created By:     ReplicaSet/kubernetes-dashboard-7798c48646
Controlled By:  ReplicaSet/kubernetes-dashboard-7798c48646
Containers:
  kubernetes-dashboard:
    Container ID:  docker://15d41739dac144360b50543a9b2ed5269720107fc11c5e19c2c9a61716c4b626
    Image:         k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Image ID:      docker-pullable://k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
    Port:          8443/TCP
    Args:
      --auto-generate-certificates
    State:          Running
      Started:      Fri, 09 Feb 2018 19:47:42 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kubernetes-dashboard-token-bjcwc (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
  tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  kubernetes-dashboard-token-bjcwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-token-bjcwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           tiller-deploy-546cf9696c-w4kq6
Namespace:      kube-system
Node:           vhosakot1-wc80d3e5ab6/10.10.97.62
Start Time:     Fri, 09 Feb 2018 19:49:23 +0000
Labels:         app=helm
                name=tiller
                pod-template-hash=1027952527
Annotations:    cni.projectcalico.org/podIP=192.168.2.6/32
                kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"tiller-deploy-546cf9696c","uid":"54347a31-0dd2-11e8-871b-005...
Status:         Running
IP:             192.168.2.6
Created By:     ReplicaSet/tiller-deploy-546cf9696c
Controlled By:  ReplicaSet/tiller-deploy-546cf9696c
Containers:
  tiller:
    Container ID:   docker://9aa05a99a4fe19195842dbbe5f406912fdd380339d496b401a690d15d6033379
    Image:          gcr.io/kubernetes-helm/tiller:v2.7.2
    Image ID:       docker-pullable://gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
    Port:           44134/TCP
    State:          Running
      Started:      Fri, 09 Feb 2018 19:49:27 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from tiller-token-chfrl (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  tiller-token-chfrl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tiller-token-chfrl
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>

================
kubectl describe podsecuritypolicies --all-namespaces
================


================
kubectl describe podtemplates --all-namespaces
================


================
kubectl describe replicasets --all-namespaces
================

Name:           my-nginx-ingress-controller-d95d4979d
Namespace:      default
Selector:       app=nginx-ingress,component=controller,pod-template-hash=851805358,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=controller
                pod-template-hash=851805358
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-controller
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=nginx-ingress
                    component=controller
                    pod-template-hash=851805358
                    release=my-nginx-ingress
  Annotations:      checksum/config=98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
  Service Account:  my-nginx-ingress
  Containers:
   nginx-ingress-controller:
    Image:  quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
    Ports:  80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=default/my-nginx-ingress-default-backend
      --election-id=ingress-controller-leader
      --ingress-class=nginx
      --configmap=default/my-nginx-ingress-controller
    Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:        (v1:metadata.name)
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:           <none>
  Volumes:            <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  51m   replicaset-controller  Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7


Name:           my-nginx-ingress-default-backend-855d89f775
Namespace:      default
Selector:       app=nginx-ingress,component=default-backend,pod-template-hash=4118459331,release=my-nginx-ingress
Labels:         app=nginx-ingress
                component=default-backend
                pod-template-hash=4118459331
                release=my-nginx-ingress
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/my-nginx-ingress-default-backend
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx-ingress
           component=default-backend
           pod-template-hash=4118459331
           release=my-nginx-ingress
  Containers:
   nginx-ingress-default-backend:
    Image:        k8s.gcr.io/defaultbackend:1.3
    Port:         8080/TCP
    Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  51m   replicaset-controller  Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6


Name:           calico-typha-65b7467b56
Namespace:      kube-system
Selector:       k8s-app=calico-typha,pod-template-hash=2163023612
Labels:         k8s-app=calico-typha
                pod-template-hash=2163023612
Annotations:    deployment.kubernetes.io/desired-replicas=0
                deployment.kubernetes.io/max-replicas=0
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/calico-typha
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-typha
                    pod-template-hash=2163023612
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  calico-node
  Containers:
   calico-typha:
    Image:      registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
    Port:       5473/TCP
    Liveness:   http-get http://:9098/liveness delay=30s timeout=1s period=30s #success=1 #failure=3
    Readiness:  http-get http://:9098/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TYPHA_LOGSEVERITYSCREEN:          info
      TYPHA_LOGFILEPATH:                none
      TYPHA_LOGSEVERITYSYS:             none
      TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
      TYPHA_DATASTORETYPE:              kubernetes
      TYPHA_HEALTHENABLED:              true
    Mounts:                             <none>
  Volumes:                              <none>
Events:                                 <none>


Name:           kibana-logging-767cf49759
Namespace:      kube-system
Selector:       k8s-app=kibana-logging,pod-template-hash=3237905315
Labels:         k8s-app=kibana-logging
                pod-template-hash=3237905315
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kibana-logging
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  k8s-app=kibana-logging
           pod-template-hash=3237905315
  Containers:
   kibana-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
    Port:   5601/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      ELASTICSEARCH_URL:         http://elasticsearch-logging:9200
      XPACK_MONITORING_ENABLED:  false
      XPACK_SECURITY_ENABLED:    false
    Mounts:                      <none>
  Volumes:                       <none>
Events:                          <none>


Name:           kube-dns-545bc4bfd4
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=1016706980
Labels:         k8s-app=kube-dns
                pod-template-hash=1016706980
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kube-dns
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=1016706980
  Service Account:  kube-dns
  Containers:
   kubedns:
    Image:  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   dnsmasq:
    Image:  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   sidecar:
    Image:  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           kubernetes-dashboard-7798c48646
Namespace:      kube-system
Selector:       k8s-app=kubernetes-dashboard,pod-template-hash=3354704202
Labels:         k8s-app=kubernetes-dashboard
                pod-template-hash=3354704202
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kubernetes-dashboard
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kubernetes-dashboard
                    pod-template-hash=3354704202
  Service Account:  kubernetes-dashboard
  Containers:
   kubernetes-dashboard:
    Image:  k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
    Port:   8443/TCP
    Args:
      --auto-generate-certificates
    Liveness:     http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
  Volumes:
   kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
   tmp-volume:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
Events:      <none>


Name:           tiller-deploy-546cf9696c
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=1027952527
Labels:         app=helm
                name=tiller
                pod-template-hash=1027952527
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/tiller-deploy
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=helm
                    name=tiller
                    pod-template-hash=1027952527
  Service Account:  tiller
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>


Name:           tiller-deploy-5b9d65c7f
Namespace:      kube-system
Selector:       app=helm,name=tiller,pod-template-hash=165821739
Labels:         app=helm
                name=tiller
                pod-template-hash=165821739
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/tiller-deploy
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=helm
           name=tiller
           pod-template-hash=165821739
  Containers:
   tiller:
    Image:      gcr.io/kubernetes-helm/tiller:v2.7.2
    Port:       44134/TCP
    Liveness:   http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  0
    Mounts:                <none>
  Volumes:                 <none>
Events:                    <none>

================
kubectl describe replicationcontrollers --all-namespaces
================

Name:         tea-rc
Namespace:    default
Selector:     app=tea
Labels:       app=tea
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=tea
  Containers:
   tea:
    Image:        nginxdemos/hello
    Port:         80/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>

================
kubectl describe resourcequotas --all-namespaces
================


================
kubectl describe rolebindings --all-namespaces
================

Name:         my-nginx-ingress
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
Role:
  Kind:  Role
  Name:  my-nginx-ingress
Subjects:
  Kind            Name              Namespace
  ----            ----              ---------
  ServiceAccount  my-nginx-ingress  default


Name:         kubeadm:bootstrap-signer-clusterinfo
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  kubeadm:bootstrap-signer-clusterinfo
Subjects:
  Kind  Name              Namespace
  ----  ----              ---------
  User  system:anonymous  


Name:         system:controller:bootstrap-signer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system:controller:bootstrap-signer
Subjects:
  Kind            Name              Namespace
  ----            ----              ---------
  ServiceAccount  bootstrap-signer  kube-system


Name:         kubernetes-dashboard-minimal
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"RoleBinding","metadata":{"annotations":{},"name":"kubernetes-dashboard-minimal","namespace":"kube-...
Role:
  Kind:  Role
  Name:  kubernetes-dashboard-minimal
Subjects:
  Kind            Name                  Namespace
  ----            ----                  ---------
  ServiceAccount  kubernetes-dashboard  kube-system


Name:         system::leader-locking-kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system::leader-locking-kube-controller-manager
Subjects:
  Kind            Name                     Namespace
  ----            ----                     ---------
  ServiceAccount  kube-controller-manager  kube-system


Name:         system::leader-locking-kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system::leader-locking-kube-scheduler
Subjects:
  Kind            Name            Namespace
  ----            ----            ---------
  ServiceAccount  kube-scheduler  kube-system


Name:         system:controller:bootstrap-signer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system:controller:bootstrap-signer
Subjects:
  Kind            Name              Namespace
  ----            ----              ---------
  ServiceAccount  bootstrap-signer  kube-system


Name:         system:controller:cloud-provider
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system:controller:cloud-provider
Subjects:
  Kind            Name            Namespace
  ----            ----            ---------
  ServiceAccount  cloud-provider  kube-system


Name:         system:controller:token-cleaner
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  Role
  Name:  system:controller:token-cleaner
Subjects:
  Kind            Name           Namespace
  ----            ----           ---------
  ServiceAccount  token-cleaner  kube-system

================
kubectl describe roles --all-namespaces
================

Name:         my-nginx-ingress
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names                     Verbs
  ---------   -----------------  --------------                     -----
  configmaps  []                 []                                 [get create]
  configmaps  []                 [ingress-controller-leader-nginx]  [get update]
  endpoints   []                 []                                 [create get update]
  namespaces  []                 []                                 [get]
  pods        []                 []                                 [get]
  secrets     []                 []                                 [get]


Name:         kubeadm:bootstrap-signer-clusterinfo
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 [cluster-info]  [get]


Name:         system:controller:bootstrap-signer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 []              [get list watch]
  configmaps  []                 [cluster-info]  [update]
  events      []                 []              [create patch update]


Name:         extension-apiserver-authentication-reader
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources   Non-Resource URLs  Resource Names                        Verbs
  ---------   -----------------  --------------                        -----
  configmaps  []                 [extension-apiserver-authentication]  [get]


Name:         kubernetes-dashboard-minimal
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1","kind":"Role","metadata":{"annotations":{},"name":"kubernetes-dashboard-minimal","namespace":"kube-system"...
PolicyRule:
  Resources       Non-Resource URLs  Resource Names                     Verbs
  ---------       -----------------  --------------                     -----
  configmaps      []                 []                                 [create]
  configmaps      []                 [kubernetes-dashboard-settings]    [get update]
  secrets         []                 []                                 [create]
  secrets         []                 [kubernetes-dashboard-certs]       [get update delete]
  secrets         []                 [kubernetes-dashboard-key-holder]  [get update delete]
  services        []                 [heapster]                         [proxy]
  services/proxy  []                 [heapster]                         [get]
  services/proxy  []                 [http:heapster:]                   [get]
  services/proxy  []                 [https:heapster:]                  [get]


Name:         system::leader-locking-kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources   Non-Resource URLs  Resource Names             Verbs
  ---------   -----------------  --------------             -----
  configmaps  []                 []                         [watch]
  configmaps  []                 [kube-controller-manager]  [get update]


Name:         system::leader-locking-kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources   Non-Resource URLs  Resource Names    Verbs
  ---------   -----------------  --------------    -----
  configmaps  []                 []                [watch]
  configmaps  []                 [kube-scheduler]  [get update]


Name:         system:controller:bootstrap-signer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  secrets    []                 []              [get list watch]


Name:         system:controller:cloud-provider
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 []              [create get list watch]


Name:         system:controller:token-cleaner
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  events     []                 []              [create patch update]
  secrets    []                 []              [delete get list watch]

================
kubectl describe secrets --all-namespaces
================

Name:         cafe-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
tls.crt:  1428 bytes
tls.key:  1675 bytes


Name:         default-token-fc6vg
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=fd020fda-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tZmM2dmciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZkMDIwZmRhLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.xZMhVm3FIU7OUqp-d42EjsNPie23TAYV9IxOFqu9T6bdSl6mIDaV62tX6WJ4Cwp3h0gvwlMrtbN0CdOcMMcxE6TTgP_pMzvZt0TsJXVJqS1W4ESILAb8dwsloxyKuRaj7SFgYO4FY2R1w6gi-BqojOb2mspFtVe_czcyI1Tm2UHj5lDHhwHohz7dmHkDPCc3lwCFMFlEHdLmXLNMsSfdwqqjNryIBVXjPZQ0yB-hY-IdnLtlKOi2I1T-zr-m0F147y7RAmfw2pV76FivAnRSXvp58pkrLld9BnL4K9ypfokw82yAdNEKEnLohf9Y4t5v_6mQkqDDrlz61BJ-lMzaWg


Name:         kube-keepalived-vip-token-2w47w
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=kube-keepalived-vip
              kubernetes.io/service-account.uid=3e12798d-10d2-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Imt1YmUta2VlcGFsaXZlZC12aXAtdG9rZW4tMnc0N3ciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3ViZS1rZWVwYWxpdmVkLXZpcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjNlMTI3OThkLTEwZDItMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Omt1YmUta2VlcGFsaXZlZC12aXAifQ.a5INIdF8nMRZFWyILPVOesdPpBsaSFIAAhG2xDUknqJbcezGnZrlmTWRYpvPZIJbllfBWWxtAlWjiBdrEWxOTvCNqEKXZr-RRYyjgVY6BfzODzUrw8AEailiRGB5nxfDu5Fu6NkJA2qxy6s_LRHoTuYv6pvqAmbq9dEAeWBKq8FTBfjn1YoTnhptG2JKvoFhtaULoKGmYTuBOk8mTM_MJF76tGxIwJ3gaQ0NtBo3RjUhdWy4bZS2WNY3NQdtr5HmXi_quETV7ZP4ujRcz5CHyoVtu0_TXHde1wgGguD8TswwX5ivPFmkt1FGty5kFCaVNl7gmtkQcKflAsWtcQYFMg


Name:         my-nginx-ingress-token-9ffd9
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=my-nginx-ingress
              kubernetes.io/service-account.uid=a144ef5b-1108-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im15LW5naW54LWluZ3Jlc3MtdG9rZW4tOWZmZDkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibXktbmdpbngtaW5ncmVzcyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImExNDRlZjViLTExMDgtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Om15LW5naW54LWluZ3Jlc3MifQ.H7b9kiVQhW7lIu9Q5pnb5k1DZjEk3ywQSQ8hFp7AziP_67eXCO01VV5UH4d9Q18u-OEcY1fuK12i_qAxVpBB0Pni_5HI0jWvgc9riGhsqF88qbQ_hG_Rh85JBZRGnvX0lwZD8HSop2oRPntsEN1FGJ9TdGYM5xOYMzro9NaApcGdJgq2kV-QmV1tvscALojssBsOfbhx7LMxqbnTRPxFoSOZKqSvVPSdm3J1ec8kbG_55gi7Nx8YHxgnNmou9tXFPewdo_dHiinn8DLebNujkSZlpAW8Q7k1qX3CetoQARHtE16mxtcGIqpJ7tqc1QpqqdB5A_B3Goq7-RTrb3GE6A
ca.crt:     1025 bytes


Name:         default-token-7g442
Namespace:    kube-public
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=fcfc2c5b-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXB1YmxpYyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLTdnNDQyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmY2ZjMmM1Yi0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1wdWJsaWM6ZGVmYXVsdCJ9.u3kgMwLOOBvZouec4b0dkW_mbX_NuPshmSECB7mdMvhdEF3LPxTakA34cA0Suo3ilImsLLbl_E1CYvnlT3tJtfscAxupWYCayTzqobjV_tKP0O_uzP_aOWJQwJBktOJaIkaR6NQR2b5u3axJQ-Qb302X2Zl3zmdh2PrNmHSVOFkDetmkIKwCX7yhY0JIcmx6ZIuNWOMXiUIpIfhK7mmsjDbZFMcwwbFDwAgiT2kpVKtb_tnzeaTDqPz3RFnAnAw1msIDm22v0pWJn-_peWN-m2k9NCAKGZOIcVKhi5L6o5bFfBfZ-TP-mxG-zphLhqAbx29ku7tSOxCDnQodMG77JA


Name:         attachdetach-controller-token-pdmt4
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=attachdetach-controller
              kubernetes.io/service-account.uid=fbd9465c-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlci10b2tlbi1wZG10NCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZiZDk0NjVjLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphdHRhY2hkZXRhY2gtY29udHJvbGxlciJ9.dVwyoU79DMwAY0sMbVkSrV4zPPbRuQIXNdgKh4TENwVaR1ut0BbKTHugrtqSbnqGAVgVPOhBhtZYVkAKIwn_ZQ68Y83gUkH7LXitprlxMR8j8icvL1HROAHDUK6vMT0Do0DpJXDvHkhHOKPV3E4lS0uKHVQdL8qT6lUDo-2MVPSh-A-5SiHur2geVE4gwwjKI2D4m40l5OHkF0Oq4ir5cowESswDALERclBlno4REr0bodYkVeqjKW5wfejfq0u6APvKl_CBYTkbRFIQZERrs0ncj7pftat6cOrqC9oAqWN234oWD9DPAhyBZb-9RjuPVhkLiUg-BngQtxfW-5uBXw


Name:         bootstrap-signer-token-nhshd
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=bootstrap-signer
              kubernetes.io/service-account.uid=f495ec61-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJib290c3RyYXAtc2lnbmVyLXRva2VuLW5oc2hkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImJvb3RzdHJhcC1zaWduZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNDk1ZWM2MS0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Ym9vdHN0cmFwLXNpZ25lciJ9.nHpGBxCRiAi0_PQ5RXqaZ22oY0LzRZ2aJYPZlNiJs2GksTu9Mu41yrADE1_8M5qGtbRg0xwSws-xDMDzfzO9GAw8oF7ov2hPK_CjdCgYMxZObIn_cD5V1DhKLCTRRv11twU4iW55btGGtbUxUka8hpteeypHp5SllMScrkq1_oSKzlCZD4gSj60Fn4kHETldL2fvHr6GMBuPESA2IK1-C0lM7ukUDKVZw0U7j7tlUu91JYt49jax2_l16NT6o7wBj4yGFfba7JbLqJSBI6axQmLpgTfk5ZMGnkyFWXbNaLRT22J9HnLlLLiHrXIJ6n1csM9xuPKETDOCta_Gp6HAxg


Name:         calico-node-token-m5t54
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=calico-node
              kubernetes.io/service-account.uid=f5b1b250-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjYWxpY28tbm9kZS10b2tlbi1tNXQ1NCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJjYWxpY28tbm9kZSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY1YjFiMjUwLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpjYWxpY28tbm9kZSJ9.y4koJVYP5jGCpzfZqFJqSYkljQB6kwlH8ginVNaKqNNsnBn_FDpaeFrzVVpxNpobg8jGUD8Ka1bygYC2cfYW_Aso3y99UxSF9-eTiaDpBedYUGNPaH5-WlG4mLS4sX1_HmMIUVYYxBO0occJXUEtXCydkT8D0tm476VrpxeOJK7k6Eqci-O8_auVc6kxJf4257Vuxdg-VCj7_WqB9mMj7IOLLDV-wkI-YOCG8j12q_L1yEXmNXy5vi8xiZhX_zfDs4sW5RZ-snEOimtnmLRWCreLUBJCfVLESUferm_0cbODgVTDncxMjIbFrmWxIGccodtM3gGMNDjvf9FztrsApw


Name:         certificate-controller-token-sxs2h
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=certificate-controller
              kubernetes.io/service-account.uid=f49322b2-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjZXJ0aWZpY2F0ZS1jb250cm9sbGVyLXRva2VuLXN4czJoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNlcnRpZmljYXRlLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNDkzMjJiMi0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2VydGlmaWNhdGUtY29udHJvbGxlciJ9.Z5vR6FoNFyvJmEprk-mA29aRkaWTRI3UESkt2WFLh6asCznIHLkUP8y0ee5cJNO09YTLf1g1yxZJpB5slZKBpXWdiiQsduwP0vkyZjlFeUKYMT_Ba8o8EcfcmLUn7mLxirNOjlfSczIAt9kXMbg4fKvCVagElE0zF6_pqua3chuQq0ByUvI-mdKG31tvxdtYhVle3tpyuPeQRDyHBmVyH6LNmRduPrv1AJExL6zaV0WKGLkr8R2cjm_PKhDTzJnHpbyAhm3F9Nyp_81nH18zV096Nnk4RrcBDfdk2u1YJaNdyhYcgSPHp2hRF3NoidvlKqKQjL_ftsVWGeZ_GQZEaw


Name:         cronjob-controller-token-4dlm7
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=cronjob-controller
              kubernetes.io/service-account.uid=f4813661-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjcm9uam9iLWNvbnRyb2xsZXItdG9rZW4tNGRsbTciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY3JvbmpvYi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjQ4MTM2NjEtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmNyb25qb2ItY29udHJvbGxlciJ9.bN5htdA10GcYS4Qb4ieoU-mki-exAZC8vOOy4tg44X1naCmkxWvBlMpP48EeeeLuQuyJ2hDKGWw7emoFn_wGwqoLWTSvx4uL24S8Q29Pg5IujYgp8gzH39aYDO1k6PyW4jl8_vYODflnZ1nwWPnjr48wHTA5CJo2Uvul2rKy9ylHEofB5RVXqGzIORTH4mOTaAXSRnf64OYn6Lk4vDc--egQqksoLBQlhq4DzkaOb1aBgXHwfmxl-9-gtl0aiNfuALe58AmMmeZtySqPYJdqFHJUqti6sQpcsRvb_slNLktNfvPmAn4E0W5-LRSWgFPhjo0Z73d-9cC-xRKla1WGQQ


Name:         daemon-set-controller-token-4jwtx
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=daemon-set-controller
              kubernetes.io/service-account.uid=fb66d5f3-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYWVtb24tc2V0LWNvbnRyb2xsZXItdG9rZW4tNGp3dHgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFlbW9uLXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmI2NmQ1ZjMtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhZW1vbi1zZXQtY29udHJvbGxlciJ9.W-xblhSuNHwbSd5bzSKqFyEzmfa3z_KuChLPKxUmgmLnIRUpcs_sH7Wnfrs860DwbGSkji-AkCv1HvJJsNjwDwSYPlvkSYY3SfNCuvSxxiM1uMloFh8KwqHoSsDb7jjX_ZuBagCdJUK7IGt61JwLz6piw-HreOn_mfANLGiaMzICH5IhhVRHOn2rgQcPheS6vtto_E2Sl8O9nZkOWAEmhBYsBeTt0AgSWECljnGCWQiVEpwpQkL3elF0XvqvJNAWCZOoz29zc6BjlhYx4iCmeRTlVCAyejNtTMcp7-P_Sb-pjcvc-Zd7N6T9x4MhSwZkY8-Zhww9MnQiKr8lH4shOA


Name:         default-token-kxl84
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=fd0d0d02-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWt4bDg0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmZDBkMGQwMi0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.CnHM5_DBBuPZFVM7erByZILBQUR5xuM6DexfxuCKE4jJD6yTUR6kBBjl3xRrI0nDAT8w-VipcJL4c-3swFDv6KiRG_DLAJWuyGn3aojpUE2ef1674oek0HYSZy_5JcljtjB1kJpvBe5J196TMql8f0nmkbbMYYP77BQNfQz_APXBJbypSTI1Gzdf4TsFo8IJkuRbTBxvEed0FKlPqmZTzcN9nzS3PafOwUVS7PHkJMa6WYVTbd_6Soz7sZIAEQXSbiYjkpwNbo-406VQlslZU6V2N0Cme1D9UHHnyiaCoTx5vfltKJT7lZb7dxaxXyNiSIK_mSnT7VPUB0tbDllrfw


Name:         deployment-controller-token-8gz7n
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=deployment-controller
              kubernetes.io/service-account.uid=fb170767-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tOGd6N24iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmIxNzA3NjctMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.BxlSsZv-8uwNqN4uDTNna1JJzIB5iw3qdl4en8vq7dx23-tKcFxIYRdxwZ8gVjR9w9uEBT1ykNcflfXhv5vQTuOSyS6nw7ILmZeVS-YasYOZ3m2J_wjwcJP4H_-ehbVVvErP15WeoPVhfEB9QAqRsVg-KrQKVOnTq-BZJBOYEJRgzgRJhO-urAei0zfW3oaRfbFmz3sqsiCBU2uqUjr1LbItrrZ5HPhJQwo6wOYKVCyGy0lZCYcYLpTo3ygT-hyx8RlgdTjD2trGctTH7ThPmjXiJLKKOXhs-0bytHGG5eSbFuUT5qhApErejAzra8Sb7vdwpbFcybuHgCm8xwmHfg
ca.crt:     1025 bytes


Name:         disruption-controller-token-sndts
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=disruption-controller
              kubernetes.io/service-account.uid=fb8cfe9e-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkaXNydXB0aW9uLWNvbnRyb2xsZXItdG9rZW4tc25kdHMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGlzcnVwdGlvbi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmI4Y2ZlOWUtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRpc3J1cHRpb24tY29udHJvbGxlciJ9.ovV6mqaunf7ZkA3J9O9XR4TnKZYXFtWwjTL2SxLsRuQfpUTu2VLAeOOXS_i6Tvp3u3c98wNPwDq5m807WbIKcGQe4eAmvRqAGoGHkjktRZCThcrK57DA9ugvCfIv1crHRjqOZVjobgfHtT08rgDCrgemvV4zcwKwg1u_onIDEiabGTtMk5JmIEvQCfiYXgQN9dOk1e8dzfkiRC7GtOGVYVb6nz4XlqJp7vIfkD9SNS2CxDNHvAjNlfYRLF11jKe_RjrMdx4acflDPvPWMMf_xOi9b8dOyQuaDMcRETGV3X-xwikja_m9AHnag3qposYOIpFLYjOOUaN1QG-wbc0P3g


Name:         elasticsearch-logging-token-6cmks
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=elasticsearch-logging
              kubernetes.io/service-account.uid=f6540f35-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJlbGFzdGljc2VhcmNoLWxvZ2dpbmctdG9rZW4tNmNta3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZWxhc3RpY3NlYXJjaC1sb2dnaW5nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjY1NDBmMzUtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmVsYXN0aWNzZWFyY2gtbG9nZ2luZyJ9.v37bvcPIELO9cmbGofNIU7FnpepU-ztUm51gmmCLLQfFQWGjNvOdcGUSqfz1t9mxSYS5PZIaP9Zh6nEpRtcLhu0oAFXUByjDnWkU-56nx8Wa0Ex3WOLykujD1475Iw6L4emEedZmzY_t6u4EjHm6YaqajrFBSkBS9n667WwcsDoXCJp8q-J2U_IXpDL_34exFmYQV1JKBMCOSRp8kKZ0286dEPUrLVsOCJ7zWbzwsKSzhHA8lnSI-pbrOWsflz0o1JJCLVZPeQhySKWT2GDx1wDY6DxAG-FCe4x0LJfWS5A55eUE92o35nKT_L9NgRqtJ6cHxKBT6HB3IqXrtgEZQg


Name:         endpoint-controller-token-6xqjn
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=endpoint-controller
              kubernetes.io/service-account.uid=fb214d94-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJlbmRwb2ludC1jb250cm9sbGVyLXRva2VuLTZ4cWpuIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImVuZHBvaW50LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmYjIxNGQ5NC0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZW5kcG9pbnQtY29udHJvbGxlciJ9.GXIgzYCSL8ZnHZqXcjf0QrLSgx8F_1h1qUMoTIVPUFnlkMMkM2JQ4CgJWkEyODqA8Ibr7lN65-dZhVIwEN4V6eU2iut4PfR_qhDpqOO3-_e5voa3MKGs24ONN-M10DmX5UtSEhpIAZXUfb9wXqlp6NPi0iYsfQl2rJgwi0b_eVweH5KLWZCNFXGeEF4LzadRTK5nZgDZmbkDIWygdPqc7pTgZj-F_8ZVXA1y1aZ09Vn6JCzWjj-5gRAYf0uI66z07b1zwBcY_36sJTNyzCYHm9lXHcseokZFwLdspqwLATc0t1QPPvXHgY6ml1_h4I_j-zoweFpvg1gGFMi0iSIJJw
ca.crt:     1025 bytes


Name:         fluentd-es-token-zw92g
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=fluentd-es
              kubernetes.io/service-account.uid=f6161ec1-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJmbHVlbnRkLWVzLXRva2VuLXp3OTJnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImZsdWVudGQtZXMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNjE2MWVjMS0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Zmx1ZW50ZC1lcyJ9.d2msJeM90QfRIvooAgh0n3ljY8ked8MxtiMMYjbL8mXnf2VLc48P3EasTHwhZs9rnjWCGuimkQ1MfJnA6LvKgfm4kB9d9kWumo9FLQULPp_Xo8QezU_6xxMTBBOvi3bo82vfG2lIkbfnBpyaxaVvRSs8zthZzhldZ8NVKuh9GwcmhFQ0ClQ1DlKH1jbGNMzKed_qHXrYVOFqv018ISoO50DcXfvoVgpXdTxHL2zu_Yaa_3YDDBtatDIldkrrddshbNr1mG06GcZRo_vBD0ThYkbtaVsm3jOdH6IwQxPLO540xjPPuIqSnbtN_klX1QwbR_dAA3dIr8pHpW3deODXQQ


Name:         generic-garbage-collector-token-m25fs
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=generic-garbage-collector
              kubernetes.io/service-account.uid=fc4bb2c6-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJnZW5lcmljLWdhcmJhZ2UtY29sbGVjdG9yLXRva2VuLW0yNWZzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImdlbmVyaWMtZ2FyYmFnZS1jb2xsZWN0b3IiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmYzRiYjJjNi0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Z2VuZXJpYy1nYXJiYWdlLWNvbGxlY3RvciJ9.L67nt3GG5x7R-46RWt3q60PasVLLHP4wx8vb-S5ZNUr7vzDFlM1iZDHWyaWMusLhfhTGRPZd7UWFY3NaybM_qZ-XBuymQMiArZo5xFvOsEhd_klx8TOgTYg8vOGLKXxsmLpyoPd79KmK4VeNXHrq1r_nB0I8hQbM6ArIb0QasbRCdu4vW7wZGoVPpKjuMQ3VNY3YGtcJGmYtaW65Lz6coV1EbKNfy-hwzazwJKmPrBkyi3r4ph___4YoIwRV6hxEn5FczxujpxxHMvjg0RWOz0Wax7DEej5O08uK88X2QFJ_eYEXqxXyLtbwqv6WDVYny85SGsY5fC7v-CnsBU746A
ca.crt:     1025 bytes


Name:         horizontal-pod-autoscaler-token-jbgqq
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=horizontal-pod-autoscaler
              kubernetes.io/service-account.uid=f4a18ca5-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJob3Jpem9udGFsLXBvZC1hdXRvc2NhbGVyLXRva2VuLWpiZ3FxIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imhvcml6b250YWwtcG9kLWF1dG9zY2FsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNGExOGNhNS0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06aG9yaXpvbnRhbC1wb2QtYXV0b3NjYWxlciJ9.vUmHFnHNmlNMHXPBNqU3G81qV-50A7ll619lRct0_nIFH4PrrCKy6H9VbzZMm2Z4qf6Fh6o1dqBLQTRvzryCRy3fLcVSqiJqMTyTAsq9jgki1R9Eje9aI_GGnv2xvOlMlCUD4kz4xcxVEcXZ5elhWP225dGfoRvRobovnzI8d7jZQReep0lmaz5XyVCn3_enfIH4Y9YCyd-CsBYEx6RY_hm5bYec_YUqdLX6kTSPIflj4d97WlPlKXkisxW9jsYxU9rRxl1AeyJoVhbIdH_UC4NmniVXfLZ2SyxaeFvySqktNDBy6Fs6Ai_S7hDAQ0JzHV5YW-f-5IPohHDSBUGjKA


Name:         job-controller-token-d769b
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=job-controller
              kubernetes.io/service-account.uid=f49f0ddd-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqb2ItY29udHJvbGxlci10b2tlbi1kNzY5YiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqb2ItY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY0OWYwZGRkLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpqb2ItY29udHJvbGxlciJ9.Bggn4ZTVEyHllQgiZkqaulOQ1SnQlIcxv1JPsTq7-bdfV5Sa63BCOipA6dOnXS5NxOQ_D1lqOmwVGUOOy3B3byFvlm5jdhl5eQPDJ4VuF5XUQw8BbaBiAgPGy_yKuhmwq3qMK-knwmXpQL8g61XVjxZaK3JPkF5u1ZTzTMWTl83Eh1Rqu6cB49Nc6L8pq2ucXoyWloq_4KlxhF2jGIv2Xmesn_J2JgLPsHiK2-wRK8R3c-rXbOd7coMGGHApL-638mgTdPJpoIPh3XpILqUBRX8Lg0T7ewtSheo3u1UHwIHNSUVsc7PfiYWTq7XRGOSfa8mJyhgHLMGNJXTsRBQ7RQ
ca.crt:     1025 bytes


Name:         kube-dns-token-66tfx
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=kube-dns
              kubernetes.io/service-account.uid=f4d65df0-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLWRucy10b2tlbi02NnRmeCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlLWRucyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY0ZDY1ZGYwLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlLWRucyJ9.o25_6WWYqWJgdqoI5buTBjSUnFikIXs5mmBfYxA1O8F3dZySdTlNS_DfnbXCg5lwkRPN_BUiVTwasUzMAH09f-DJY51qprqm0jrBFYvtmouRa2vX5KTWe9AcvPZOr_zh-_Ju2RcMkrT2MIWBzg9B6cdgwQI471uKCP1O8A3iQorT2OVvBKoLjIB4mHM_oVODt-vznuhAD9G9t6U4aPSl_rbDVmxc6mJ4FA1Sf48NAyV8qJ1HyZksr-mwFS45o41-lhpx8_SiEgZQuBEwNq66HVKD5Anor3ZaPQduVI5Kg9OJnbWu85i-hvPor_VwYzd8riL3V-KDFkRJPycB53JL8g
ca.crt:     1025 bytes


Name:         kube-proxy-token-ktcv9
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=kube-proxy
              kubernetes.io/service-account.uid=f4e8d418-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLXByb3h5LXRva2VuLWt0Y3Y5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imt1YmUtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNGU4ZDQxOC0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06a3ViZS1wcm94eSJ9.lVycts70IjTwkG4CBiay2arm1Qpm8RvPyPbgGHjiAEgZ_JWaIQseVVpnPkJSG5OVyGm5Jq6XUB6pnqtDIszcjN9oVMVrOSf2Vn7XekVN6skZcgmOwnPPCcWImjeQZ7OJiIL_MXwGyRfFcziagalahrtlSN96BrpAjCQkvOvTtguiiNQ8wzqMphcq3NqAdxg5ez-AIPfaj3ZQcQ_Td6KqlNU7LaNNcsLf6EyL_tDEMxiErY7f7V3SmST_FMceJ6dlc9hCkAMFGogxSBMo29WwL6yGj7DQjQ8iHlUso3EtPPUMXKj5-92YwZqq37I4ZoN5SWFJo_SAn08qxTR-mPmBBA


Name:         kubernetes-dashboard-certs
Namespace:    kube-system
Labels:       k8s-app=kubernetes-dashboard
Annotations:  
Type:         Opaque

Data
====
dashboard.key:  1704 bytes


Name:         kubernetes-dashboard-key-holder
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
priv:  1679 bytes
pub:   459 bytes


Name:         kubernetes-dashboard-token-bjcwc
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=kubernetes-dashboard
              kubernetes.io/service-account.uid=f790fb2e-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1iamN3YyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImY3OTBmYjJlLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.GtmfLkTUwwpR2ibl-51AQ80MekeRLigzHXhIFvehG2JR9gFvaq3c-kWC4xgXcufnt1rGFjJuv4Av3sHjjHWRjI8Ag9s9fZwDhREoufcvjtGw80Xr54VeOAWiXDgEGKC5osRZIRpOnaRCtso-SkegAloZn8waMUBB0RXE-u9_2O2esO_EEoWh8omIDSOWv-hEHWxLB8WxpK7Jg42UFxhtJtbjOzPVopqddBBgw0f7BzWF-wAePLCwbIQsxKOm68TpbaTq6UfvwO9RJQ8PjtX0y1mTZyRgFslJXE_tKDsYSZX3RhaM1m4bIEahuBla9IXTnElgz3Lk8t92qs-DmpvGOA


Name:         namespace-controller-token-w7zqj
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=namespace-controller
              kubernetes.io/service-account.uid=fb40aee0-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi13N3pxaiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZiNDBhZWUwLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.G_9GvS8D99YeUHmgPKEeKeTfYj6n97HSOQaURRiGerr377noBfamV1DCbjPG-C7jtbX99Y4ykgjqopaxrxUsbq71zs-SgsNqoUKbyxYdPwC_KFWgJFYhz_V92DuXyvAe9HZafIvXNz595NUoX7pTbhlE5d7gTdHzm-3fN-IzSRTQosfRQZOKyetsx1C6_2MCJD2yIm9lpb8dmZS3NE7nx_id1CufTkEYUGDkSQRVL0y4wg1dK4ha8pHFrmxAeTs41tkDu6Pzh41dxf0vGb_dAarRJHfcwVeg_5WYAWfCFGUgUtZpDyZT61tYiPdExZGMV4_xqUwYpunvs7MO4W7PgQ


Name:         node-controller-token-mg5m9
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=node-controller
              kubernetes.io/service-account.uid=f50dc874-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJub2RlLWNvbnRyb2xsZXItdG9rZW4tbWc1bTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibm9kZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjUwZGM4NzQtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOm5vZGUtY29udHJvbGxlciJ9.yXbO6wWtqsYRV8vdPMiVezSkEopwnpsp0dRvb0BUJVLV1cqQzPtS3CptkdLNq21pDhJ8talIfgp6ewTD_Ugv3W382L2kEnt8JUD5XIQaghWhzZ9sZso7KJC5cZfkfmKLJRiogMIUaSyTUqs9WIws5KE8ZGF2wWyyu7SsoLNAxk5ZHFHeWXTySzvMi0u-xC0n8ypbyHM9Ihaagel-lbkH4jnIfFoCNUuUKDGRvbN4yPQWJRjdC-zCMH-d9KjFMSlFT1hvbTYHTmh222o7T-UtUc4ebnO_mni9dL5RzyOIPQqcVN56oaiVoFdPckErqM1LNh8OOysoHa2YICfbOKJg5A
ca.crt:     1025 bytes


Name:         persistent-volume-binder-token-jn99w
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=persistent-volume-binder
              kubernetes.io/service-account.uid=fbb31edd-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwZXJzaXN0ZW50LXZvbHVtZS1iaW5kZXItdG9rZW4tam45OXciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicGVyc2lzdGVudC12b2x1bWUtYmluZGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmJiMzFlZGQtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBlcnNpc3RlbnQtdm9sdW1lLWJpbmRlciJ9.LGN7B7isR569l697EUGUMnDi985ucRcTGUxlk1B3a1Kvlc4XPxxKpCAeMhiSaFa5t4KbOzmMNAEVmMCO_XAa_5f1qQmiy6RpMQ_fB_qj4I1uOud_WuGwhK4P5lYyIUG0sk5xnAaQkwTV0c-Hv20Kn_xQAijZEo7KrbIErN7u5Dtnv_NfWm0GZc3tkFEmOacePkHgYRxQLPFB9HXZE2NTRiMqjd-TiwkEp1ha3lbvNq71VtRtULB5bqJOIaC4hj7vhoPmnyZxUmPyOTtH1gX9RSlyu2ACgsttayl_99lM8U_fD4uygcjDxNt0jgyb-1EI-Zs5y0VX3Dne5ONOWX1QLA


Name:         pod-garbage-collector-token-ktn79
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=pod-garbage-collector
              kubernetes.io/service-account.uid=fbff725b-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwb2QtZ2FyYmFnZS1jb2xsZWN0b3ItdG9rZW4ta3RuNzkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicG9kLWdhcmJhZ2UtY29sbGVjdG9yIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmJmZjcyNWItMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBvZC1nYXJiYWdlLWNvbGxlY3RvciJ9.W9XMZapKxCI35Mi6Lg1_CooXnvYnT_QSois3DM6hu8iLxqm54s5FVveMQQjFETl51tD9ZVu0qrmuaQC2QPo5orU5sHvadOcQpbsoubsbeZqsradqAse83f7d--PSPteNaPspSFj2ntwA7D7ZazBWQNwksGt4gZnlPBPO6oYxNYXsZOkmKKUGbsjTdhJeuMBZgikK2nx0fZ3NjS_otWcxeICr7K0F2mRHOzBG20xOfqlWt5Gl3JIWgYg0qXpQfO74vvxXV5jn4qAxIYUyAMnhwQWMAdmhpgq_8G3jyDRmxSJniD8y93gOGAL-zNssxOZGQO_dylWHxp4Z1qY-bI9kaw


Name:         replicaset-controller-token-qgr8m
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=replicaset-controller
              kubernetes.io/service-account.uid=fb1a87ec-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhc2V0LWNvbnRyb2xsZXItdG9rZW4tcWdyOG0iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVwbGljYXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZmIxYTg3ZWMtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlcGxpY2FzZXQtY29udHJvbGxlciJ9.zf51YRHGEy4RihEbs1Yz9Fi7ygMo-E-8UiSan2mMS6NGDzLggP_GP6C1ZrGrlM0isvrnQYKY6OHD4c-hUvaTV86x9yrR663-EqTupD5tUpvIeWkVTRjbTBJMsJZlwd_flYnKj8XUEFU6IBF_kyCsV1LOMPKcpI4XvtbnUO0j8ZKQ_fBYDiI6w8_R9vZeNc46v1JA5MO0PLWKvwB6kvCgq7GZmxiODxx17jebgaeYDGXOOatCut9NGoZ17dfXD4ThMUDRUaIJPIHCZbFpVDAWStdZqX3HXwdq7Zj1et7cyFsGu7N7FUjMVZjPK3I1V6ENYMyW_UvFvDSCD7WrohzRtA


Name:         replication-controller-token-gw4vz
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=replication-controller
              kubernetes.io/service-account.uid=fb144c79-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhdGlvbi1jb250cm9sbGVyLXRva2VuLWd3NHZ6Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InJlcGxpY2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmYjE0NGM3OS0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cmVwbGljYXRpb24tY29udHJvbGxlciJ9.XTQzcW-PgmUqBVdQmAeozkJr8PctOKArefUSBcl3rncPSw0xZJiZXLmRXZgcU7vD_ZujkvPJq8SAgEDoor2FoJkmAqC-pH09KPPydRG1f4-BT-E4sQ-Gc92woRLO_0acTFajLMQCPvDgDYXbx5DdqqUDWvjaLZDLEkxK6m9cCWwnUSuKpVxNj01Iparnb8-J3AybVi4YhqjL2JrJ7DBHfx4DSIlhwS2jUIzGhMzyYQxAOYh8VOLP-naOtETYO0zWrSxj94oglLyS7KqJe7laZtzT_ZrekZpMVDicU8wjGAy9IwcTOEQZsTYm0iGt_drl2Aaoa0rSRbs7GLoThNPLiA
ca.crt:     1025 bytes


Name:         resourcequota-controller-token-fpsgt
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=resourcequota-controller
              kubernetes.io/service-account.uid=f49ca445-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXNvdXJjZXF1b3RhLWNvbnRyb2xsZXItdG9rZW4tZnBzZ3QiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVzb3VyY2VxdW90YS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjQ5Y2E0NDUtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlc291cmNlcXVvdGEtY29udHJvbGxlciJ9.ug2t0Blt8n0YLOk045JJNPNLxVHRpwOJ0jrhYRdWh5Fqq5psBN9ioQNThtz0IbhjddTVrjBNvaaGf1CpsMg7AZQOJeIUW-ycggMsvs8DMixtrtARZ0oRlzctDXHPY9H03FUbo9Mj8NkXzU2XMmTD0Vsi-HhxoKIzegjK_XjzpSd31oAWPt61kfNTa-kM2_IGeZlArp-rqj7g3g-30blHu_o1ypA5MQTJwKMVt_SfZHdbQN5WNf8_-XUdkBomSZcbsfxvh8wRJ7kqhLHhJhfyf7FuFLnyXqdA8ieFDlSe2fpw5hICYg9JTb704cKq9XLZn7WrrRClVNv558oo5c_4RQ


Name:         service-account-controller-token-xqln5
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=service-account-controller
              kubernetes.io/service-account.uid=fc258895-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlci10b2tlbi14cWxuNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZjMjU4ODk1LTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciJ9.aU1usgWHZPGzMaEPLt9mdQQ1OtvIUdbsXIy8c5cnCz8xJsDUL1ag_gyJe7sXL058m3fErcnHCPdb61mBUYth9UxAImcOnzm9DH8RtePJsAapSnacpB_JPLBjT_cDTqOWumNa5Uz46ok2So6BPncmS5EizQVzyAhTZKrDx5_907V1Jb2bBYgnpk2yUDGVHEEfTFUsPC1AcNTsQ6_epwh9ZXRQf-UwCIcyezJYPTUcHlEQfkSzHru2whYwuw36L9GTgT7lnoVdoTUsW3gFA44KJD6xE2otr7DvDo716h0iC456REmm3dDsK_d7aFWGFRR3Q8ZAMLF-C5iwJ4A8s6utLw
ca.crt:     1025 bytes


Name:         service-controller-token-p75wq
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=service-controller
              kubernetes.io/service-account.uid=f49a37cc-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWNvbnRyb2xsZXItdG9rZW4tcDc1d3EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2VydmljZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjQ5YTM3Y2MtMGRkMS0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnNlcnZpY2UtY29udHJvbGxlciJ9.Ykgv4qVocaWNdNE_lUheO2UywirKBCGulBtHdNT1rbJSa-8NFkWjd8cV1cVcKXOs6lVDrYWFUUFqFc9czDO2kvfTR1CkMeySGHbMbnbNMdq8JgH221E6T6EqHmftrx-fqo4YtQ4LS_G57VIxGKkwpnAllX7pbMGcdGtFFaRe30iEq2ZMn53pK3U6R2Loqkf7H4VvACiRli3RY_y3DkUHFNO9mWeKYpkNAIgVj3vj6BJGW4fO4OPpzysiSsnuqdxspaXCvzR-I6dM54DKAHsUckvx7jFcGW6c08Ttwbljg-JivCrfMZ1k6NulOEa4SBa7MNaywZ9bhV5lJqBrVDxi_Q


Name:         statefulset-controller-token-k7k92
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=statefulset-controller
              kubernetes.io/service-account.uid=fb1ccddc-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzdGF0ZWZ1bHNldC1jb250cm9sbGVyLXRva2VuLWs3azkyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InN0YXRlZnVsc2V0LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmYjFjY2RkYy0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06c3RhdGVmdWxzZXQtY29udHJvbGxlciJ9.gPZJzANNVUu01pg_RCQEc2v8_e2fJC8qLLPQDzU5MVKzXE2S0wBFh8WjUsRF0MuntSBnxTVU9LcfjIm9o80se-eW-sZnlJZZWhNZCGYcBARbt8BDWrG9sWzLG0i_a1iIs0D9FbcLG4UXcXo4BmkM4KJFDaPWdsKnp1DzU_h2iVwMS00aopwX6tw06K57ELUkLm8ZAeTS3Ake6KiYLi4GTQqLugQRhLaO4RHBBoe-XXq_9m-BNGzXT_vdxseoruSvHOWaOk0yqNuh5tpgvt0ChtjyZ2IYKbdZ3Z9hrkTtayO03q7cds5V4w-YYk2Dd7G5RCFgTQiJ6pY72yua5Wd4FQ
ca.crt:     1025 bytes
namespace:  11 bytes


Name:         tiller-token-chfrl
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=tiller
              kubernetes.io/service-account.uid=53fa2b6e-0dd2-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0aWxsZXItdG9rZW4tY2hmcmwiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoidGlsbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTNmYTJiNmUtMGRkMi0xMWU4LTg3MWItMDA1MDU2YWY5ZTk3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnRpbGxlciJ9.BGDMPNCUMia1aXJe8_iTW62sIlsy3hcikOr7JgaYgh4PZVr5KSWL2vzBjhm55dI85CiyjPR-bdFYBA7gkK9RazrmY5W94Yx5zb8yMDJi9zhyTikRyiafekppT7hsQnD3T3W1ypXO2pQgWiavw9-W36n3s2bQNAwWFZodY-DIz6-vJ_jPWmfXi1kM0ayp_eR29oOrh9Dg4tl6SVFTGErAXV99AjrIGkTW72j9Gwx2M7LxMo1LlWwZ8DpnNj-N6KXrF_afcuU75edCS2lhA-zV0H5Bp4gAF3bCsfxqNJOxInisgvFPB1TfDMz49snwjtUxPHbu3NhRvzXrvrM-mneXOQ


Name:         token-cleaner-token-9zl6n
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=token-cleaner
              kubernetes.io/service-account.uid=f4e795e8-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0b2tlbi1jbGVhbmVyLXRva2VuLTl6bDZuIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InRva2VuLWNsZWFuZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNGU3OTVlOC0wZGQxLTExZTgtODcxYi0wMDUwNTZhZjllOTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06dG9rZW4tY2xlYW5lciJ9.u6nY1qCPwA0U0mgZ9rZeFDhHyAmez1qGIHjyFGutxjPrRL5m3XguO0hYQ0yu4eVLPGhp18wiMBL4htxbv6ZhAcWLRfyZAY875B4WxAeR7uCKsSr6eDADkZDtixJvO9KoDWJqqu6SqqPNYAuG11YeEsjPFHBgzqj0H5sh3_anANNnitZV5N2Izm4OPMXOOxiVodtBYSU6OQ5MkXAlLkIN5T7u-jUEIzekhl5po741ba2cp9fwdzBXW6eMHdkz4CCu-QWv67qDsh0OPTqSCj3Tv_5Ik8KGy6eDUaUdJAmHDK-uBYA82X7pWfBn8WsUHkuCCuizKdSpHsK9qUwgA7t_mA
ca.crt:     1025 bytes


Name:         ttl-controller-token-xgk8l
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=ttl-controller
              kubernetes.io/service-account.uid=fb1ef2dc-0dd1-11e8-871b-005056af9e97

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0dGwtY29udHJvbGxlci10b2tlbi14Z2s4bCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0dGwtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZiMWVmMmRjLTBkZDEtMTFlOC04NzFiLTAwNTA1NmFmOWU5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp0dGwtY29udHJvbGxlciJ9.xMQq4u31EwDdsllR6Ea89hH5Ch_c5I9CIXw2S2MD7sGeSMr8fqHb_Vs55sNCdH8f3iCh2931cqviZ7uf3Mbb7-znk4NnZTD0gebg4dY0Ofwnr3pHBrKcg436zG9Nue96bGhBbuANQeUCcl2jZwpbHW4Tl19wTGF5PLLeN2Jt0exORDFe2rcI9dqeyvwL0h0irR0Sjy8aIsiAxLzJc4uyg5gIGomrwpSI_2vqD62h4tPEPQ4idAxWoGEAYb_Kz-MAyHPkQ9t2mdLD4aapW85d4YGW6g2VFsg2w3aCC5JpWaLlRhzfkyMkkdmFpPIxk1a31wDetOWLkQPQN_zLOR9xDw

================
kubectl describe serviceaccounts --all-namespaces
================

Name:         default
Namespace:    default
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   default-token-fc6vg

Tokens:              default-token-fc6vg

Events:  <none>


Name:         kube-keepalived-vip
Namespace:    default
Labels:       <none>
Annotations:  <none>

Tokens:              kube-keepalived-vip-token-2w47w

Image pull secrets:  <none>

Mountable secrets:   kube-keepalived-vip-token-2w47w

Events:  <none>


Name:         my-nginx-ingress
Namespace:    default
Labels:       app=nginx-ingress
              chart=nginx-ingress-0.9.2
              heritage=Tiller
              release=my-nginx-ingress
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   my-nginx-ingress-token-9ffd9

Tokens:              my-nginx-ingress-token-9ffd9

Events:  <none>


Name:         default
Namespace:    kube-public
Labels:       <none>
Annotations:  <none>

Mountable secrets:   default-token-7g442

Tokens:              default-token-7g442

Image pull secrets:  <none>

Events:  <none>


Name:         attachdetach-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   attachdetach-controller-token-pdmt4

Tokens:              attachdetach-controller-token-pdmt4

Events:  <none>


Name:         bootstrap-signer
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   bootstrap-signer-token-nhshd

Tokens:              bootstrap-signer-token-nhshd

Events:  <none>


Name:         calico-node
Namespace:    kube-system
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"calico-node","namespace":"kube-system"}}


Image pull secrets:  <none>

Mountable secrets:   calico-node-token-m5t54

Tokens:              calico-node-token-m5t54

Events:  <none>


Name:         certificate-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Tokens:              certificate-controller-token-sxs2h

Image pull secrets:  <none>

Mountable secrets:   certificate-controller-token-sxs2h

Events:  <none>


Name:         cronjob-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Mountable secrets:   cronjob-controller-token-4dlm7

Tokens:              cronjob-controller-token-4dlm7

Image pull secrets:  <none>

Events:  <none>


Name:         daemon-set-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   daemon-set-controller-token-4jwtx

Tokens:              daemon-set-controller-token-4jwtx

Events:  <none>


Name:         default
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   default-token-kxl84

Tokens:              default-token-kxl84

Events:  <none>


Name:         deployment-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   deployment-controller-token-8gz7n

Tokens:              deployment-controller-token-8gz7n

Events:  <none>


Name:         disruption-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   disruption-controller-token-sndts

Tokens:              disruption-controller-token-sndts

Events:  <none>


Name:         elasticsearch-logging
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=elasticsearch-logging
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearc...

Image pull secrets:  <none>

Mountable secrets:   elasticsearch-logging-token-6cmks

Tokens:              elasticsearch-logging-token-6cmks

Events:  <none>


Name:         endpoint-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   endpoint-controller-token-6xqjn

Tokens:              endpoint-controller-token-6xqjn

Events:  <none>


Name:         fluentd-es
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=fluentd-es
              kubernetes.io/cluster-service=true
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es",...

Tokens:              fluentd-es-token-zw92g

Image pull secrets:  <none>

Mountable secrets:   fluentd-es-token-zw92g

Events:  <none>


Name:         generic-garbage-collector
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Mountable secrets:   generic-garbage-collector-token-m25fs

Tokens:              generic-garbage-collector-token-m25fs

Image pull secrets:  <none>

Events:  <none>


Name:         horizontal-pod-autoscaler
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   horizontal-pod-autoscaler-token-jbgqq

Tokens:              horizontal-pod-autoscaler-token-jbgqq

Events:  <none>


Name:         job-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   job-controller-token-d769b

Tokens:              job-controller-token-d769b

Events:  <none>


Name:         kube-dns
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   kube-dns-token-66tfx

Tokens:              kube-dns-token-66tfx

Events:  <none>


Name:         kube-proxy
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Tokens:              kube-proxy-token-ktcv9

Image pull secrets:  <none>

Mountable secrets:   kube-proxy-token-ktcv9

Events:  <none>


Name:         kubernetes-dashboard
Namespace:    kube-system
Labels:       k8s-app=kubernetes-dashboard
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","name...

Image pull secrets:  <none>

Mountable secrets:   kubernetes-dashboard-token-bjcwc

Tokens:              kubernetes-dashboard-token-bjcwc

Events:  <none>


Name:         namespace-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   namespace-controller-token-w7zqj

Tokens:              namespace-controller-token-w7zqj

Events:  <none>


Name:         node-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Tokens:              node-controller-token-mg5m9

Image pull secrets:  <none>

Mountable secrets:   node-controller-token-mg5m9

Events:  <none>


Name:         persistent-volume-binder
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   persistent-volume-binder-token-jn99w

Tokens:              persistent-volume-binder-token-jn99w

Events:  <none>


Name:         pod-garbage-collector
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   pod-garbage-collector-token-ktn79

Tokens:              pod-garbage-collector-token-ktn79

Events:  <none>


Name:         replicaset-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   replicaset-controller-token-qgr8m

Tokens:              replicaset-controller-token-qgr8m

Events:  <none>


Name:         replication-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   replication-controller-token-gw4vz

Tokens:              replication-controller-token-gw4vz

Events:  <none>


Name:         resourcequota-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   resourcequota-controller-token-fpsgt

Tokens:              resourcequota-controller-token-fpsgt

Events:  <none>


Name:         service-account-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   service-account-controller-token-xqln5

Tokens:              service-account-controller-token-xqln5

Events:  <none>


Name:         service-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   service-controller-token-p75wq

Tokens:              service-controller-token-p75wq

Events:  <none>


Name:         statefulset-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   statefulset-controller-token-k7k92

Tokens:              statefulset-controller-token-k7k92

Events:  <none>


Name:         tiller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   tiller-token-chfrl

Tokens:              tiller-token-chfrl

Events:  <none>


Name:         token-cleaner
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   token-cleaner-token-9zl6n

Tokens:              token-cleaner-token-9zl6n

Events:  <none>


Name:         ttl-controller
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Image pull secrets:  <none>

Mountable secrets:   ttl-controller-token-xgk8l

Tokens:              ttl-controller-token-xgk8l

Events:  <none>

================
kubectl describe services --all-namespaces
================

Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         10.10.97.20:6443
Session Affinity:  ClientIP
Events:            <none>


Name:              my-nginx-ingress-controller
Namespace:         default
Labels:            app=nginx-ingress
                   chart=nginx-ingress-0.9.2
                   component=controller
                   heritage=Tiller
                   release=my-nginx-ingress
Annotations:       <none>
Selector:          app=nginx-ingress,component=controller,release=my-nginx-ingress
Type:              ClusterIP
IP:                10.107.38.119
External IPs:      10.10.97.200
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         192.168.2.19:80
Port:              https  443/TCP
TargetPort:        443/TCP
Endpoints:         192.168.2.19:443
Session Affinity:  None
Events:            <none>


Name:              my-nginx-ingress-default-backend
Namespace:         default
Labels:            app=nginx-ingress
                   chart=nginx-ingress-0.9.2
                   component=default-backend
                   heritage=Tiller
                   release=my-nginx-ingress
Annotations:       <none>
Selector:          app=nginx-ingress,component=default-backend,release=my-nginx-ingress
Type:              ClusterIP
IP:                10.96.69.91
Port:              <unset>  80/TCP
TargetPort:        8080/TCP
Endpoints:         192.168.2.18:8080
Session Affinity:  None
Events:            <none>


Name:              tea-svc
Namespace:         default
Labels:            app=tea
Annotations:       <none>
Selector:          app=tea
Type:              ClusterIP
IP:                10.96.196.71
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         192.168.1.52:80,192.168.2.14:80,192.168.2.15:80
Session Affinity:  None
Events:            <none>


Name:              calico-typha
Namespace:         kube-system
Labels:            k8s-app=calico-typha
Annotations:       kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"...
Selector:          k8s-app=calico-typha
Type:              ClusterIP
IP:                10.99.51.177
Port:              calico-typha  5473/TCP
TargetPort:        %!d(string=calico-typha)/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:              elasticsearch-logging
Namespace:         kube-system
Labels:            addonmanager.kubernetes.io/mode=Reconcile
                   k8s-app=elasticsearch-logging
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=Elasticsearch
Annotations:       kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-loggi...
Selector:          k8s-app=elasticsearch-logging
Type:              ClusterIP
IP:                10.105.23.60
Port:              <unset>  9200/TCP
TargetPort:        %!d(string=db)/TCP
Endpoints:         192.168.1.4:9200,192.168.2.5:9200
Session Affinity:  None
Events:            <none>


Name:                     kibana-logging
Namespace:                kube-system
Labels:                   addonmanager.kubernetes.io/mode=Reconcile
                          k8s-app=kibana-logging
                          kubernetes.io/cluster-service=true
                          kubernetes.io/name=Kibana
Annotations:              kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","ku...
Selector:                 k8s-app=kibana-logging
Type:                     NodePort
IP:                       10.96.208.116
Port:                     <unset>  5601/TCP
TargetPort:               %!d(string=ui)/TCP
NodePort:                 <unset>  30601/TCP
Endpoints:                192.168.2.4:5601
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=KubeDNS
Annotations:       <none>
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP:                10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         192.168.0.4:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         192.168.0.4:53
Session Affinity:  None
Events:            <none>


Name:                     kubernetes-dashboard
Namespace:                kube-system
Labels:                   k8s-app=kubernetes-dashboard
Annotations:              kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":...
Selector:                 k8s-app=kubernetes-dashboard
Type:                     NodePort
IP:                       10.106.31.65
Port:                     <unset>  443/TCP
TargetPort:               8443/TCP
NodePort:                 <unset>  31443/TCP
Endpoints:                192.168.0.3:8443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


Name:              tiller-deploy
Namespace:         kube-system
Labels:            app=helm
                   name=tiller
Annotations:       <none>
Selector:          app=helm,name=tiller
Type:              ClusterIP
IP:                10.104.87.82
Port:              tiller  44134/TCP
TargetPort:        %!d(string=tiller)/TCP
Endpoints:         192.168.2.6:44134
Session Affinity:  None
Events:            <none>

================
kubectl describe statefulsets --all-namespaces
================

Name:               elasticsearch-logging
Namespace:          kube-system
CreationTimestamp:  Fri, 09 Feb 2018 19:46:46 +0000
Selector:           k8s-app=elasticsearch-logging,version=v5.6.4
Labels:             addonmanager.kubernetes.io/mode=Reconcile
                    k8s-app=elasticsearch-logging
                    kubernetes.io/cluster-service=true
                    version=v5.6.4
Annotations:        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elast...
Replicas:           2 desired | 2 total
Pods Status:        2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=elasticsearch-logging
                    kubernetes.io/cluster-service=true
                    version=v5.6.4
  Service Account:  elasticsearch-logging
  Init Containers:
   elasticsearch-logging-init:
    Image:  registry.ci.dfj.io/cpsg_ccp/alpine:3.6
    Port:   <none>
    Command:
      /sbin/sysctl
      -w
      vm.max_map_count=262144
    Environment:  <none>
    Mounts:       <none>
  Containers:
   elasticsearch-logging:
    Image:  registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
    Ports:  9200/TCP, 9300/TCP
    Limits:
      cpu:  1
    Requests:
      cpu:  100m
    Environment:
      NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /data from elasticsearch-logging (rw)
  Volumes:
   elasticsearch-logging:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
Volume Claims:  <none>
Events:         <none>

================
kubectl describe storageclasses --all-namespaces
================


================
kubectl describe kubectl --all-namespaces && echo ================n && kubectl describe kubectl --all-namespaces

================
kubectl get all --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"default"},"spec":{"template":{"metadata":{"labels":{"name":"ccphxvolume"}},"spec":{"containers":[{"command":["/bin/sh","-c","while true; do sleep 2; done"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","imagePullPolicy":"IfNotPresent","name":"hxvolume","securityContext":{"privileged":true}}],"initContainers":[{"command":["sh","-c","SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name \u003e /etc/iscsi/initiatorname.iscsi"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"iscsi-initiator"},{"command":["sh","-c","cp /hxcache/hxvolume /hxhostmount/"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"hxvolume-copy","volumeMounts":[{"mountPath":"/hxhostmount","name":"hxvolume-mount"},{"mountPath":"/etc/iscsi","name":"iscsi-volume"}]}],"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"hostPath":{"path":"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/"},"name":"hxvolume-mount"},{"hostPath":{"path":"/etc/iscsi"},"name":"iscsi-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      name: ccphxvolume
    name: ccphxvolume
    namespace: default
    resourceVersion: "446863"
    selfLink: /apis/extensions/v1beta1/namespaces/default/daemonsets/ccphxvolume
    uid: f73cde15-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: ccphxvolume
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: ccphxvolume
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - while true; do sleep 2; done
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: iscsi-initiator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - sh
          - -c
          - cp /hxcache/hxvolume /hxhostmount/
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume-copy
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hxhostmount
            name: hxvolume-mount
          - mountPath: /etc/iscsi
            name: iscsi-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
            type: ""
          name: hxvolume-mount
        - hostPath:
            path: /etc/iscsi
            type: ""
          name: iscsi-volume
    templateGeneration: 1
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    creationTimestamp: 2018-02-13T19:01:11Z
    generation: 1
    labels:
      name: kube-keepalived-vip
    name: kube-keepalived-vip
    namespace: default
    resourceVersion: "473538"
    selfLink: /apis/extensions/v1beta1/namespaces/default/daemonsets/kube-keepalived-vip
    uid: 41d5383d-10f0-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: kube-keepalived-vip
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: kube-keepalived-vip
      spec:
        containers:
        - args:
          - --services-configmap=default/vip-configmap
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/kube-keepalived-vip:0.11
          imagePullPolicy: Always
          name: kube-keepalived-vip
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: modules
            readOnly: true
          - mountPath: /dev
            name: dev
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-keepalived-vip
        serviceAccountName: kube-keepalived-vip
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: modules
        - hostPath:
            path: /dev
            type: ""
          name: dev
    templateGeneration: 1
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"FELIX_LOGSEVERITYSCREEN","value":"info"},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_IPINIPMTU","value":"1440"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"CALICO_IPV4POOL_CIDR","value":"192.168.0.0/16"},{"name":"CALICO_IPV4POOL_IPIP","value":"Always"},{"name":"FELIX_IPINIPENABLED","value":"true"},{"name":"FELIX_TYPHAK8SSERVICENAME","valueFrom":{"configMapKeyRef":{"key":"typha_service_name","name":"calico-config"}}},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"IP","value":""},{"name":"FELIX_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master","livenessProbe":{"failureThreshold":6,"httpGet":{"path":"/liveness","port":9099},"initialDelaySeconds":10,"periodSeconds":10},"name":"calico-node","readinessProbe":{"httpGet":{"path":"/readiness","port":9099},"periodSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false}]},{"command":["/install-cni.sh"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master","name":"install-cni","volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"hostNetwork":true,"serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-node
    name: calico-node
    namespace: kube-system
    resourceVersion: "446861"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/calico-node
    uid: f590af99-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: FELIX_LOGSEVERITYSCREEN
            value: info
          - name: CLUSTER_TYPE
            value: k8s,bgp
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_IPINIPMTU
            value: "1440"
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: CALICO_IPV4POOL_CIDR
            value: 192.168.0.0/16
          - name: CALICO_IPV4POOL_IPIP
            value: Always
          - name: FELIX_IPINIPENABLED
            value: "true"
          - name: FELIX_TYPHAK8SSERVICENAME
            valueFrom:
              configMapKeyRef:
                key: typha_service_name
                name: calico-config
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: IP
          - name: FELIX_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /liveness
              port: 9099
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-node
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9099
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /var/run/calico
            name: var-run-calico
        - command:
          - /install-cni.sh
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: cni_network_config
                name: calico-config
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"},"name":"fluentd-es-v2.0.2","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-es","version":"v2.0.2"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"}},"spec":{"containers":[{"env":[{"name":"FLUENTD_ARGS","value":"--no-supervisor -q"}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2","name":"fluentd-es","resources":{"limits":{"memory":"500Mi"},"requests":{"cpu":"100m","memory":"200Mi"}},"volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/host/lib","name":"libsystemddir","readOnly":true},{"mountPath":"/etc/fluent/config.d","name":"config-volume"}]}],"serviceAccountName":"fluentd-es","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"hostPath":{"path":"/usr/lib64"},"name":"libsystemddir"},{"configMap":{"name":"fluentd-es-config-v0.1.1"},"name":"config-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:46Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      version: v2.0.2
    name: fluentd-es-v2.0.2
    namespace: kube-system
    resourceVersion: "446860"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/fluentd-es-v2.0.2
    uid: f626704d-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-es
        version: v2.0.2
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: fluentd-es
          kubernetes.io/cluster-service: "true"
          version: v2.0.2
      spec:
        containers:
        - env:
          - name: FLUENTD_ARGS
            value: --no-supervisor -q
          image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
          imagePullPolicy: IfNotPresent
          name: fluentd-es
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/log
            name: varlog
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
          - mountPath: /host/lib
            name: libsystemddir
            readOnly: true
          - mountPath: /etc/fluent/config.d
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-es
        serviceAccountName: fluentd-es
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
        - hostPath:
            path: /usr/lib64
            type: ""
          name: libsystemddir
        - configMap:
            defaultMode: 420
            name: fluentd-es-config-v0.1.1
          name: config-volume
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 1
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "446859"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/kube-proxy
    uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
          - --cluster-cidr=192.168.0.0/16
          image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          value: "true"
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473488"
    selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/my-nginx-ingress-controller
    uid: a14f1a1b-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473454"
    selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/my-nginx-ingress-default-backend
    uid: a14f9e60-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"spec":{"replicas":0,"revisionHistoryLimit":2,"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha"}},"spec":{"containers":[{"env":[{"name":"TYPHA_LOGSEVERITYSCREEN","value":"info"},{"name":"TYPHA_LOGFILEPATH","value":"none"},{"name":"TYPHA_LOGSEVERITYSYS","value":"none"},{"name":"TYPHA_CONNECTIONREBALANCINGMODE","value":"kubernetes"},{"name":"TYPHA_DATASTORETYPE","value":"kubernetes"},{"name":"TYPHA_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master","livenessProbe":{"httpGet":{"path":"/liveness","port":9098},"initialDelaySeconds":30,"periodSeconds":30},"name":"calico-typha","ports":[{"containerPort":5473,"name":"calico-typha","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/readiness","port":9098},"periodSeconds":10}}],"hostNetwork":true,"serviceAccountName":"calico-node","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "360"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/calico-typha
    uid: f58c96a5-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: calico-typha
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    conditions:
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: ReplicaSet "calico-typha-65b7467b56" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","kubernetes.io/cluster-service":"true"},"name":"kibana-logging","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"kibana-logging"}},"template":{"metadata":{"labels":{"k8s-app":"kibana-logging"}},"spec":{"containers":[{"env":[{"name":"ELASTICSEARCH_URL","value":"http://elasticsearch-logging:9200"},{"name":"XPACK_MONITORING_ENABLED","value":"false"},{"name":"XPACK_SECURITY_ENABLED","value":"false"}],"image":"registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4","name":"kibana-logging","ports":[{"containerPort":5601,"name":"ui","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}}}]}}}}
    creationTimestamp: 2018-02-09T19:46:47Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "807"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kibana-logging
    uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kibana-logging
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:45Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: ReplicaSet "kibana-logging-767cf49759" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-dns
    name: kube-dns
    namespace: kube-system
    resourceVersion: "854"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-dns
    uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:50Z
      lastUpdateTime: 2018-02-09T19:47:50Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"},"spec":{"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"k8s-app":"kubernetes-dashboard"}},"template":{"metadata":{"labels":{"k8s-app":"kubernetes-dashboard"}},"spec":{"containers":[{"args":["--auto-generate-certificates"],"image":"k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1","livenessProbe":{"httpGet":{"path":"/","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":30,"timeoutSeconds":30},"name":"kubernetes-dashboard","ports":[{"containerPort":8443,"protocol":"TCP"}],"volumeMounts":[{"mountPath":"/certs","name":"kubernetes-dashboard-certs"},{"mountPath":"/tmp","name":"tmp-volume"}]}],"serviceAccountName":"kubernetes-dashboard","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"name":"kubernetes-dashboard-certs","secret":{"secretName":"kubernetes-dashboard-certs"}},{"emptyDir":{},"name":"tmp-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "774"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kubernetes-dashboard
    uid: f79d83b7-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:42Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: ReplicaSet "kubernetes-dashboard-7798c48646" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "1069"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/tiller-deploy
    uid: 4998e55c-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:49:06Z
      lastUpdateTime: 2018-02-09T19:49:06Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: "851805358"
      release: my-nginx-ingress
    name: my-nginx-ingress-controller-d95d4979d
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-controller
      uid: a14f1a1b-1108-11e8-871b-005056af9e97
    resourceVersion: "473487"
    selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/my-nginx-ingress-controller-d95d4979d
    uid: a14fe756-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: "851805358"
        release: my-nginx-ingress
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: "851805358"
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: "4118459331"
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend-855d89f775
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-default-backend
      uid: a14f9e60-1108-11e8-871b-005056af9e97
    resourceVersion: "473452"
    selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/my-nginx-ingress-default-backend-855d89f775
    uid: a151592a-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        pod-template-hash: "4118459331"
        release: my-nginx-ingress
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          pod-template-hash: "4118459331"
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: calico-typha
      pod-template-hash: "2163023612"
    name: calico-typha-65b7467b56
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: f58c96a5-0dd1-11e8-871b-005056af9e97
    resourceVersion: "340"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/calico-typha-65b7467b56
    uid: fcf331c4-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: "2163023612"
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
          pod-template-hash: "2163023612"
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kibana-logging
      pod-template-hash: "3237905315"
    name: kibana-logging-767cf49759
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kibana-logging
      uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
    resourceVersion: "806"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kibana-logging-767cf49759
    uid: fcf34643-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kibana-logging
        pod-template-hash: "3237905315"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
          pod-template-hash: "3237905315"
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: "1016706980"
    name: kube-dns-545bc4bfd4
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
    resourceVersion: "852"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kube-dns-545bc4bfd4
    uid: fcf396ae-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: "1016706980"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: "1016706980"
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
      pod-template-hash: "3354704202"
    name: kubernetes-dashboard-7798c48646
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubernetes-dashboard
      uid: f79d83b7-0dd1-11e8-871b-005056af9e97
    resourceVersion: "773"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kubernetes-dashboard-7798c48646
    uid: fcf3720c-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: "3354704202"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
          pod-template-hash: "3354704202"
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:24Z
    generation: 1
    labels:
      app: helm
      name: tiller
      pod-template-hash: "1027952527"
    name: tiller-deploy-546cf9696c
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1068"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/tiller-deploy-546cf9696c
    uid: 54347a31-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "1027952527"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "1027952527"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
      pod-template-hash: "165821739"
    name: tiller-deploy-5b9d65c7f
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1037"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/tiller-deploy-5b9d65c7f
    uid: 499996c4-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "165821739"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "165821739"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473488"
    selfLink: /apis/apps/v1beta1/namespaces/default/deployments/my-nginx-ingress-controller
    uid: a14f1a1b-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473454"
    selfLink: /apis/apps/v1beta1/namespaces/default/deployments/my-nginx-ingress-default-backend
    uid: a14f9e60-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"spec":{"replicas":0,"revisionHistoryLimit":2,"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha"}},"spec":{"containers":[{"env":[{"name":"TYPHA_LOGSEVERITYSCREEN","value":"info"},{"name":"TYPHA_LOGFILEPATH","value":"none"},{"name":"TYPHA_LOGSEVERITYSYS","value":"none"},{"name":"TYPHA_CONNECTIONREBALANCINGMODE","value":"kubernetes"},{"name":"TYPHA_DATASTORETYPE","value":"kubernetes"},{"name":"TYPHA_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master","livenessProbe":{"httpGet":{"path":"/liveness","port":9098},"initialDelaySeconds":30,"periodSeconds":30},"name":"calico-typha","ports":[{"containerPort":5473,"name":"calico-typha","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/readiness","port":9098},"periodSeconds":10}}],"hostNetwork":true,"serviceAccountName":"calico-node","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "360"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/deployments/calico-typha
    uid: f58c96a5-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: calico-typha
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    conditions:
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: ReplicaSet "calico-typha-65b7467b56" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","kubernetes.io/cluster-service":"true"},"name":"kibana-logging","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"kibana-logging"}},"template":{"metadata":{"labels":{"k8s-app":"kibana-logging"}},"spec":{"containers":[{"env":[{"name":"ELASTICSEARCH_URL","value":"http://elasticsearch-logging:9200"},{"name":"XPACK_MONITORING_ENABLED","value":"false"},{"name":"XPACK_SECURITY_ENABLED","value":"false"}],"image":"registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4","name":"kibana-logging","ports":[{"containerPort":5601,"name":"ui","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}}}]}}}}
    creationTimestamp: 2018-02-09T19:46:47Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "807"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/deployments/kibana-logging
    uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kibana-logging
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:45Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: ReplicaSet "kibana-logging-767cf49759" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-dns
    name: kube-dns
    namespace: kube-system
    resourceVersion: "854"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/deployments/kube-dns
    uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:50Z
      lastUpdateTime: 2018-02-09T19:47:50Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"},"spec":{"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"k8s-app":"kubernetes-dashboard"}},"template":{"metadata":{"labels":{"k8s-app":"kubernetes-dashboard"}},"spec":{"containers":[{"args":["--auto-generate-certificates"],"image":"k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1","livenessProbe":{"httpGet":{"path":"/","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":30,"timeoutSeconds":30},"name":"kubernetes-dashboard","ports":[{"containerPort":8443,"protocol":"TCP"}],"volumeMounts":[{"mountPath":"/certs","name":"kubernetes-dashboard-certs"},{"mountPath":"/tmp","name":"tmp-volume"}]}],"serviceAccountName":"kubernetes-dashboard","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"name":"kubernetes-dashboard-certs","secret":{"secretName":"kubernetes-dashboard-certs"}},{"emptyDir":{},"name":"tmp-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "774"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/deployments/kubernetes-dashboard
    uid: f79d83b7-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:42Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: ReplicaSet "kubernetes-dashboard-7798c48646" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "1069"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/deployments/tiller-deploy
    uid: 4998e55c-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:49:06Z
      lastUpdateTime: 2018-02-09T19:49:06Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"},"name":"elasticsearch-logging","namespace":"kube-system"},"spec":{"replicas":2,"selector":{"matchLabels":{"k8s-app":"elasticsearch-logging","version":"v5.6.4"}},"serviceName":"elasticsearch-logging","template":{"metadata":{"labels":{"k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"}},"spec":{"containers":[{"env":[{"name":"NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4","name":"elasticsearch-logging","ports":[{"containerPort":9200,"name":"db","protocol":"TCP"},{"containerPort":9300,"name":"transport","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}},"volumeMounts":[{"mountPath":"/data","name":"elasticsearch-logging"}]}],"initContainers":[{"command":["/sbin/sysctl","-w","vm.max_map_count=262144"],"image":"registry.ci.dfj.io/cpsg_ccp/alpine:3.6","name":"elasticsearch-logging-init","securityContext":{"privileged":true}}],"serviceAccountName":"elasticsearch-logging","volumes":[{"emptyDir":{},"name":"elasticsearch-logging"}]}}}}
    creationTimestamp: 2018-02-09T19:46:46Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "843"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/statefulsets/elasticsearch-logging
    uid: f66f6df9-0dd1-11e8-871b-005056af9e97
  spec:
    podManagementPolicy: OrderedReady
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: elasticsearch-logging
        version: v5.6.4
    serviceName: elasticsearch-logging
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: elasticsearch-logging
          kubernetes.io/cluster-service: "true"
          version: v5.6.4
      spec:
        containers:
        - env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
          imagePullPolicy: IfNotPresent
          name: elasticsearch-logging
          ports:
          - containerPort: 9200
            name: db
            protocol: TCP
          - containerPort: 9300
            name: transport
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: elasticsearch-logging
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /sbin/sysctl
          - -w
          - vm.max_map_count=262144
          image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
          imagePullPolicy: IfNotPresent
          name: elasticsearch-logging-init
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: elasticsearch-logging
        serviceAccountName: elasticsearch-logging
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: elasticsearch-logging
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    collisionCount: 0
    currentReplicas: 2
    currentRevision: elasticsearch-logging-7978c6964c
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updateRevision: elasticsearch-logging-7978c6964c
- apiVersion: apps/v1beta2
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"default"},"spec":{"template":{"metadata":{"labels":{"name":"ccphxvolume"}},"spec":{"containers":[{"command":["/bin/sh","-c","while true; do sleep 2; done"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","imagePullPolicy":"IfNotPresent","name":"hxvolume","securityContext":{"privileged":true}}],"initContainers":[{"command":["sh","-c","SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name \u003e /etc/iscsi/initiatorname.iscsi"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"iscsi-initiator"},{"command":["sh","-c","cp /hxcache/hxvolume /hxhostmount/"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"hxvolume-copy","volumeMounts":[{"mountPath":"/hxhostmount","name":"hxvolume-mount"},{"mountPath":"/etc/iscsi","name":"iscsi-volume"}]}],"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"hostPath":{"path":"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/"},"name":"hxvolume-mount"},{"hostPath":{"path":"/etc/iscsi"},"name":"iscsi-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      name: ccphxvolume
    name: ccphxvolume
    namespace: default
    resourceVersion: "446863"
    selfLink: /apis/apps/v1beta2/namespaces/default/daemonsets/ccphxvolume
    uid: f73cde15-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: ccphxvolume
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: ccphxvolume
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - while true; do sleep 2; done
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: iscsi-initiator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - sh
          - -c
          - cp /hxcache/hxvolume /hxhostmount/
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume-copy
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hxhostmount
            name: hxvolume-mount
          - mountPath: /etc/iscsi
            name: iscsi-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
            type: ""
          name: hxvolume-mount
        - hostPath:
            path: /etc/iscsi
            type: ""
          name: iscsi-volume
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1beta2
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: 2018-02-13T19:01:11Z
    generation: 1
    labels:
      name: kube-keepalived-vip
    name: kube-keepalived-vip
    namespace: default
    resourceVersion: "473538"
    selfLink: /apis/apps/v1beta2/namespaces/default/daemonsets/kube-keepalived-vip
    uid: 41d5383d-10f0-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: kube-keepalived-vip
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: kube-keepalived-vip
      spec:
        containers:
        - args:
          - --services-configmap=default/vip-configmap
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/kube-keepalived-vip:0.11
          imagePullPolicy: Always
          name: kube-keepalived-vip
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: modules
            readOnly: true
          - mountPath: /dev
            name: dev
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-keepalived-vip
        serviceAccountName: kube-keepalived-vip
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: modules
        - hostPath:
            path: /dev
            type: ""
          name: dev
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1beta2
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"FELIX_LOGSEVERITYSCREEN","value":"info"},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_IPINIPMTU","value":"1440"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"CALICO_IPV4POOL_CIDR","value":"192.168.0.0/16"},{"name":"CALICO_IPV4POOL_IPIP","value":"Always"},{"name":"FELIX_IPINIPENABLED","value":"true"},{"name":"FELIX_TYPHAK8SSERVICENAME","valueFrom":{"configMapKeyRef":{"key":"typha_service_name","name":"calico-config"}}},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"IP","value":""},{"name":"FELIX_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master","livenessProbe":{"failureThreshold":6,"httpGet":{"path":"/liveness","port":9099},"initialDelaySeconds":10,"periodSeconds":10},"name":"calico-node","readinessProbe":{"httpGet":{"path":"/readiness","port":9099},"periodSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false}]},{"command":["/install-cni.sh"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master","name":"install-cni","volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"hostNetwork":true,"serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-node
    name: calico-node
    namespace: kube-system
    resourceVersion: "446861"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/daemonsets/calico-node
    uid: f590af99-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: FELIX_LOGSEVERITYSCREEN
            value: info
          - name: CLUSTER_TYPE
            value: k8s,bgp
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_IPINIPMTU
            value: "1440"
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: CALICO_IPV4POOL_CIDR
            value: 192.168.0.0/16
          - name: CALICO_IPV4POOL_IPIP
            value: Always
          - name: FELIX_IPINIPENABLED
            value: "true"
          - name: FELIX_TYPHAK8SSERVICENAME
            valueFrom:
              configMapKeyRef:
                key: typha_service_name
                name: calico-config
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: IP
          - name: FELIX_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /liveness
              port: 9099
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-node
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9099
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /var/run/calico
            name: var-run-calico
        - command:
          - /install-cni.sh
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: cni_network_config
                name: calico-config
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1beta2
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"},"name":"fluentd-es-v2.0.2","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-es","version":"v2.0.2"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"}},"spec":{"containers":[{"env":[{"name":"FLUENTD_ARGS","value":"--no-supervisor -q"}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2","name":"fluentd-es","resources":{"limits":{"memory":"500Mi"},"requests":{"cpu":"100m","memory":"200Mi"}},"volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/host/lib","name":"libsystemddir","readOnly":true},{"mountPath":"/etc/fluent/config.d","name":"config-volume"}]}],"serviceAccountName":"fluentd-es","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"hostPath":{"path":"/usr/lib64"},"name":"libsystemddir"},{"configMap":{"name":"fluentd-es-config-v0.1.1"},"name":"config-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:46Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      version: v2.0.2
    name: fluentd-es-v2.0.2
    namespace: kube-system
    resourceVersion: "446860"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/daemonsets/fluentd-es-v2.0.2
    uid: f626704d-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-es
        version: v2.0.2
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: fluentd-es
          kubernetes.io/cluster-service: "true"
          version: v2.0.2
      spec:
        containers:
        - env:
          - name: FLUENTD_ARGS
            value: --no-supervisor -q
          image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
          imagePullPolicy: IfNotPresent
          name: fluentd-es
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/log
            name: varlog
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
          - mountPath: /host/lib
            name: libsystemddir
            readOnly: true
          - mountPath: /etc/fluent/config.d
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-es
        serviceAccountName: fluentd-es
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
        - hostPath:
            path: /usr/lib64
            type: ""
          name: libsystemddir
        - configMap:
            defaultMode: 420
            name: fluentd-es-config-v0.1.1
          name: config-volume
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 1
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1beta2
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "446859"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/daemonsets/kube-proxy
    uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
          - --cluster-cidr=192.168.0.0/16
          image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          value: "true"
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: "851805358"
      release: my-nginx-ingress
    name: my-nginx-ingress-controller-d95d4979d
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-controller
      uid: a14f1a1b-1108-11e8-871b-005056af9e97
    resourceVersion: "473487"
    selfLink: /apis/apps/v1beta2/namespaces/default/replicasets/my-nginx-ingress-controller-d95d4979d
    uid: a14fe756-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: "851805358"
        release: my-nginx-ingress
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: "851805358"
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: "4118459331"
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend-855d89f775
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-default-backend
      uid: a14f9e60-1108-11e8-871b-005056af9e97
    resourceVersion: "473452"
    selfLink: /apis/apps/v1beta2/namespaces/default/replicasets/my-nginx-ingress-default-backend-855d89f775
    uid: a151592a-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        pod-template-hash: "4118459331"
        release: my-nginx-ingress
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          pod-template-hash: "4118459331"
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: calico-typha
      pod-template-hash: "2163023612"
    name: calico-typha-65b7467b56
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: f58c96a5-0dd1-11e8-871b-005056af9e97
    resourceVersion: "340"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/calico-typha-65b7467b56
    uid: fcf331c4-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: "2163023612"
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
          pod-template-hash: "2163023612"
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kibana-logging
      pod-template-hash: "3237905315"
    name: kibana-logging-767cf49759
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kibana-logging
      uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
    resourceVersion: "806"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/kibana-logging-767cf49759
    uid: fcf34643-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kibana-logging
        pod-template-hash: "3237905315"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
          pod-template-hash: "3237905315"
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: "1016706980"
    name: kube-dns-545bc4bfd4
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
    resourceVersion: "852"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/kube-dns-545bc4bfd4
    uid: fcf396ae-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: "1016706980"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: "1016706980"
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
      pod-template-hash: "3354704202"
    name: kubernetes-dashboard-7798c48646
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubernetes-dashboard
      uid: f79d83b7-0dd1-11e8-871b-005056af9e97
    resourceVersion: "773"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/kubernetes-dashboard-7798c48646
    uid: fcf3720c-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: "3354704202"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
          pod-template-hash: "3354704202"
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:24Z
    generation: 1
    labels:
      app: helm
      name: tiller
      pod-template-hash: "1027952527"
    name: tiller-deploy-546cf9696c
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1068"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/tiller-deploy-546cf9696c
    uid: 54347a31-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "1027952527"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "1027952527"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1beta2
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
      pod-template-hash: "165821739"
    name: tiller-deploy-5b9d65c7f
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1037"
    selfLink: /apis/apps/v1beta2/namespaces/kube-system/replicasets/tiller-deploy-5b9d65c7f
    uid: 499996c4-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "165821739"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "165821739"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"534"}}
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-t972j
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "762"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-t972j
    uid: 09b09390-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:38Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://133d74a2f67c41276c95642a7ee2cc640806f1e710e38d528b615833a7cc6832
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:39Z
    hostIP: 10.10.97.62
    initContainerStatuses:
    - containerID: docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    - containerID: docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
          exitCode: 0
          finishedAt: 2018-02-09T19:47:39Z
          reason: Completed
          startedAt: 2018-02-09T19:47:39Z
    phase: Running
    podIP: 192.168.2.3
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"400"}}
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-tgb9r
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "753"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-tgb9r
    uid: 09937871-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:37Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9f4d367ff709fa795b1d3aa31b2a97474c57c8235735c1240f14aa1142059e4f
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:40Z
    hostIP: 10.10.97.46
    initContainerStatuses:
    - containerID: docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
          exitCode: 0
          finishedAt: 2018-02-09T19:47:37Z
          reason: Completed
          startedAt: 2018-02-09T19:47:37Z
    - containerID: docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    phase: Running
    podIP: 192.168.1.2
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"263"}}
    creationTimestamp: 2018-02-09T19:46:58Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-vlgmz
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "751"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-vlgmz
    uid: fd2456e4-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:10Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://db0ba7b2436c0809e058838a7510f8150b381d0c473ce93a46fa90d77c77ed55
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:40Z
    hostIP: 10.10.97.20
    initContainerStatuses:
    - containerID: docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    - containerID: docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    phase: Running
    podIP: 192.168.0.2
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:58Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"kube-keepalived-vip","uid":"41d5383d-10f0-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"458290"}}
    creationTimestamp: 2018-02-13T19:01:11Z
    generateName: kube-keepalived-vip-
    labels:
      controller-revision-hash: "617734581"
      name: kube-keepalived-vip
      pod-template-generation: "1"
    name: kube-keepalived-vip-fph6b
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-keepalived-vip
      uid: 41d5383d-10f0-11e8-871b-005056af9e97
    resourceVersion: "473537"
    selfLink: /api/v1/namespaces/default/pods/kube-keepalived-vip-fph6b
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - --services-configmap=default/vip-configmap
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/kube-keepalived-vip:0.11
      imagePullPolicy: Always
      name: kube-keepalived-vip
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: modules
        readOnly: true
      - mountPath: /dev
        name: dev
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-keepalived-vip-token-2w47w
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-keepalived-vip
    serviceAccountName: kube-keepalived-vip
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: modules
    - hostPath:
        path: /dev
        type: ""
      name: dev
    - name: kube-keepalived-vip-token-2w47w
      secret:
        defaultMode: 420
        secretName: kube-keepalived-vip-token-2w47w
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T19:01:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:56:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T19:01:14Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://00aa555e894c867aef4c94268145997e0d4e0fb8e2bc01194d34c2ee495a4137
      image: k8s.gcr.io/kube-keepalived-vip:0.11
      imageID: docker-pullable://k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
      lastState:
        terminated:
          containerID: docker://8fa004a89dc92a38b29575c75273e73f34b1eea92b9d605a1ea13ce6477ba77d
          exitCode: 0
          finishedAt: 2018-02-13T21:55:53Z
          reason: Completed
          startedAt: 2018-02-13T21:55:47Z
      name: kube-keepalived-vip
      ready: true
      restartCount: 3
      state:
        running:
          startedAt: 2018-02-13T21:56:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: BestEffort
    startTime: 2018-02-13T19:01:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
      cni.projectcalico.org/podIP: 192.168.2.19/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-controller-d95d4979d","uid":"a14fe756-1108-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"473413"}}
    creationTimestamp: 2018-02-13T21:55:39Z
    generateName: my-nginx-ingress-controller-d95d4979d-
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: "851805358"
      release: my-nginx-ingress
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-nginx-ingress-controller-d95d4979d
      uid: a14fe756-1108-11e8-871b-005056af9e97
    resourceVersion: "473485"
    selfLink: /api/v1/namespaces/default/pods/my-nginx-ingress-controller-d95d4979d-kzgk7
    uid: a1522304-1108-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=default/my-nginx-ingress-default-backend
      - --election-id=ingress-controller-leader
      - --ingress-class=nginx
      - --configmap=default/my-nginx-ingress-controller
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: nginx-ingress-controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: my-nginx-ingress-token-9ffd9
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: my-nginx-ingress
    serviceAccountName: my-nginx-ingress
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: my-nginx-ingress-token-9ffd9
      secret:
        defaultMode: 420
        secretName: my-nginx-ingress-token-9ffd9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:40Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:54Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:39Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b839ab72ad639b16185c8238762de9213cf04afa7c2aa93a80b6a23d87f36faa
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      imageID: docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
      lastState: {}
      name: nginx-ingress-controller
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-13T21:55:41Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.19
    qosClass: BestEffort
    startTime: 2018-02-13T21:55:40Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.18/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-default-backend-855d89f775","uid":"a151592a-1108-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"473418"}}
    creationTimestamp: 2018-02-13T21:55:39Z
    generateName: my-nginx-ingress-default-backend-855d89f775-
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: "4118459331"
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-nginx-ingress-default-backend-855d89f775
      uid: a151592a-1108-11e8-871b-005056af9e97
    resourceVersion: "473451"
    selfLink: /api/v1/namespaces/default/pods/my-nginx-ingress-default-backend-855d89f775-p8jk6
    uid: a155085d-1108-11e8-871b-005056af9e97
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend:1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: nginx-ingress-default-backend
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:40Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:42Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:39Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://59a4fe54449af40c3801c5057d82fe3e17c6e7ea7e4b5b2020e4bb82fe98ff5c
      image: k8s.gcr.io/defaultbackend:1.3
      imageID: docker-pullable://k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
      lastState: {}
      name: nginx-ingress-default-backend
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-13T21:55:41Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.18
    qosClass: BestEffort
    startTime: 2018-02-13T21:55:40Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.15/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-96442
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327037"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-96442
    uid: 1666089f-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:16Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1d0b5fdc1a237a0d73e3fb736d0730125d76e145a5926d2c77210aa174e73355
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:15Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.15
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.14/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-fxj6x
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327030"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-fxj6x
    uid: 1664100f-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:15Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://353edef9a7f5eb68513f5a9b463dd7535ebc9abd1a7eec03941101351d84694b
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:14Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.14
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.52/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-v8rxv
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327028"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-v8rxv
    uid: 16662cc9-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:14Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8f455e857f54947f253ab4a470b00b34f55086b42dd75e1a40ef0fc9d0a5ba11
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:14Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 192.168.1.52
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"526"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-2cpcv
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "890"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-2cpcv
    uid: 09acb8b2-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:48:08Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:26Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b9ba3c46ec16b030166cc9085a61ea34f8d39462ee880c5b088ab51332d36a53
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:24Z
    - containerID: docker://bcd4205803fe0b6f9d0d06b6f7d43e23f0bfa48a56a24efe5a118873f4ed6e55
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"208"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-g99qc
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "787"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-g99qc
    uid: fd057bd7-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:57Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:00Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://80863955dfa346126c501bfde786831975a65e4188f4697348220b10f902b15f
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:58Z
    - containerID: docker://3e78bce5869ac82e5283dd30c852640141f5914d3d2459534e1c94be1ed53470
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:59Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:57Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"365"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-kw9hm
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "880"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-kw9hm
    uid: 098dcdac-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:48:02Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:26Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://153accc108eab50e06c45a0aae1788230477384976ef825a64b55f6114d85856
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    - containerID: docker://185c93be449c0f711ce22e774a8ec43d4fd993cbca35bd4445cb286d9941814f
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:26Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 10.10.97.46
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-005056af9e97","apiVersion":"apps/v1beta1","resourceVersion":"247"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: elasticsearch-logging-
    labels:
      controller-revision-hash: elasticsearch-logging-7978c6964c
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging-0
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch-logging
      uid: f66f6df9-0dd1-11e8-871b-005056af9e97
    resourceVersion: "810"
    selfLink: /api/v1/namespaces/kube-system/pods/elasticsearch-logging-0
    uid: fd00ccb1-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging
      ports:
      - containerPort: 9200
        name: db
        protocol: TCP
      - containerPort: 9300
        name: transport
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: elasticsearch-logging
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    dnsPolicy: ClusterFirst
    hostname: elasticsearch-logging-0
    initContainers:
    - command:
      - /sbin/sysctl
      - -w
      - vm.max_map_count=262144
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging-init
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: elasticsearch-logging
    serviceAccountName: elasticsearch-logging
    subdomain: elasticsearch-logging
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: elasticsearch-logging
    - name: elasticsearch-logging-token-6cmks
      secret:
        defaultMode: 420
        secretName: elasticsearch-logging-token-6cmks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:45Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:29Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://26e38830a98fcd14580f1349b2e0643db39093b17e7f086e29cce5c07fe75d6a
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      lastState: {}
      name: elasticsearch-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    hostIP: 10.10.97.46
    initContainerStatuses:
    - containerID: docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      lastState: {}
      name: elasticsearch-logging-init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
          exitCode: 0
          finishedAt: 2018-02-09T19:47:43Z
          reason: Completed
          startedAt: 2018-02-09T19:47:43Z
    phase: Running
    podIP: 192.168.1.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:28Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.5/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-005056af9e97","apiVersion":"apps/v1beta1","resourceVersion":"366"}}
    creationTimestamp: 2018-02-09T19:47:45Z
    generateName: elasticsearch-logging-
    labels:
      controller-revision-hash: elasticsearch-logging-7978c6964c
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging-1
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch-logging
      uid: f66f6df9-0dd1-11e8-871b-005056af9e97
    resourceVersion: "841"
    selfLink: /api/v1/namespaces/kube-system/pods/elasticsearch-logging-1
    uid: 1982328b-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging
      ports:
      - containerPort: 9200
        name: db
        protocol: TCP
      - containerPort: 9300
        name: transport
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: elasticsearch-logging
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    dnsPolicy: ClusterFirst
    hostname: elasticsearch-logging-1
    initContainers:
    - command:
      - /sbin/sysctl
      - -w
      - vm.max_map_count=262144
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging-init
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: elasticsearch-logging
    serviceAccountName: elasticsearch-logging
    subdomain: elasticsearch-logging
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: elasticsearch-logging
    - name: elasticsearch-logging-token-6cmks
      secret:
        defaultMode: 420
        secretName: elasticsearch-logging-token-6cmks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:46Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:47Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:45Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://598eb6f9ef563829dcef2355b141ea06eb162e11e8d429236c9426580c22620b
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      lastState: {}
      name: elasticsearch-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:47Z
    hostIP: 10.10.97.62
    initContainerStatuses:
    - containerID: docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      lastState: {}
      name: elasticsearch-logging-init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
          exitCode: 0
          finishedAt: 2018-02-09T19:47:46Z
          reason: Completed
          startedAt: 2018-02-09T19:47:46Z
    phase: Running
    podIP: 192.168.2.5
    qosClass: Burstable
    startTime: 2018-02-09T19:47:44Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: d76e26fba3bf2bfd215eb29011d55250
      kubernetes.io/config.mirror: d76e26fba3bf2bfd215eb29011d55250
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807466634Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:50Z
    labels:
      component: etcd
      tier: control-plane
    name: etcd-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "858"
    selfLink: /api/v1/namespaces/kube-system/pods/etcd-vhosakot1-m51b5b468be
    uid: 1c935fc9-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - etcd
      - --listen-client-urls=http://127.0.0.1:2379
      - --advertise-client-urls=http://127.0.0.1:2379
      - --data-dir=/var/lib/etcd
      image: gcr.io/google_containers/etcd-amd64:3.0.17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2379
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://11eb72aba33cbf28f02623a3e01c996da3542f375262a338c769dc91417a8865
      image: gcr.io/google_containers/etcd-amd64:3.0.17
      imageID: docker-pullable://gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"532"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: fluentd-es-v2.0.2-
    labels:
      controller-revision-hash: "1193446001"
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
      version: v2.0.2
    name: fluentd-es-v2.0.2-9bmjw
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-es-v2.0.2
      uid: f626704d-0dd1-11e8-871b-005056af9e97
    resourceVersion: "716"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-es-v2.0.2-9bmjw
    uid: 09afd5c6-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: FLUENTD_ARGS
        value: --no-supervisor -q
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imagePullPolicy: IfNotPresent
      name: fluentd-es
      resources:
        limits:
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /host/lib
        name: libsystemddir
        readOnly: true
      - mountPath: /etc/fluent/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-es-token-zw92g
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-es
    serviceAccountName: fluentd-es
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - hostPath:
        path: /usr/lib64
        type: ""
      name: libsystemddir
    - configMap:
        defaultMode: 420
        name: fluentd-es-config-v0.1.1
      name: config-volume
    - name: fluentd-es-token-zw92g
      secret:
        defaultMode: 420
        secretName: fluentd-es-token-zw92g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:36Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:36Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5a66378b3e72ce2fd8df7b7181818302c86b7cb85ce53c1ac81410c171169e70
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      lastState: {}
      name: fluentd-es
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:36Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.2
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"386"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: fluentd-es-v2.0.2-
    labels:
      controller-revision-hash: "1193446001"
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
      version: v2.0.2
    name: fluentd-es-v2.0.2-kmbqt
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-es-v2.0.2
      uid: f626704d-0dd1-11e8-871b-005056af9e97
    resourceVersion: "764"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-es-v2.0.2-kmbqt
    uid: 09911ebd-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: FLUENTD_ARGS
        value: --no-supervisor -q
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imagePullPolicy: IfNotPresent
      name: fluentd-es
      resources:
        limits:
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /host/lib
        name: libsystemddir
        readOnly: true
      - mountPath: /etc/fluent/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-es-token-zw92g
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-es
    serviceAccountName: fluentd-es
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - hostPath:
        path: /usr/lib64
        type: ""
      name: libsystemddir
    - configMap:
        defaultMode: 420
        name: fluentd-es-config-v0.1.1
      name: config-volume
    - name: fluentd-es-token-zw92g
      secret:
        defaultMode: 420
        secretName: fluentd-es-token-zw92g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c650008103084454c1d238468843c301b568d1919e3ae02c60338e1303189954
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      lastState: {}
      name: fluentd-es
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:39Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 192.168.1.3
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kibana-logging-767cf49759","uid":"fcf34643-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"338"}}
    creationTimestamp: 2018-02-09T19:46:58Z
    generateName: kibana-logging-767cf49759-
    labels:
      k8s-app: kibana-logging
      pod-template-hash: "3237905315"
    name: kibana-logging-767cf49759-f8zjt
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kibana-logging-767cf49759
      uid: fcf34643-0dd1-11e8-871b-005056af9e97
    resourceVersion: "803"
    selfLink: /api/v1/namespaces/kube-system/pods/kibana-logging-767cf49759-f8zjt
    uid: fd2570b3-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: ELASTICSEARCH_URL
        value: http://elasticsearch-logging:9200
      - name: XPACK_MONITORING_ENABLED
        value: "false"
      - name: XPACK_SECURITY_ENABLED
        value: "false"
      image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      imagePullPolicy: IfNotPresent
      name: kibana-logging
      ports:
      - containerPort: 5601
        name: ui
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kxl84
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kxl84
      secret:
        defaultMode: 420
        secretName: default-token-kxl84
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:28Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:29Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://32e30c8b573b980690ec999dbb4f981dec9caadfb001356bd428b26187c1a451
      image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
      lastState: {}
      name: kibana-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:44Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:28Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 3f93d95099662b6d548520f6873d8454
      kubernetes.io/config.mirror: 3f93d95099662b6d548520f6873d8454
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807479364Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:45Z
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "859"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-apiserver-vhosakot1-m51b5b468be
    uid: 199882ac-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-apiserver
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --allow-privileged=true
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --requestheader-username-headers=X-Remote-User
      - --requestheader-group-headers=X-Remote-Group
      - --service-cluster-ip-range=10.96.0.0/12
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --insecure-port=0
      - --requestheader-allowed-names=front-proxy-client
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      - --secure-port=6443
      - --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --advertise-address=10.10.97.20
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --enable-bootstrap-token-auth=true
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --authorization-mode=Node,RBAC
      - --etcd-servers=http://127.0.0.1:2379
      image: gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      resources:
        requests:
          cpu: 250m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a04e800ebb80645cd9826489e7d56e778b23f495984e5d32822831c1178e0915
      image: gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: ab7c88cd37d9d92c34bb35c6a377a96f
      kubernetes.io/config.mirror: ab7c88cd37d9d92c34bb35c6a377a96f
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807482481Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:44Z
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "861"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-controller-manager-vhosakot1-m51b5b468be
    uid: 18ff898f-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --address=127.0.0.1
      - --controllers=*,bootstrapsigner,tokencleaner
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --leader-elect=true
      - --use-service-account-credentials=true
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --allocate-node-cidrs=true
      - --cluster-cidr=192.168.0.0/16
      - --node-cidr-mask-size=24
      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10252
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://952f4f63dea04d6c2c7ed47c7c6d4a720b7eea4f69f43d3daaf0113ab685163c
      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kube-dns-545bc4bfd4","uid":"fcf396ae-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"332"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kube-dns-545bc4bfd4-
    labels:
      k8s-app: kube-dns
      pod-template-hash: "1016706980"
    name: kube-dns-545bc4bfd4-rzpz4
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-545bc4bfd4
      uid: fcf396ae-0dd1-11e8-871b-005056af9e97
    resourceVersion: "851"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-rzpz4
    uid: fd048a95-0dd1-11e8-871b-005056af9e97
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: beta.kubernetes.io/arch
              operator: In
              values:
              - amd64
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
      image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    dnsPolicy: Default
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-66tfx
      secret:
        defaultMode: 420
        secretName: kube-dns-token-66tfx
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:13Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:50Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:13Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ead0625a9610ac4f66bee25ee8ed4d6b5418bc31d7ac8b7e6eb3d38b339b3e7e
      image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    - containerID: docker://585a3048cfcdc2498ba543ef02124e15c7c7e8ea79f84f5df37d87e46b6cfb19
      image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
      lastState: {}
      name: kubedns
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    - containerID: docker://313b3d5c54c63059e63c3bc2b8d23d2902e8033a007ebd78f015ae22556f9603
      image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
      lastState: {}
      name: sidecar
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:46Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 192.168.0.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:13Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"189"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-8vlsv
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "440"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-8vlsv
    uid: fd05a03b-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:57Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:59Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:59Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bf6367debcebe328805c54a362b8767b8e8810c327094c53b98ea7a59c4e5514
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:58Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:57Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"432"}}
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-nfkkf
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "643"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-nfkkf
    uid: 098dc202-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d47a6b44a292ed53cc15d6e72546c50f08a3037a306a847e63f39660af3b8a4a
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 10.10.97.46
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"524"}}
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-q8ng8
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "642"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-q8ng8
    uid: 09ac7c1f-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3ac1e13578a22c5fe5e2f3c8209af657906680224d8faf34cf95880e1a320243
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: e554495c6f8701f21accd04866090b05
      kubernetes.io/config.mirror: e554495c6f8701f21accd04866090b05
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807485209Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:43Z
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "860"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-scheduler-vhosakot1-m51b5b468be
    uid: 1866988a-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-scheduler
      - --address=127.0.0.1
      - --leader-elect=true
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      image: gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dc6d33ae2ff92d6b18190f6edd6d943f352b37dabb80d6785d1e40238ec1530d
      image: gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kubernetes-dashboard-7798c48646","uid":"fcf3720c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"339"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kubernetes-dashboard-7798c48646-
    labels:
      k8s-app: kubernetes-dashboard
      pod-template-hash: "3354704202"
    name: kubernetes-dashboard-7798c48646-rjmch
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubernetes-dashboard-7798c48646
      uid: fcf3720c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "771"
    selfLink: /api/v1/namespaces/kube-system/pods/kubernetes-dashboard-7798c48646-rjmch
    uid: fcff0932-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - --auto-generate-certificates
      image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8443
          scheme: HTTPS
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: kubernetes-dashboard
      ports:
      - containerPort: 8443
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: kubernetes-dashboard-certs
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kubernetes-dashboard-token-bjcwc
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubernetes-dashboard
    serviceAccountName: kubernetes-dashboard
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kubernetes-dashboard-certs
      secret:
        defaultMode: 420
        secretName: kubernetes-dashboard-certs
    - emptyDir: {}
      name: tmp-volume
    - name: kubernetes-dashboard-token-bjcwc
      secret:
        defaultMode: 420
        secretName: kubernetes-dashboard-token-bjcwc
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:42Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://15d41739dac144360b50543a9b2ed5269720107fc11c5e19c2c9a61716c4b626
      image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      imageID: docker-pullable://k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
      lastState: {}
      name: kubernetes-dashboard
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:42Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 192.168.0.3
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.6/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"tiller-deploy-546cf9696c","uid":"54347a31-0dd2-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"1022"}}
    creationTimestamp: 2018-02-09T19:49:24Z
    generateName: tiller-deploy-546cf9696c-
    labels:
      app: helm
      name: tiller
      pod-template-hash: "1027952527"
    name: tiller-deploy-546cf9696c-w4kq6
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tiller-deploy-546cf9696c
      uid: 54347a31-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1066"
    selfLink: /api/v1/namespaces/kube-system/pods/tiller-deploy-546cf9696c-w4kq6
    uid: 5435726c-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: TILLER_NAMESPACE
        value: kube-system
      - name: TILLER_HISTORY_MAX
        value: "0"
      image: gcr.io/kubernetes-helm/tiller:v2.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /liveness
          port: 44135
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: tiller
      ports:
      - containerPort: 44134
        name: tiller
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 44135
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: tiller-token-chfrl
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: tiller
    serviceAccountName: tiller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tiller-token-chfrl
      secret:
        defaultMode: 420
        secretName: tiller-token-chfrl
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:31Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:24Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9aa05a99a4fe19195842dbbe5f406912fdd380339d496b401a690d15d6033379
      image: gcr.io/kubernetes-helm/tiller:v2.7.2
      imageID: docker-pullable://gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
      lastState: {}
      name: tiller
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:49:27Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.6
    qosClass: BestEffort
    startTime: 2018-02-09T19:49:23Z
- apiVersion: v1
  kind: ReplicationController
  metadata:
    creationTimestamp: 2018-02-12T16:02:12Z
    generation: 1
    labels:
      app: tea
    name: tea-rc
    namespace: default
    resourceVersion: "327038"
    selfLink: /api/v1/namespaces/default/replicationcontrollers/tea-rc
    uid: 16628069-100e-11e8-871b-005056af9e97
  spec:
    replicas: 3
    selector:
      app: tea
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: tea
      spec:
        containers:
        - image: nginxdemos/hello
          imagePullPolicy: Always
          name: tea
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:46:40Z
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "31"
    selfLink: /api/v1/namespaces/default/services/kubernetes
    uid: f2ea6e5d-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473407"
    selfLink: /api/v1/namespaces/default/services/my-nginx-ingress-controller
    uid: a149d7a0-1108-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.107.38.119
    externalIPs:
    - 10.10.97.200
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    selector:
      app: nginx-ingress
      component: controller
      release: my-nginx-ingress
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473410"
    selfLink: /api/v1/namespaces/default/services/my-nginx-ingress-default-backend
    uid: a14e0e37-1108-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.69.91
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: nginx-ingress
      component: default-backend
      release: my-nginx-ingress
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-12T16:02:17Z
    labels:
      app: tea
    name: tea-svc
    namespace: default
    resourceVersion: "327042"
    selfLink: /api/v1/namespaces/default/services/tea-svc
    uid: 198efef6-100e-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.196.71
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: tea
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"spec":{"ports":[{"name":"calico-typha","port":5473,"protocol":"TCP","targetPort":"calico-typha"}],"selector":{"k8s-app":"calico-typha"}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "206"
    selfLink: /api/v1/namespaces/kube-system/services/calico-typha
    uid: f587a909-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.99.51.177
    ports:
    - name: calico-typha
      port: 5473
      protocol: TCP
      targetPort: calico-typha
    selector:
      k8s-app: calico-typha
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Elasticsearch"},"name":"elasticsearch-logging","namespace":"kube-system"},"spec":{"ports":[{"port":9200,"protocol":"TCP","targetPort":"db"}],"selector":{"k8s-app":"elasticsearch-logging"}}}
    creationTimestamp: 2018-02-09T19:46:47Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Elasticsearch
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "252"
    selfLink: /api/v1/namespaces/kube-system/services/elasticsearch-logging
    uid: f69f1197-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.105.23.60
    ports:
    - port: 9200
      protocol: TCP
      targetPort: db
    selector:
      k8s-app: elasticsearch-logging
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Kibana"},"name":"kibana-logging","namespace":"kube-system"},"spec":{"ports":[{"nodePort":30601,"port":5601,"protocol":"TCP","targetPort":"ui"}],"selector":{"k8s-app":"kibana-logging"},"type":"NodePort"}}
    creationTimestamp: 2018-02-09T19:46:47Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Kibana
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "261"
    selfLink: /api/v1/namespaces/kube-system/services/kibana-logging
    uid: f70bbbe0-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.208.116
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 30601
      port: 5601
      protocol: TCP
      targetPort: ui
    selector:
      k8s-app: kibana-logging
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "184"
    selfLink: /api/v1/namespaces/kube-system/services/kube-dns
    uid: f4e7eb3e-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.0.10
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"},"spec":{"ports":[{"nodePort":31443,"port":443,"targetPort":8443}],"selector":{"k8s-app":"kubernetes-dashboard"},"type":"NodePort"}}
    creationTimestamp: 2018-02-09T19:46:48Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "274"
    selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard
    uid: f7a33124-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.106.31.65
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 31443
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      k8s-app: kubernetes-dashboard
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:49:06Z
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "979"
    selfLink: /api/v1/namespaces/kube-system/services/tiller-deploy
    uid: 49a392f0-0dd2-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.104.87.82
    ports:
    - name: tiller
      port: 44134
      protocol: TCP
      targetPort: tiller
    selector:
      app: helm
      name: tiller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get certificatesigningrequests --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: certificates.k8s.io/v1beta1
  kind: CertificateSigningRequest
  metadata:
    creationTimestamp: 2018-02-09T19:47:18Z
    name: node-csr-8Aa5zv54iOqpJoBvflvn2nWYyB8c7zTe1Ptsk8YFepY
    namespace: ""
    resourceVersion: "516"
    selfLink: /apis/certificates.k8s.io/v1beta1/certificatesigningrequests/node-csr-8Aa5zv54iOqpJoBvflvn2nWYyB8c7zTe1Ptsk8YFepY
    uid: 0979d37f-0dd2-11e8-871b-005056af9e97
  spec:
    groups:
    - system:bootstrappers
    - system:bootstrappers:kubeadm:default-node-token
    - system:authenticated
    request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlIK01JR2xBZ0VBTUVNeEZUQVRCZ05WQkFvVERITjVjM1JsYlRwdWIyUmxjekVxTUNnR0ExVUVBeE1oYzNsegpkR1Z0T201dlpHVTZkbWh2YzJGcmIzUXhMWGRqT0RCa00yVTFZV0kyTUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJCnpqMERBUWNEUWdBRTNPVUs2a0FzQTIzWU91ZkNmT3dYM0EvQTc5SFB0NmFubzB4bnpQeWNDOTlxNVlNSTVmYjIKSzVIc2lYYWI5alJ2bW9OcUlObGNzVHB0bkVYVm9aMDEyYUFBTUFvR0NDcUdTTTQ5QkFNQ0EwZ0FNRVVDSURIegppWTJZaWZtdnZrdDh1ZjdZZzh4TXhwYzFOWGwzQm5mNklUYXpOdS9MQWlFQWx1cHd1QkRUamZIbWlLMGpCckZjClBEejZENHg2Y1BTUURSN2hsZ2pnNFpjPQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
    usages:
    - digital signature
    - key encipherment
    - client auth
    username: system:bootstrap:e06da1
  status:
    certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNiekNDQVZlZ0F3SUJBZ0lVRFNsYWNMVTdDQ3NXMG5mL09PWWQ4N2h1bGowd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZURVRNQkVHQTFVRUF4TUthM1ZpWlhKdVpYUmxjekFlRncweE9EQXlNRGt4T1RReU1EQmFGdzB4T1RBeQpNRGt4T1RReU1EQmFNRU14RlRBVEJnTlZCQW9UREhONWMzUmxiVHB1YjJSbGN6RXFNQ2dHQTFVRUF4TWhjM2x6CmRHVnRPbTV2WkdVNmRtaHZjMkZyYjNReExYZGpPREJrTTJVMVlXSTJNRmt3RXdZSEtvWkl6ajBDQVFZSUtvWkkKemowREFRY0RRZ0FFM09VSzZrQXNBMjNZT3VmQ2ZPd1gzQS9BNzlIUHQ2YW5vMHhuelB5Y0M5OXE1WU1JNWZiMgpLNUhzaVhhYjlqUnZtb05xSU5sY3NUcHRuRVhWb1owMTJhTlVNRkl3RGdZRFZSMFBBUUgvQkFRREFnV2dNQk1HCkExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SFFZRFZSME9CQllFRkcxWUdhY1MKemtrY0xsRDBSV2Y4akFEN2VEcnhNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUNvemI3RGhCM21QeXFBMnJJSwo0UjhGeVFacmJDM0hZUkxOVURieGlaS0VhV0tDZXVSbnpMcStWNUdSS1dSdnNyaVhUL1UydGZUb1lBejk4Q1YxCjJWdkw2d0dKSHMxNVFmRjdmakdkQUpVNDdzKy9XTS82bzRsNGMza1VydG9ab2pxMDFneWtSZVZPeEt0UW5SWjYKZWV1Y1Nidmlqd0YrUkZRMEpIQzhHRDlKK1NqeCtmTUZQVlFGRlBrT0pxcVo0SjlQWnE2SVg1VVJoQ1hsT2pCWQpxalgyY3haclF2dUVISnVzQ1R6WkRNaHRaL3hxUHZvRlFITXlJZXhUc2hQZDgxcFJFdkh3RXR3d0hRck1kV2lKCmtUK3NxbU1MREI5dXB3bTRLWkVtT2JEblp5OE1jVGZ2UVgrbUZRSU14bVNmZGd2M3lkTHcvRzlkbTZ4NDlNR0gKM09NVQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    conditions:
    - lastUpdateTime: null
      message: Auto approving kubelet client certificate after SubjectAccessReview.
      reason: AutoApproved
      type: Approved
- apiVersion: certificates.k8s.io/v1beta1
  kind: CertificateSigningRequest
  metadata:
    creationTimestamp: 2018-02-09T19:47:18Z
    name: node-csr-rU0zAAXNDV-3D7DihpcOemzwANHaDyeaLGBYVz8lO1U
    namespace: ""
    resourceVersion: "508"
    selfLink: /apis/certificates.k8s.io/v1beta1/certificatesigningrequests/node-csr-rU0zAAXNDV-3D7DihpcOemzwANHaDyeaLGBYVz8lO1U
    uid: 09453fd3-0dd2-11e8-871b-005056af9e97
  spec:
    groups:
    - system:bootstrappers
    - system:bootstrappers:kubeadm:default-node-token
    - system:authenticated
    request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlIOU1JR2xBZ0VBTUVNeEZUQVRCZ05WQkFvVERITjVjM1JsYlRwdWIyUmxjekVxTUNnR0ExVUVBeE1oYzNsegpkR1Z0T201dlpHVTZkbWh2YzJGcmIzUXhMWGRsTW1RNE5tWmhaV0l5TUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJCnpqMERBUWNEUWdBRXdEN0pSZjZOMmdaUFNZVDlvUkxnVWtzRkk5S3pJY2JiNE56clVGTWkveExvd3lyNUZLM3cKZnU3WWNabTJtbHowRlRJRXJIbms0UDJPaXpYOEg1cFkyNkFBTUFvR0NDcUdTTTQ5QkFNQ0EwY0FNRVFDSURoOQowY0lYODhHaEllM2gycWVmb3pzU2trQzNQai9uaHhQamIwQTlDUU5wQWlCQ055SG5nSkxOS2pFdDZFdm4yNmp2CnVHY2VScS9xdHNKSmZPQ3JXanB6emc9PQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
    usages:
    - digital signature
    - key encipherment
    - client auth
    username: system:bootstrap:e06da1
  status:
    certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNiekNDQVZlZ0F3SUJBZ0lVQVUxQXErRDlpODdtVElpRCtXZzlpY3BhK1NNd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZURVRNQkVHQTFVRUF4TUthM1ZpWlhKdVpYUmxjekFlRncweE9EQXlNRGt4T1RReU1EQmFGdzB4T1RBeQpNRGt4T1RReU1EQmFNRU14RlRBVEJnTlZCQW9UREhONWMzUmxiVHB1YjJSbGN6RXFNQ2dHQTFVRUF4TWhjM2x6CmRHVnRPbTV2WkdVNmRtaHZjMkZyYjNReExYZGxNbVE0Tm1aaFpXSXlNRmt3RXdZSEtvWkl6ajBDQVFZSUtvWkkKemowREFRY0RRZ0FFd0Q3SlJmNk4yZ1pQU1lUOW9STGdVa3NGSTlLekljYmI0TnpyVUZNaS94TG93eXI1RkszdwpmdTdZY1ptMm1sejBGVElFckhuazRQMk9pelg4SDVwWTI2TlVNRkl3RGdZRFZSMFBBUUgvQkFRREFnV2dNQk1HCkExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SFFZRFZSME9CQllFRkY1UlZ2YWkKb0o4d2tkZ0EyT1ZqSzFYNVFMajFNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJUNEVkcHhiYmtpeDkxblViOApVeHhuR0FhTXIwRndHOGtxdDZ1UWFIZmdWME9LQXVPNWc1cFUva1dNK3l4WHNmdjNmR0hHWU02bXFEYVZ6WXBZCjBLQ01YKzhTbzBtZk1DeUdtTmtwOXA0MDRYMEo5UHNJVUZVcGQyTkxhU1hIYitqL3ZiMkswTmh2WUU5R2lKY2IKT3AxRlFiamlRRC9LcEl6dGRFR0QvOFBpcDNKbk9aM09LU0ZHZkNKankyT1NLUkMwWnFTRy96WGhsNk5xVWhtdgpEMW8yQUppMXFPMnZsWkt1RDYzdkpJODlqQXRkdkZlNlZOeHdxY3pLTVF5TS9WSXhNTVlMWGR6c20xc1JWemkvCnEvcVhiVzVPMk9LUUhUWTJkVkJ4UzRqNlVsaytINU03TXlHYUVxa1h4WG83cXVveUNPNEhPMG5nZ2xtOVV0WXMKOEhYVQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    conditions:
    - lastUpdateTime: null
      message: Auto approving kubelet client certificate after SubjectAccessReview.
      reason: AutoApproved
      type: Approved
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get clusterrolebindings --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"name":"calico-node","namespace":""},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"ClusterRole","name":"calico-node"},"subjects":[{"kind":"ServiceAccount","name":"calico-node","namespace":"kube-system"}]}
    creationTimestamp: 2018-02-09T19:46:44Z
    name: calico-node
    namespace: ""
    resourceVersion: "201"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/calico-node
    uid: f550034c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-node
  subjects:
  - kind: ServiceAccount
    name: calico-node
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: cluster-admin
    namespace: ""
    resourceVersion: "88"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
    uid: f3891b5f-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:masters
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true"},"name":"elasticsearch-logging","namespace":""},"roleRef":{"apiGroup":"","kind":"ClusterRole","name":"elasticsearch-logging"},"subjects":[{"apiGroup":"","kind":"ServiceAccount","name":"elasticsearch-logging","namespace":"kube-system"}]}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
    name: elasticsearch-logging
    namespace: ""
    resourceVersion: "246"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/elasticsearch-logging
    uid: f660060f-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: elasticsearch-logging
  subjects:
  - kind: ServiceAccount
    name: elasticsearch-logging
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true"},"name":"fluentd-es","namespace":""},"roleRef":{"apiGroup":"","kind":"ClusterRole","name":"fluentd-es"},"subjects":[{"apiGroup":"","kind":"ServiceAccount","name":"fluentd-es","namespace":"kube-system"}]}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
    name: fluentd-es
    namespace: ""
    resourceVersion: "238"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/fluentd-es
    uid: f6224c25-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: fluentd-es
  subjects:
  - kind: ServiceAccount
    name: fluentd-es
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-13T15:34:52Z
    name: kube-keepalived-vip
    namespace: ""
    resourceVersion: "439408"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kube-keepalived-vip
    uid: 6f322fe9-10d3-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-keepalived-vip
  subjects:
  - kind: ServiceAccount
    name: kube-keepalived-vip
    namespace: default
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:kubelet-bootstrap
    namespace: ""
    resourceVersion: "170"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Akubelet-bootstrap
    uid: f4d26764-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:node-bootstrapper
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:bootstrappers:kubeadm:default-node-token
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:node-autoapprove-bootstrap
    namespace: ""
    resourceVersion: "171"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-autoapprove-bootstrap
    uid: f4d2db36-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:bootstrappers:kubeadm:default-node-token
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:node-autoapprove-certificate-rotation
    namespace: ""
    resourceVersion: "172"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-autoapprove-certificate-rotation
    uid: f4d344ff-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:nodes
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:node-proxier
    namespace: ""
    resourceVersion: "192"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier
    uid: f4eb59cb-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:node-proxier
  subjects:
  - kind: ServiceAccount
    name: kube-proxy
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":""},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"ClusterRole","name":"cluster-admin"},"subjects":[{"kind":"ServiceAccount","name":"kubernetes-dashboard","namespace":"kube-system"}]}
    creationTimestamp: 2018-02-09T19:46:49Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: ""
    resourceVersion: "276"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubernetes-dashboard
    uid: f7d39127-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress
    namespace: ""
    resourceVersion: "473403"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/my-nginx-ingress
    uid: a147c291-1108-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: my-nginx-ingress
  subjects:
  - kind: ServiceAccount
    name: my-nginx-ingress
    namespace: default
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:basic-user
    namespace: ""
    resourceVersion: "90"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Abasic-user
    uid: f389d838-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:basic-user
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:unauthenticated
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:attachdetach-controller
    namespace: ""
    resourceVersion: "96"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aattachdetach-controller
    uid: f38c670b-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:attachdetach-controller
  subjects:
  - kind: ServiceAccount
    name: attachdetach-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:certificate-controller
    namespace: ""
    resourceVersion: "120"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Acertificate-controller
    uid: f3ec53b6-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:certificate-controller
  subjects:
  - kind: ServiceAccount
    name: certificate-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:cronjob-controller
    namespace: ""
    resourceVersion: "97"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Acronjob-controller
    uid: f38cc0c2-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:cronjob-controller
  subjects:
  - kind: ServiceAccount
    name: cronjob-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:daemon-set-controller
    namespace: ""
    resourceVersion: "98"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Adaemon-set-controller
    uid: f38d1cda-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:daemon-set-controller
  subjects:
  - kind: ServiceAccount
    name: daemon-set-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:deployment-controller
    namespace: ""
    resourceVersion: "99"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Adeployment-controller
    uid: f38d8c3b-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:deployment-controller
  subjects:
  - kind: ServiceAccount
    name: deployment-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:disruption-controller
    namespace: ""
    resourceVersion: "100"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Adisruption-controller
    uid: f38de50c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:disruption-controller
  subjects:
  - kind: ServiceAccount
    name: disruption-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:endpoint-controller
    namespace: ""
    resourceVersion: "101"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aendpoint-controller
    uid: f38e3ee8-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:endpoint-controller
  subjects:
  - kind: ServiceAccount
    name: endpoint-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:generic-garbage-collector
    namespace: ""
    resourceVersion: "102"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Ageneric-garbage-collector
    uid: f390c79d-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:generic-garbage-collector
  subjects:
  - kind: ServiceAccount
    name: generic-garbage-collector
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:horizontal-pod-autoscaler
    namespace: ""
    resourceVersion: "104"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Ahorizontal-pod-autoscaler
    uid: f396db68-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:horizontal-pod-autoscaler
  subjects:
  - kind: ServiceAccount
    name: horizontal-pod-autoscaler
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:job-controller
    namespace: ""
    resourceVersion: "105"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Ajob-controller
    uid: f39cf6ef-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:job-controller
  subjects:
  - kind: ServiceAccount
    name: job-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:namespace-controller
    namespace: ""
    resourceVersion: "106"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Anamespace-controller
    uid: f3a31b6c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:namespace-controller
  subjects:
  - kind: ServiceAccount
    name: namespace-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:node-controller
    namespace: ""
    resourceVersion: "107"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Anode-controller
    uid: f3a93428-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:node-controller
  subjects:
  - kind: ServiceAccount
    name: node-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:persistent-volume-binder
    namespace: ""
    resourceVersion: "108"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Apersistent-volume-binder
    uid: f3af505f-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:persistent-volume-binder
  subjects:
  - kind: ServiceAccount
    name: persistent-volume-binder
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:pod-garbage-collector
    namespace: ""
    resourceVersion: "110"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Apod-garbage-collector
    uid: f3b56206-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:pod-garbage-collector
  subjects:
  - kind: ServiceAccount
    name: pod-garbage-collector
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:replicaset-controller
    namespace: ""
    resourceVersion: "111"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Areplicaset-controller
    uid: f3bb82b3-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:replicaset-controller
  subjects:
  - kind: ServiceAccount
    name: replicaset-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:replication-controller
    namespace: ""
    resourceVersion: "112"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Areplication-controller
    uid: f3c1a57c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:replication-controller
  subjects:
  - kind: ServiceAccount
    name: replication-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:resourcequota-controller
    namespace: ""
    resourceVersion: "113"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aresourcequota-controller
    uid: f3c7bf62-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:resourcequota-controller
  subjects:
  - kind: ServiceAccount
    name: resourcequota-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:route-controller
    namespace: ""
    resourceVersion: "114"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aroute-controller
    uid: f3cddade-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:route-controller
  subjects:
  - kind: ServiceAccount
    name: route-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:service-account-controller
    namespace: ""
    resourceVersion: "116"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aservice-account-controller
    uid: f3d3e9ed-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:service-account-controller
  subjects:
  - kind: ServiceAccount
    name: service-account-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:service-controller
    namespace: ""
    resourceVersion: "117"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Aservice-controller
    uid: f3da0af2-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:service-controller
  subjects:
  - kind: ServiceAccount
    name: service-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:statefulset-controller
    namespace: ""
    resourceVersion: "118"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Astatefulset-controller
    uid: f3e02238-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:statefulset-controller
  subjects:
  - kind: ServiceAccount
    name: statefulset-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:ttl-controller
    namespace: ""
    resourceVersion: "119"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Acontroller%3Attl-controller
    uid: f3e63eb9-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:controller:ttl-controller
  subjects:
  - kind: ServiceAccount
    name: ttl-controller
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:discovery
    namespace: ""
    resourceVersion: "89"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Adiscovery
    uid: f3897b4a-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:discovery
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:authenticated
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:unauthenticated
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-controller-manager
    namespace: ""
    resourceVersion: "92"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Akube-controller-manager
    uid: f38af434-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:kube-controller-manager
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:kube-controller-manager
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-dns
    namespace: ""
    resourceVersion: "93"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Akube-dns
    uid: f38b5161-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:kube-dns
  subjects:
  - kind: ServiceAccount
    name: kube-dns
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-scheduler
    namespace: ""
    resourceVersion: "94"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Akube-scheduler
    uid: f38bad8a-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:kube-scheduler
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:kube-scheduler
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node
    namespace: ""
    resourceVersion: "95"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Anode
    uid: f38c10ad-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:node
  subjects: null
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node-proxier
    namespace: ""
    resourceVersion: "91"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Anode-proxier
    uid: f38a979d-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:node-proxier
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:kube-proxy
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:49:23Z
    name: tiller-cluster-rule
    namespace: ""
    resourceVersion: "1019"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/tiller-cluster-rule
    uid: 5413543e-0dd2-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get clusterroles --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: admin
    namespace: ""
    resourceVersion: "50"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/admin
    uid: f37ac782-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/attach
    - pods/exec
    - pods/portforward
    - pods/proxy
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - endpoints
    - persistentvolumeclaims
    - replicationcontrollers
    - replicationcontrollers/scale
    - secrets
    - serviceaccounts
    - services
    - services/proxy
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - bindings
    - events
    - limitranges
    - namespaces/status
    - pods/log
    - pods/status
    - replicationcontrollers/status
    - resourcequotas
    - resourcequotas/status
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - serviceaccounts
    verbs:
    - impersonate
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - deployments
    - deployments/rollback
    - deployments/scale
    - replicasets
    - replicasets/scale
    - statefulsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - batch
    resources:
    - cronjobs
    - jobs
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    - deployments
    - deployments/rollback
    - deployments/scale
    - ingresses
    - replicasets
    - replicasets/scale
    - replicationcontrollers/scale
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - authorization.k8s.io
    resources:
    - localsubjectaccessreviews
    verbs:
    - create
  - apiGroups:
    - rbac.authorization.k8s.io
    resources:
    - rolebindings
    - roles
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"ClusterRole","metadata":{"annotations":{},"name":"calico-node","namespace":""},"rules":[{"apiGroups":[""],"resources":["namespaces"],"verbs":["get","list","watch"]},{"apiGroups":[""],"resources":["pods/status"],"verbs":["update"]},{"apiGroups":[""],"resources":["pods"],"verbs":["get","list","watch","patch"]},{"apiGroups":[""],"resources":["services"],"verbs":["get"]},{"apiGroups":[""],"resources":["endpoints"],"verbs":["get"]},{"apiGroups":[""],"resources":["nodes"],"verbs":["get","list","update","watch"]},{"apiGroups":["extensions"],"resources":["networkpolicies"],"verbs":["get","list","watch"]},{"apiGroups":["crd.projectcalico.org"],"resources":["globalfelixconfigs","felixconfigurations","bgppeers","globalbgpconfigs","bgpconfigurations","ippools","globalnetworkpolicies","networkpolicies","clusterinformations"],"verbs":["create","get","list","update","watch"]}]}
    creationTimestamp: 2018-02-09T19:46:44Z
    name: calico-node
    namespace: ""
    resourceVersion: "199"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/calico-node
    uid: f548ec07-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - get
    - list
    - watch
    - patch
  - apiGroups:
    - ""
    resources:
    - services
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - networkpolicies
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - crd.projectcalico.org
    resources:
    - globalfelixconfigs
    - felixconfigurations
    - bgppeers
    - globalbgpconfigs
    - bgpconfigurations
    - ippools
    - globalnetworkpolicies
    - networkpolicies
    - clusterinformations
    verbs:
    - create
    - get
    - list
    - update
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:37Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: cluster-admin
    namespace: ""
    resourceVersion: "12"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin
    uid: f116be87-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - '*'
  - nonResourceURLs:
    - '*'
    verbs:
    - '*'
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: edit
    namespace: ""
    resourceVersion: "51"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/edit
    uid: f37b329d-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/attach
    - pods/exec
    - pods/portforward
    - pods/proxy
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - endpoints
    - persistentvolumeclaims
    - replicationcontrollers
    - replicationcontrollers/scale
    - secrets
    - serviceaccounts
    - services
    - services/proxy
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - bindings
    - events
    - limitranges
    - namespaces/status
    - pods/log
    - pods/status
    - replicationcontrollers/status
    - resourcequotas
    - resourcequotas/status
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - serviceaccounts
    verbs:
    - impersonate
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - deployments
    - deployments/rollback
    - deployments/scale
    - replicasets
    - replicasets/scale
    - statefulsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - batch
    resources:
    - cronjobs
    - jobs
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    - deployments
    - deployments/rollback
    - deployments/scale
    - ingresses
    - replicasets
    - replicasets/scale
    - replicationcontrollers/scale
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true"},"name":"elasticsearch-logging","namespace":""},"rules":[{"apiGroups":[""],"resources":["services","namespaces","endpoints"],"verbs":["get"]}]}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
    name: elasticsearch-logging
    namespace: ""
    resourceVersion: "245"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/elasticsearch-logging
    uid: f65c37f6-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - services
    - namespaces
    - endpoints
    verbs:
    - get
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true"},"name":"fluentd-es","namespace":""},"rules":[{"apiGroups":[""],"resources":["namespaces","pods"],"verbs":["get","watch","list"]}]}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
    name: fluentd-es
    namespace: ""
    resourceVersion: "237"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/fluentd-es
    uid: f61db67a-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - namespaces
    - pods
    verbs:
    - get
    - watch
    - list
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRole","metadata":{"annotations":{},"name":"kube-keepalived-vip","namespace":""},"rules":[{"apiGroups":[""],"resources":["pods","nodes","endpoints","services","configmaps"],"verbs":["get","list","watch"]}]}
    creationTimestamp: 2018-02-13T15:33:32Z
    name: kube-keepalived-vip
    namespace: ""
    resourceVersion: "439303"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/kube-keepalived-vip
    uid: 3f555b96-10d3-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - nodes
    - endpoints
    - services
    - configmaps
    verbs:
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress
    namespace: ""
    resourceVersion: "473402"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/my-nginx-ingress
    uid: a1477e24-1108-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    - endpoints
    - nodes
    - pods
    - secrets
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - services
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - ingresses
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
  - apiGroups:
    - extensions
    resources:
    - ingresses/status
    verbs:
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:auth-delegator
    namespace: ""
    resourceVersion: "58"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Aauth-delegator
    uid: f37dbdc1-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - authentication.k8s.io
    resources:
    - tokenreviews
    verbs:
    - create
  - apiGroups:
    - authorization.k8s.io
    resources:
    - subjectaccessreviews
    verbs:
    - create
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:basic-user
    namespace: ""
    resourceVersion: "49"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Abasic-user
    uid: f37a49d3-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - authorization.k8s.io
    resources:
    - selfsubjectaccessreviews
    verbs:
    - create
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
    namespace: ""
    resourceVersion: "64"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acertificates.k8s.io%3Acertificatesigningrequests%3Anodeclient
    uid: f37fc939-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests/nodeclient
    verbs:
    - create
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
    namespace: ""
    resourceVersion: "65"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acertificates.k8s.io%3Acertificatesigningrequests%3Aselfnodeclient
    uid: f3801bcf-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests/selfnodeclient
    verbs:
    - create
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:attachdetach-controller
    namespace: ""
    resourceVersion: "66"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aattachdetach-controller
    uid: f380746b-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    - persistentvolumes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:certificate-controller
    namespace: ""
    resourceVersion: "87"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Acertificate-controller
    uid: f388bf46-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests/approval
    - certificatesigningrequests/status
    verbs:
    - update
  - apiGroups:
    - authorization.k8s.io
    resources:
    - subjectaccessreviews
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:cronjob-controller
    namespace: ""
    resourceVersion: "67"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Acronjob-controller
    uid: f380e8d7-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - batch
    resources:
    - cronjobs
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - batch
    resources:
    - jobs
    verbs:
    - create
    - delete
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - batch
    resources:
    - cronjobs/status
    verbs:
    - update
  - apiGroups:
    - batch
    resources:
    - cronjobs/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - delete
    - list
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:daemon-set-controller
    namespace: ""
    resourceVersion: "68"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Adaemon-set-controller
    uid: f3815060-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - apps
    - extensions
    resources:
    - daemonsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - daemonsets/status
    verbs:
    - update
  - apiGroups:
    - apps
    - extensions
    resources:
    - daemonsets/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - list
    - patch
    - watch
  - apiGroups:
    - ""
    resources:
    - pods/binding
    verbs:
    - create
  - apiGroups:
    - apps
    resources:
    - controllerrevisions
    verbs:
    - create
    - delete
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:deployment-controller
    namespace: ""
    resourceVersion: "69"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Adeployment-controller
    uid: f381d345-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - apps
    - extensions
    resources:
    - deployments
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - deployments/status
    verbs:
    - update
  - apiGroups:
    - apps
    - extensions
    resources:
    - deployments/finalizers
    verbs:
    - update
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - create
    - delete
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:disruption-controller
    namespace: ""
    resourceVersion: "70"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Adisruption-controller
    uid: f3823ffc-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - apps
    - extensions
    resources:
    - deployments
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - policy
    resources:
    - poddisruptionbudgets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - policy
    resources:
    - poddisruptionbudgets/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:endpoint-controller
    namespace: ""
    resourceVersion: "71"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aendpoint-controller
    uid: f382aaa1-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - create
    - delete
    - get
    - list
    - update
  - apiGroups:
    - ""
    resources:
    - endpoints/restricted
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:generic-garbage-collector
    namespace: ""
    resourceVersion: "72"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Ageneric-garbage-collector
    uid: f3830c3f-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - delete
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:horizontal-pod-autoscaler
    namespace: ""
    resourceVersion: "73"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Ahorizontal-pod-autoscaler
    uid: f3837697-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers/scale
    verbs:
    - get
    - update
  - apiGroups:
    - extensions
    resources:
    - replicationcontrollers/scale
    verbs:
    - get
    - update
  - apiGroups:
    - apps
    - extensions
    resources:
    - deployments/scale
    - replicasets/scale
    verbs:
    - get
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - list
  - apiGroups:
    - ""
    resourceNames:
    - 'http:heapster:'
    - 'https:heapster:'
    resources:
    - services/proxy
    verbs:
    - get
  - apiGroups:
    - metrics.k8s.io
    resources:
    - pods
    verbs:
    - list
  - apiGroups:
    - custom.metrics.k8s.io
    resources:
    - '*'
    verbs:
    - get
    - list
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:job-controller
    namespace: ""
    resourceVersion: "74"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Ajob-controller
    uid: f383e045-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - batch
    resources:
    - jobs
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - batch
    resources:
    - jobs/status
    verbs:
    - update
  - apiGroups:
    - batch
    resources:
    - jobs/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - list
    - patch
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:namespace-controller
    namespace: ""
    resourceVersion: "75"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Anamespace-controller
    uid: f384440d-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces/finalize
    - namespaces/status
    verbs:
    - update
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - delete
    - deletecollection
    - get
    - list
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:node-controller
    namespace: ""
    resourceVersion: "76"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Anode-controller
    uid: f384aa2d-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - delete
    - get
    - list
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - pods/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - delete
    - list
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:persistent-volume-binder
    namespace: ""
    resourceVersion: "77"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Apersistent-volume-binder
    uid: f3851437-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - create
    - delete
    - get
    - list
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumes/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    verbs:
    - create
    - delete
    - get
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
    - list
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:pod-garbage-collector
    namespace: ""
    resourceVersion: "78"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Apod-garbage-collector
    uid: f3856731-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - delete
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:replicaset-controller
    namespace: ""
    resourceVersion: "79"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Areplicaset-controller
    uid: f385e140-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets/status
    verbs:
    - update
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - list
    - patch
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:replication-controller
    namespace: ""
    resourceVersion: "80"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Areplication-controller
    uid: f386456a-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - list
    - patch
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:resourcequota-controller
    namespace: ""
    resourceVersion: "81"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aresourcequota-controller
    uid: f386989c-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - resourcequotas/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:route-controller
    namespace: ""
    resourceVersion: "82"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aroute-controller
    uid: f386f492-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:service-account-controller
    namespace: ""
    resourceVersion: "83"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aservice-account-controller
    uid: f38745bd-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - serviceaccounts
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:service-controller
    namespace: ""
    resourceVersion: "84"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Aservice-controller
    uid: f387a2e9-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - services/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:statefulset-controller
    namespace: ""
    resourceVersion: "85"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Astatefulset-controller
    uid: f387fd35-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - statefulsets/status
    verbs:
    - update
  - apiGroups:
    - apps
    resources:
    - statefulsets/finalizers
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - get
    - patch
    - update
  - apiGroups:
    - apps
    resources:
    - controllerrevisions
    verbs:
    - create
    - delete
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    verbs:
    - create
    - get
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:ttl-controller
    namespace: ""
    resourceVersion: "86"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Acontroller%3Attl-controller
    uid: f3886701-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:discovery
    namespace: ""
    resourceVersion: "48"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery
    uid: f379a186-0dd1-11e8-871b-005056af9e97
  rules:
  - nonResourceURLs:
    - /api
    - /api/*
    - /apis
    - /apis/*
    - /healthz
    - /swagger-2.0.0.pb-v1
    - /swagger.json
    - /swaggerapi
    - /swaggerapi/*
    - /version
    verbs:
    - get
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:heapster
    namespace: ""
    resourceVersion: "53"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Aheapster
    uid: f37bf146-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - events
    - namespaces
    - nodes
    - pods
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - deployments
    verbs:
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-aggregator
    namespace: ""
    resourceVersion: "59"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Akube-aggregator
    uid: f37e124e-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    verbs:
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-controller-manager
    namespace: ""
    resourceVersion: "60"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Akube-controller-manager
    uid: f37e6e58-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - endpoints
    - secrets
    - serviceaccounts
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - delete
  - apiGroups:
    - ""
    resources:
    - endpoints
    - namespaces
    - secrets
    - serviceaccounts
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - endpoints
    - secrets
    - serviceaccounts
    verbs:
    - update
  - apiGroups:
    - authentication.k8s.io
    resources:
    - tokenreviews
    verbs:
    - create
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-dns
    namespace: ""
    resourceVersion: "62"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Akube-dns
    uid: f37f1cbd-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    verbs:
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-scheduler
    namespace: ""
    resourceVersion: "61"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Akube-scheduler
    uid: f37ec692-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - create
  - apiGroups:
    - ""
    resourceNames:
    - kube-scheduler
    resources:
    - endpoints
    verbs:
    - delete
    - get
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - bindings
    - pods/binding
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - pods/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    - persistentvolumes
    verbs:
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node
    namespace: ""
    resourceVersion: "54"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode
    uid: f37c50e9-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - authentication.k8s.io
    resources:
    - tokenreviews
    verbs:
    - create
  - apiGroups:
    - authorization.k8s.io
    resources:
    - localsubjectaccessreviews
    - subjectaccessreviews
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - create
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - delete
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
  - apiGroups:
    - ""
    resources:
    - pods/status
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - pods/eviction
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    - persistentvolumes
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - get
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests
    verbs:
    - create
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node-bootstrapper
    namespace: ""
    resourceVersion: "57"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode-bootstrapper
    uid: f37d6ba3-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - certificates.k8s.io
    resources:
    - certificatesigningrequests
    verbs:
    - create
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node-problem-detector
    namespace: ""
    resourceVersion: "55"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode-problem-detector
    uid: f37ca9c0-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - nodes/status
    verbs:
    - patch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:node-proxier
    namespace: ""
    resourceVersion: "56"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode-proxier
    uid: f37d0c0f-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:persistent-volume-provisioner
    namespace: ""
    resourceVersion: "63"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Apersistent-volume-provisioner
    uid: f37f7b1a-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - create
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    verbs:
    - get
    - list
    - update
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:41Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: view
    namespace: ""
    resourceVersion: "52"
    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/view
    uid: f37b87a2-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    - endpoints
    - persistentvolumeclaims
    - pods
    - replicationcontrollers
    - replicationcontrollers/scale
    - serviceaccounts
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - bindings
    - events
    - limitranges
    - namespaces/status
    - pods/log
    - pods/status
    - replicationcontrollers/status
    - resourcequotas
    - resourcequotas/status
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - deployments
    - deployments/scale
    - replicasets
    - replicasets/scale
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - batch
    resources:
    - cronjobs
    - jobs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    - deployments
    - deployments/scale
    - ingresses
    - replicasets
    - replicasets/scale
    - replicationcontrollers/scale
    verbs:
    - get
    - list
    - watch
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get clusters --all-namespaces -o yaml
================

the server doesn't have a resource type "clusters"

================
kubectl get componentstatuses --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  conditions:
  - message: ok
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: controller-manager
    namespace: ""
    selfLink: /api/v1/componentstatuses/controller-manager
- apiVersion: v1
  conditions:
  - message: ok
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: scheduler
    namespace: ""
    selfLink: /api/v1/componentstatuses/scheduler
- apiVersion: v1
  conditions:
  - message: '{"health": "true"}'
    status: "True"
    type: Healthy
  kind: ComponentStatus
  metadata:
    creationTimestamp: null
    name: etcd-0
    namespace: ""
    selfLink: /api/v1/componentstatuses/etcd-0
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get configmaps --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"my-nginx-ingress-controller-d95d4979d-kzgk7","leaseDurationSeconds":30,"acquireTime":"2018-02-13T21:56:23Z","renewTime":"2018-02-13T22:47:18Z","leaderTransitions":22}'
    creationTimestamp: 2018-02-09T19:50:00Z
    name: ingress-controller-leader-nginx
    namespace: default
    resourceVersion: "477915"
    selfLink: /api/v1/namespaces/default/configmaps/ingress-controller-leader-nginx
    uid: 69aa1b57-0dd2-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    enable-vts-status: "false"
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473398"
    selfLink: /api/v1/namespaces/default/configmaps/my-nginx-ingress-controller
    uid: a144a7df-1108-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    10.10.97.200: default/my-nginx-ingress-controller
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-13T18:56:15Z
    name: vip-configmap
    namespace: default
    resourceVersion: "457836"
    selfLink: /api/v1/namespaces/default/configmaps/vip-configmap
    uid: 913a1cac-10ef-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    kubeconfig: |
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
          server: https://10.10.97.20:6443
        name: ""
      contexts: []
      current-context: ""
      kind: Config
      preferences: {}
      users: []
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: cluster-info
    namespace: kube-public
    resourceVersion: "118889"
    selfLink: /api/v1/namespaces/kube-public/configmaps/cluster-info
    uid: f4d40057-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    cni_network_config: |-
      {
        "name": "k8s-pod-network",
        "cniVersion": "0.3.0",
        "plugins": [
          {
            "type": "calico",
            "log_level": "info",
            "datastore_type": "kubernetes",
            "nodename": "__KUBERNETES_NODE_NAME__",
            "mtu": 1500,
            "ipam": {
              "type": "host-local",
              "subnet": "usePodCidr"
            },
            "policy": {
              "type": "k8s",
              "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
            },
            "kubernetes": {
              "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
              "kubeconfig": "__KUBECONFIG_FILEPATH__"
            }
          },
          {
            "type": "portmap",
            "snat": true,
            "capabilities": {"portMappings": true}
          }
        ]
      }
    typha_service_name: none
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"cni_network_config":"{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.3.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"log_level\": \"info\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n      \"mtu\": 1500,\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\",\n        \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n      },\n      \"kubernetes\": {\n        \"k8s_api_root\": \"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__\",\n        \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n      }\n    },\n    {\n      \"type\": \"portmap\",\n      \"snat\": true,\n      \"capabilities\": {\"portMappings\": true}\n    }\n  ]\n}","typha_service_name":"none"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"calico-config","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    name: calico-config
    namespace: kube-system
    resourceVersion: "204"
    selfLink: /api/v1/namespaces/kube-system/configmaps/calico-config
    uid: f580095e-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    client-ca-file: |
      -----BEGIN CERTIFICATE-----
      MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
      cm5ldGVzMB4XDTE4MDIwOTE5NDYxNFoXDTI4MDIwNzE5NDYxNFowFTETMBEGA1UE
      AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALqu
      arSh9xo+PRq4bT+a/D/2eFxNxetj7jcjcwB1o8MLzPCvpu94voQLXag44mJuuDEG
      TpEde2SJ+1PYv/XwPsqafpxNSvf92U9UVBAUXqhqBesfS+SKy+laauYu4Ein/U4I
      1C2Ndoav2uEkkasaOAX7mrSCo09pysSKR+ybEdn8q0wWLZlOg/Nx9YcaNn9Tvsiy
      p6zPA9oDdYQ7cruEApwao+NC8+4ANv/ZlUlhcBFtohuIi2EbDGIFGR1a1ca0PS9c
      X/m3z6LDtVZZY1W44Djk+1W34v9Hsz3f2e70Ct7lxtgjdkay89Reez2IU8p1XjrO
      jWWNkO54CBNeT++zGO0CAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
      /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKzzPmtWofZezXbhiVd0vDFBHaLb
      RIwBeI7Yi1C83ZeD/GZ5tBFGE6Xc9deaqqRIFYRf7Y9migo8veBD4tDn4MGFqPd9
      wRZMFXslzzRX7aiR8hPAxIkEzwAaFoYRms67aTnqvDKb/fnL/LCZQ76EB+AiFiRs
      415kaJss/11Yb1zRItZY0dHxBu+mOqv0uvZ1yC7nARp3QbHxVsz/xH0kFia12BUg
      eYydStwfEzplXG0H9zOp5PkDn70I714ie/RpCr2AEIelmBYyf1sm2+SxDtuV/skm
      4UHJGW6bTRes/jEBWSUnXvRW4ImH97sqR6MNYXoiWhchMzrtmp3G1tcfCRQ=
      -----END CERTIFICATE-----
    requestheader-allowed-names: '["front-proxy-client"]'
    requestheader-client-ca-file: |
      -----BEGIN CERTIFICATE-----
      MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
      cm5ldGVzMB4XDTE4MDIwOTE5NDYxNVoXDTI4MDIwNzE5NDYxNVowFTETMBEGA1UE
      AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKVr
      QPZjj+JvGnpt5fEBZ3vn3Mj/TKC5eYQnAKT6Up+KiqB6CLsubnmqQX1LGCOR0uZZ
      4jekqxDT2In7gk8+gb5/EchQ+za1tRIYZlkkqDIrE257FvMU+8AKU9f9ynfT9J1F
      FXOIwzMZW7seHDh6p98W7P3e/mlJjywSwJTNVb/iYB+voYzFIq/y+3BVG1TzKvlM
      Qc/bq9/D6UUS+YNgbDgwSNNTD3p/V6J0ATJAEYfHrzyP3QBOlbHA1cyLGDCsqgay
      A2dyasXNQRA39eyixBLgHEPJYNbmBH1kQWrfCFQk0K5phLpM6Ow0/XXkLROq8sYO
      G6yFErv9fs/mQ8Geq+cCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
      /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAIvY+VVHOKBo9KHV8iIug1jq31RZ
      qeYNJNZ5g80koGxJ0g46rXhFlJ8CXz7PAGDaKHFCjuzbeOmVQ5q5PDAg15BfizvR
      Rx/q/ugJJRitxZR0zIuCSNQ29EwokyBE7k5j54wYIX61aY/yHe4GEfv9DTZIztP6
      E6z2gW7RfZbcKV+komATaN8bQCM9W3T/FvYTei1tZ89gzbJbkVk/Y7AxyDQ9Ym2O
      8H/paYAiMy0npNqviC6eyMEcM9m+G1Sxye4cOkFLkkjlxWFSuXuUqSnUbqTGlhSt
      C1DtrSg1rnzGqnsp7bdRmgevNluy2h7STk0fO1zB42MtQq5JNI/1TyeQeGM=
      -----END CERTIFICATE-----
    requestheader-extra-headers-prefix: '["X-Remote-Extra-"]'
    requestheader-group-headers: '["X-Remote-Group"]'
    requestheader-username-headers: '["X-Remote-User"]'
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-09T19:46:41Z
    name: extension-apiserver-authentication
    namespace: kube-system
    resourceVersion: "47"
    selfLink: /api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication
    uid: f3796963-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    containers.input.conf: |-
      # This configuration file for Fluentd / td-agent is used
      # to watch changes to Docker log files. The kubelet creates symlinks that
      # capture the pod name, namespace, container name & Docker container ID
      # to the docker logs for pods in the /var/log/containers directory on the host.
      # If running this fluentd configuration in a Docker container, the /var/log
      # directory should be mounted in the container.
      #
      # These logs are then submitted to Elasticsearch which assumes the
      # installation of the fluent-plugin-elasticsearch & the
      # fluent-plugin-kubernetes_metadata_filter plugins.
      # See https://github.com/uken/fluent-plugin-elasticsearch &
      # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
      # more information about the plugins.
      #
      # Example
      # =======
      # A line in the Docker log file might look like this JSON:
      #
      # {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
      #  "stream":"stderr",
      #   "time":"2014-09-25T21:15:03.499185026Z"}
      #
      # The time_format specification below makes sure we properly
      # parse the time format produced by Docker. This will be
      # submitted to Elasticsearch and should appear like:
      # $ curl 'http://elasticsearch-logging:9200/_search?pretty'
      # ...
      # {
      #      "_index" : "logstash-2014.09.25",
      #      "_type" : "fluentd",
      #      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
      #      "_score" : 1.0,
      #      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
      #                 "stream":"stderr","tag":"docker.container.all",
      #                 "@timestamp":"2014-09-25T22:45:50+00:00"}
      #    },
      # ...
      #
      # The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
      # record & add labels to the log record if properly configured. This enables users
      # to filter & search logs on any metadata.
      # For example a Docker container's logs might be in the directory:
      #
      #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
      #
      # and in the file:
      #
      #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      #
      # where 997599971ee6... is the Docker ID of the running container.
      # The Kubernetes kubelet makes a symbolic link to this file on the host machine
      # in the /var/log/containers directory which includes the pod name and the Kubernetes
      # container name:
      #
      #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #    ->
      #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      #
      # The /var/log directory on the host is mapped to the /var/log directory in the container
      # running this instance of Fluentd and we end up collecting the file:
      #
      #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #
      # This results in the tag:
      #
      #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #
      # The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
      # which are added to the log message as a kubernetes field object & the Docker container ID
      # is also added under the docker field object.
      # The final tag is:
      #
      #   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #
      # And the final log record look like:
      #
      # {
      #   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
      #   "stream":"stderr",
      #   "time":"2014-09-25T21:15:03.499185026Z",
      #   "kubernetes": {
      #     "namespace": "default",
      #     "pod_name": "synthetic-logger-0.25lps-pod",
      #     "container_name": "synth-lgr"
      #   },
      #   "docker": {
      #     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
      #   }
      # }
      #
      # This makes it easier for users to search for logs by pod name or by
      # the name of the Kubernetes container regardless of how many times the
      # Kubernetes pod has been restarted (resulting in a several Docker container IDs).

      # Json Log Example:
      # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
      # CRI Log Example:
      # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
      <source>
        type tail
        path /var/log/containers/*.log
        pos_file /var/log/es-containers.log.pos
        time_format %Y-%m-%dT%H:%M:%S.%NZ
        tag kubernetes.*
        read_from_head true
        format multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </source>
    forward.input.conf: |-
      # Takes the messages sent over TCP
      <source>
        type forward
      </source>
    monitoring.conf: |-
      # Prometheus Exporter Plugin
      # input plugin that exports metrics
      <source>
        @type prometheus
      </source>

      <source>
        @type monitor_agent
      </source>

      # input plugin that collects metrics from MonitorAgent
      <source>
        @type prometheus_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # input plugin that collects metrics for output plugin
      <source>
        @type prometheus_output_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # input plugin that collects metrics for in_tail plugin
      <source>
        @type prometheus_tail_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>
    output.conf: |-
      # Enriches records with Kubernetes metadata
      <filter kubernetes.**>
        type kubernetes_metadata
      </filter>

      <match **>
         type elasticsearch
         log_level info
         include_tag_key true
         host elasticsearch-logging
         port 9200
         logstash_format true
         # Set the chunk limits.
         buffer_chunk_limit 2M
         buffer_queue_limit 8
         flush_interval 5s
         # Never wait longer than 5 minutes between retries.
         max_retry_wait 30
         # Disable the limit on the number of retries (retry forever).
         disable_retry_limit
         # Use multiple threads for processing.
         num_threads 2
      </match>
    system.input.conf: |-
      # Example:
      # 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
      <source>
        type tail
        format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
        path /var/log/salt/minion
        pos_file /var/log/es-salt.pos
        tag salt
      </source>

      # Example:
      # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
      <source>
        type tail
        format syslog
        path /var/log/startupscript.log
        pos_file /var/log/es-startupscript.log.pos
        tag startupscript
      </source>

      # Examples:
      # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
      # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
      <source>
        type tail
        format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
        path /var/log/docker.log
        pos_file /var/log/es-docker.log.pos
        tag docker
      </source>

      # Example:
      # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
      <source>
        type tail
        # Not parsing this, because it doesn't have anything particularly useful to
        # parse out of it (like severities).
        format none
        path /var/log/etcd.log
        pos_file /var/log/es-etcd.log.pos
        tag etcd
      </source>

      # Multi-line parsing is required for all the kube logs because very large log
      # statements, such as those that include entire object bodies, get split into
      # multiple lines by glog.

      # Example:
      # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kubelet.log
        pos_file /var/log/es-kubelet.log.pos
        tag kubelet
      </source>

      # Example:
      # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-proxy.log
        pos_file /var/log/es-kube-proxy.log.pos
        tag kube-proxy
      </source>

      # Example:
      # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-apiserver.log
        pos_file /var/log/es-kube-apiserver.log.pos
        tag kube-apiserver
      </source>

      # Example:
      # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-controller-manager.log
        pos_file /var/log/es-kube-controller-manager.log.pos
        tag kube-controller-manager
      </source>

      # Example:
      # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-scheduler.log
        pos_file /var/log/es-kube-scheduler.log.pos
        tag kube-scheduler
      </source>

      # Example:
      # I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/rescheduler.log
        pos_file /var/log/es-rescheduler.log.pos
        tag rescheduler
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/glbc.log
        pos_file /var/log/es-glbc.log.pos
        tag glbc
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/cluster-autoscaler.log
        pos_file /var/log/es-cluster-autoscaler.log.pos
        tag cluster-autoscaler
      </source>

      # Logs from systemd-journal for interesting services.
      <source>
        type systemd
        filters [{ "_SYSTEMD_UNIT": "docker.service" }]
        pos_file /var/log/gcp-journald-docker.pos
        read_from_head true
        tag docker
      </source>

      <source>
        type systemd
        filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        pos_file /var/log/gcp-journald-kubelet.pos
        read_from_head true
        tag kubelet
      </source>

      <source>
        type systemd
        filters [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
        pos_file /var/log/gcp-journald-node-problem-detector.pos
        read_from_head true
        tag node-problem-detector
      </source>
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"containers.input.conf":"# This configuration file for Fluentd / td-agent is used\n# to watch changes to Docker log files. The kubelet creates symlinks that\n# capture the pod name, namespace, container name \u0026 Docker container ID\n# to the docker logs for pods in the /var/log/containers directory on the host.\n# If running this fluentd configuration in a Docker container, the /var/log\n# directory should be mounted in the container.\n#\n# These logs are then submitted to Elasticsearch which assumes the\n# installation of the fluent-plugin-elasticsearch \u0026 the\n# fluent-plugin-kubernetes_metadata_filter plugins.\n# See https://github.com/uken/fluent-plugin-elasticsearch \u0026\n# https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for\n# more information about the plugins.\n#\n# Example\n# =======\n# A line in the Docker log file might look like this JSON:\n#\n# {\"log\":\"2014/09/25 21:15:03 Got request with path wombat\\n\",\n#  \"stream\":\"stderr\",\n#   \"time\":\"2014-09-25T21:15:03.499185026Z\"}\n#\n# The time_format specification below makes sure we properly\n# parse the time format produced by Docker. This will be\n# submitted to Elasticsearch and should appear like:\n# $ curl 'http://elasticsearch-logging:9200/_search?pretty'\n# ...\n# {\n#      \"_index\" : \"logstash-2014.09.25\",\n#      \"_type\" : \"fluentd\",\n#      \"_id\" : \"VBrbor2QTuGpsQyTCdfzqA\",\n#      \"_score\" : 1.0,\n#      \"_source\":{\"log\":\"2014/09/25 22:45:50 Got request with path wombat\\n\",\n#                 \"stream\":\"stderr\",\"tag\":\"docker.container.all\",\n#                 \"@timestamp\":\"2014-09-25T22:45:50+00:00\"}\n#    },\n# ...\n#\n# The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log\n# record \u0026 add labels to the log record if properly configured. This enables users\n# to filter \u0026 search logs on any metadata.\n# For example a Docker container's logs might be in the directory:\n#\n#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\n#\n# and in the file:\n#\n#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n#\n# where 997599971ee6... is the Docker ID of the running container.\n# The Kubernetes kubelet makes a symbolic link to this file on the host machine\n# in the /var/log/containers directory which includes the pod name and the Kubernetes\n# container name:\n#\n#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#    -\u003e\n#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n#\n# The /var/log directory on the host is mapped to the /var/log directory in the container\n# running this instance of Fluentd and we end up collecting the file:\n#\n#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#\n# This results in the tag:\n#\n#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#\n# The Kubernetes fluentd plugin is used to extract the namespace, pod name \u0026 container name\n# which are added to the log message as a kubernetes field object \u0026 the Docker container ID\n# is also added under the docker field object.\n# The final tag is:\n#\n#   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#\n# And the final log record look like:\n#\n# {\n#   \"log\":\"2014/09/25 21:15:03 Got request with path wombat\\n\",\n#   \"stream\":\"stderr\",\n#   \"time\":\"2014-09-25T21:15:03.499185026Z\",\n#   \"kubernetes\": {\n#     \"namespace\": \"default\",\n#     \"pod_name\": \"synthetic-logger-0.25lps-pod\",\n#     \"container_name\": \"synth-lgr\"\n#   },\n#   \"docker\": {\n#     \"container_id\": \"997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\"\n#   }\n# }\n#\n# This makes it easier for users to search for logs by pod name or by\n# the name of the Kubernetes container regardless of how many times the\n# Kubernetes pod has been restarted (resulting in a several Docker container IDs).\n\n# Json Log Example:\n# {\"log\":\"[info:2016-02-16T16:04:05.930-08:00] Some log text here\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"}\n# CRI Log Example:\n# 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here\n\u003csource\u003e\n  type tail\n  path /var/log/containers/*.log\n  pos_file /var/log/es-containers.log.pos\n  time_format %Y-%m-%dT%H:%M:%S.%NZ\n  tag kubernetes.*\n  read_from_head true\n  format multi_format\n  \u003cpattern\u003e\n    format json\n    time_key time\n    time_format %Y-%m-%dT%H:%M:%S.%NZ\n  \u003c/pattern\u003e\n  \u003cpattern\u003e\n    format /^(?\u003ctime\u003e.+) (?\u003cstream\u003estdout|stderr) [^ ]* (?\u003clog\u003e.*)$/\n    time_format %Y-%m-%dT%H:%M:%S.%N%:z\n  \u003c/pattern\u003e\n\u003c/source\u003e","forward.input.conf":"# Takes the messages sent over TCP\n\u003csource\u003e\n  type forward\n\u003c/source\u003e","monitoring.conf":"# Prometheus Exporter Plugin\n# input plugin that exports metrics\n\u003csource\u003e\n  @type prometheus\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type monitor_agent\n\u003c/source\u003e\n\n# input plugin that collects metrics from MonitorAgent\n\u003csource\u003e\n  @type prometheus_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e\n\n# input plugin that collects metrics for output plugin\n\u003csource\u003e\n  @type prometheus_output_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e\n\n# input plugin that collects metrics for in_tail plugin\n\u003csource\u003e\n  @type prometheus_tail_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e","output.conf":"# Enriches records with Kubernetes metadata\n\u003cfilter kubernetes.**\u003e\n  type kubernetes_metadata\n\u003c/filter\u003e\n\n\u003cmatch **\u003e\n   type elasticsearch\n   log_level info\n   include_tag_key true\n   host elasticsearch-logging\n   port 9200\n   logstash_format true\n   # Set the chunk limits.\n   buffer_chunk_limit 2M\n   buffer_queue_limit 8\n   flush_interval 5s\n   # Never wait longer than 5 minutes between retries.\n   max_retry_wait 30\n   # Disable the limit on the number of retries (retry forever).\n   disable_retry_limit\n   # Use multiple threads for processing.\n   num_threads 2\n\u003c/match\u003e","system.input.conf":"# Example:\n# 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081\n\u003csource\u003e\n  type tail\n  format /^(?\u003ctime\u003e[^ ]* [^ ,]*)[^\\[]*\\[[^\\]]*\\]\\[(?\u003cseverity\u003e[^ \\]]*) *\\] (?\u003cmessage\u003e.*)$/\n  time_format %Y-%m-%d %H:%M:%S\n  path /var/log/salt/minion\n  pos_file /var/log/es-salt.pos\n  tag salt\n\u003c/source\u003e\n\n# Example:\n# Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script\n\u003csource\u003e\n  type tail\n  format syslog\n  path /var/log/startupscript.log\n  pos_file /var/log/es-startupscript.log.pos\n  tag startupscript\n\u003c/source\u003e\n\n# Examples:\n# time=\"2016-02-04T06:51:03.053580605Z\" level=info msg=\"GET /containers/json\"\n# time=\"2016-02-04T07:53:57.505612354Z\" level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\n\u003csource\u003e\n  type tail\n  format /^time=\"(?\u003ctime\u003e[^)]*)\" level=(?\u003cseverity\u003e[^ ]*) msg=\"(?\u003cmessage\u003e[^\"]*)\"( err=\"(?\u003cerror\u003e[^\"]*)\")?( statusCode=($\u003cstatus_code\u003e\\d+))?/\n  path /var/log/docker.log\n  pos_file /var/log/es-docker.log.pos\n  tag docker\n\u003c/source\u003e\n\n# Example:\n# 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\n\u003csource\u003e\n  type tail\n  # Not parsing this, because it doesn't have anything particularly useful to\n  # parse out of it (like severities).\n  format none\n  path /var/log/etcd.log\n  pos_file /var/log/es-etcd.log.pos\n  tag etcd\n\u003c/source\u003e\n\n# Multi-line parsing is required for all the kube logs because very large log\n# statements, such as those that include entire object bodies, get split into\n# multiple lines by glog.\n\n# Example:\n# I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kubelet.log\n  pos_file /var/log/es-kubelet.log.pos\n  tag kubelet\n\u003c/source\u003e\n\n# Example:\n# I1118 21:26:53.975789       6 proxier.go:1096] Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp) was open before and is still needed\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-proxy.log\n  pos_file /var/log/es-kube-proxy.log.pos\n  tag kube-proxy\n\u003c/source\u003e\n\n# Example:\n# I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-apiserver.log\n  pos_file /var/log/es-kube-apiserver.log.pos\n  tag kube-apiserver\n\u003c/source\u003e\n\n# Example:\n# I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-controller-manager.log\n  pos_file /var/log/es-kube-controller-manager.log.pos\n  tag kube-controller-manager\n\u003c/source\u003e\n\n# Example:\n# W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-scheduler.log\n  pos_file /var/log/es-kube-scheduler.log.pos\n  tag kube-scheduler\n\u003c/source\u003e\n\n# Example:\n# I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/rescheduler.log\n  pos_file /var/log/es-rescheduler.log.pos\n  tag rescheduler\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/glbc.log\n  pos_file /var/log/es-glbc.log.pos\n  tag glbc\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/cluster-autoscaler.log\n  pos_file /var/log/es-cluster-autoscaler.log.pos\n  tag cluster-autoscaler\n\u003c/source\u003e\n\n# Logs from systemd-journal for interesting services.\n\u003csource\u003e\n  type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }]\n  pos_file /var/log/gcp-journald-docker.pos\n  read_from_head true\n  tag docker\n\u003c/source\u003e\n\n\u003csource\u003e\n  type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\n  pos_file /var/log/gcp-journald-kubelet.pos\n  read_from_head true\n  tag kubelet\n\u003c/source\u003e\n\n\u003csource\u003e\n  type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"node-problem-detector.service\" }]\n  pos_file /var/log/gcp-journald-node-problem-detector.pos\n  read_from_head true\n  tag node-problem-detector\n\u003c/source\u003e"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile"},"name":"fluentd-es-config-v0.1.1","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
    name: fluentd-es-config-v0.1.1
    namespace: kube-system
    resourceVersion: "231"
    selfLink: /api/v1/namespaces/kube-system/configmaps/fluentd-es-config-v0.1.1
    uid: f5e09bc0-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    kubeconfig.conf: |
      apiVersion: v1
      kind: Config
      clusters:
      - cluster:
          certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          server: https://10.10.97.20:6443
        name: default
      contexts:
      - context:
          cluster: default
          namespace: default
          user: default
        name: default
      current-context: default
      users:
      - name: default
        user:
          tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    labels:
      app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "186"
    selfLink: /api/v1/namespaces/kube-system/configmaps/kube-proxy
    uid: f4e92f79-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    MasterConfiguration: |
      api:
        advertiseAddress: 10.10.97.20
        bindPort: 6443
      authorizationModes:
      - Node
      - RBAC
      certificatesDir: /etc/kubernetes/pki
      cloudProvider: ""
      etcd:
        caFile: ""
        certFile: ""
        dataDir: /var/lib/etcd
        endpoints: null
        image: ""
        keyFile: ""
      imageRepository: gcr.io/google_containers
      kubernetesVersion: v1.8.4
      networking:
        dnsDomain: cluster.local
        podSubnet: 192.168.0.0/16
        serviceSubnet: 10.96.0.0/12
      nodeName: vhosakot1-m51b5b468be
      token: ""
      tokenTTL: 24h0m0s
      unifiedControlPlaneImage: ""
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: kubeadm-config
    namespace: kube-system
    resourceVersion: "144"
    selfLink: /api/v1/namespaces/kube-system/configmaps/kubeadm-config
    uid: f482e49d-0dd1-11e8-871b-005056af9e97
- apiVersion: v1
  data:
    release: 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
  kind: ConfigMap
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      MODIFIED_AT: "1518558940"
      NAME: my-nginx-ingress
      OWNER: TILLER
      STATUS: DEPLOYED
      VERSION: "1"
    name: my-nginx-ingress.v1
    namespace: kube-system
    resourceVersion: "473417"
    selfLink: /api/v1/namespaces/kube-system/configmaps/my-nginx-ingress.v1
    uid: a11d31d0-1108-11e8-871b-005056af9e97
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get controllerrevisions --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          creationTimestamp: null
          labels:
            name: ccphxvolume
        spec:
          containers:
          - command:
            - /bin/sh
            - -c
            - while true; do sleep 2; done
            image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
            imagePullPolicy: IfNotPresent
            name: hxvolume
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - sh
            - -c
            - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
            image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
            imagePullPolicy: IfNotPresent
            name: iscsi-initiator
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          - command:
            - sh
            - -c
            - cp /hxcache/hxvolume /hxhostmount/
            image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
            imagePullPolicy: IfNotPresent
            name: hxvolume-copy
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /hxhostmount
              name: hxvolume-mount
            - mountPath: /etc/iscsi
              name: iscsi-volume
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - hostPath:
              path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
              type: ""
            name: hxvolume-mount
          - hostPath:
              path: /etc/iscsi
              type: ""
            name: iscsi-volume
  kind: ControllerRevision
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"default"},"spec":{"template":{"metadata":{"labels":{"name":"ccphxvolume"}},"spec":{"containers":[{"command":["/bin/sh","-c","while true; do sleep 2; done"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","imagePullPolicy":"IfNotPresent","name":"hxvolume","securityContext":{"privileged":true}}],"initContainers":[{"command":["sh","-c","SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name \u003e /etc/iscsi/initiatorname.iscsi"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"iscsi-initiator"},{"command":["sh","-c","cp /hxcache/hxvolume /hxhostmount/"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"hxvolume-copy","volumeMounts":[{"mountPath":"/hxhostmount","name":"hxvolume-mount"},{"mountPath":"/etc/iscsi","name":"iscsi-volume"}]}],"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"hostPath":{"path":"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/"},"name":"hxvolume-mount"},{"hostPath":{"path":"/etc/iscsi"},"name":"iscsi-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:58Z
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
    name: ccphxvolume-8b677dc65
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "375"
    selfLink: /apis/apps/v1beta1/namespaces/default/controllerrevisions/ccphxvolume-8b677dc65
    uid: fd184e83-0dd1-11e8-871b-005056af9e97
  revision: 1
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          creationTimestamp: null
          labels:
            name: kube-keepalived-vip
        spec:
          containers:
          - args:
            - --services-configmap=default/vip-configmap
            env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            image: k8s.gcr.io/kube-keepalived-vip:0.11
            imagePullPolicy: Always
            name: kube-keepalived-vip
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /lib/modules
              name: modules
              readOnly: true
            - mountPath: /dev
              name: dev
          dnsPolicy: ClusterFirst
          hostNetwork: true
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kube-keepalived-vip
          serviceAccountName: kube-keepalived-vip
          terminationGracePeriodSeconds: 30
          volumes:
          - hostPath:
              path: /lib/modules
              type: ""
            name: modules
          - hostPath:
              path: /dev
              type: ""
            name: dev
  kind: ControllerRevision
  metadata:
    creationTimestamp: 2018-02-13T19:01:11Z
    labels:
      controller-revision-hash: "617734581"
      name: kube-keepalived-vip
    name: kube-keepalived-vip-b5cc789d5
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-keepalived-vip
      uid: 41d5383d-10f0-11e8-871b-005056af9e97
    resourceVersion: "458291"
    selfLink: /apis/apps/v1beta1/namespaces/default/controllerrevisions/kube-keepalived-vip-b5cc789d5
    uid: 41d629a4-10f0-11e8-871b-005056af9e97
  revision: 1
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-node
        spec:
          containers:
          - env:
            - name: DATASTORE_TYPE
              value: kubernetes
            - name: FELIX_LOGSEVERITYSCREEN
              value: info
            - name: CLUSTER_TYPE
              value: k8s,bgp
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: ACCEPT
            - name: FELIX_IPV6SUPPORT
              value: "false"
            - name: FELIX_IPINIPMTU
              value: "1440"
            - name: WAIT_FOR_DATASTORE
              value: "true"
            - name: CALICO_IPV4POOL_CIDR
              value: 192.168.0.0/16
            - name: CALICO_IPV4POOL_IPIP
              value: Always
            - name: FELIX_IPINIPENABLED
              value: "true"
            - name: FELIX_TYPHAK8SSERVICENAME
              valueFrom:
                configMapKeyRef:
                  key: typha_service_name
                  name: calico-config
            - name: NODENAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: IP
            - name: FELIX_HEALTHENABLED
              value: "true"
            image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 6
              httpGet:
                path: /liveness
                port: 9099
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            name: calico-node
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /readiness
                port: 9099
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              requests:
                cpu: 250m
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /var/run/calico
              name: var-run-calico
          - command:
            - /install-cni.sh
            env:
            - name: CNI_CONF_NAME
              value: 10-calico.conflist
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  key: cni_network_config
                  name: calico-config
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
            imagePullPolicy: IfNotPresent
            name: install-cni
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
          dnsPolicy: ClusterFirst
          hostNetwork: true
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-node
          serviceAccountName: calico-node
          terminationGracePeriodSeconds: 0
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - key: CriticalAddonsOnly
            operator: Exists
          volumes:
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
          - hostPath:
              path: /var/run/calico
              type: ""
            name: var-run-calico
          - hostPath:
              path: /opt/cni/bin
              type: ""
            name: cni-bin-dir
          - hostPath:
              path: /etc/cni/net.d
              type: ""
            name: cni-net-dir
  kind: ControllerRevision
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"FELIX_LOGSEVERITYSCREEN","value":"info"},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_IPINIPMTU","value":"1440"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"CALICO_IPV4POOL_CIDR","value":"192.168.0.0/16"},{"name":"CALICO_IPV4POOL_IPIP","value":"Always"},{"name":"FELIX_IPINIPENABLED","value":"true"},{"name":"FELIX_TYPHAK8SSERVICENAME","valueFrom":{"configMapKeyRef":{"key":"typha_service_name","name":"calico-config"}}},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"IP","value":""},{"name":"FELIX_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master","livenessProbe":{"failureThreshold":6,"httpGet":{"path":"/liveness","port":9099},"initialDelaySeconds":10,"periodSeconds":10},"name":"calico-node","readinessProbe":{"httpGet":{"path":"/readiness","port":9099},"periodSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false}]},{"command":["/install-cni.sh"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master","name":"install-cni","volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"hostNetwork":true,"serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
    name: calico-node-76cc6dcd86
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "329"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/calico-node-76cc6dcd86
    uid: fcf5befc-0dd1-11e8-871b-005056af9e97
  revision: 1
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: elasticsearch-logging
            kubernetes.io/cluster-service: "true"
            version: v5.6.4
        spec:
          containers:
          - env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
            imagePullPolicy: IfNotPresent
            name: elasticsearch-logging
            ports:
            - containerPort: 9200
              name: db
              protocol: TCP
            - containerPort: 9300
              name: transport
              protocol: TCP
            resources:
              limits:
                cpu: "1"
              requests:
                cpu: 100m
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /data
              name: elasticsearch-logging
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - /sbin/sysctl
            - -w
            - vm.max_map_count=262144
            image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
            imagePullPolicy: IfNotPresent
            name: elasticsearch-logging-init
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: elasticsearch-logging
          serviceAccountName: elasticsearch-logging
          terminationGracePeriodSeconds: 30
          volumes:
          - emptyDir: {}
            name: elasticsearch-logging
  kind: ControllerRevision
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"},"name":"elasticsearch-logging","namespace":"kube-system"},"spec":{"replicas":2,"selector":{"matchLabels":{"k8s-app":"elasticsearch-logging","version":"v5.6.4"}},"serviceName":"elasticsearch-logging","template":{"metadata":{"labels":{"k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"}},"spec":{"containers":[{"env":[{"name":"NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4","name":"elasticsearch-logging","ports":[{"containerPort":9200,"name":"db","protocol":"TCP"},{"containerPort":9300,"name":"transport","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}},"volumeMounts":[{"mountPath":"/data","name":"elasticsearch-logging"}]}],"initContainers":[{"command":["/sbin/sysctl","-w","vm.max_map_count=262144"],"image":"registry.ci.dfj.io/cpsg_ccp/alpine:3.6","name":"elasticsearch-logging-init","securityContext":{"privileged":true}}],"serviceAccountName":"elasticsearch-logging","volumes":[{"emptyDir":{},"name":"elasticsearch-logging"}]}}}}
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      controller.kubernetes.io/hash: "3534725207"
      k8s-app: elasticsearch-logging
      version: v5.6.4
    name: elasticsearch-logging-7978c6964c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch-logging
      uid: f66f6df9-0dd1-11e8-871b-005056af9e97
    resourceVersion: "342"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/elasticsearch-logging-7978c6964c
    uid: fcf7ed0a-0dd1-11e8-871b-005056af9e97
  revision: 1
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: fluentd-es
            kubernetes.io/cluster-service: "true"
            version: v2.0.2
        spec:
          containers:
          - env:
            - name: FLUENTD_ARGS
              value: --no-supervisor -q
            image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
            imagePullPolicy: IfNotPresent
            name: fluentd-es
            resources:
              limits:
                memory: 500Mi
              requests:
                cpu: 100m
                memory: 200Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /var/log
              name: varlog
            - mountPath: /var/lib/docker/containers
              name: varlibdockercontainers
              readOnly: true
            - mountPath: /host/lib
              name: libsystemddir
              readOnly: true
            - mountPath: /etc/fluent/config.d
              name: config-volume
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: fluentd-es
          serviceAccountName: fluentd-es
          terminationGracePeriodSeconds: 30
          volumes:
          - hostPath:
              path: /var/log
              type: ""
            name: varlog
          - hostPath:
              path: /var/lib/docker/containers
              type: ""
            name: varlibdockercontainers
          - hostPath:
              path: /usr/lib64
              type: ""
            name: libsystemddir
          - configMap:
              defaultMode: 420
              name: fluentd-es-config-v0.1.1
            name: config-volume
  kind: ControllerRevision
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"},"name":"fluentd-es-v2.0.2","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-es","version":"v2.0.2"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"}},"spec":{"containers":[{"env":[{"name":"FLUENTD_ARGS","value":"--no-supervisor -q"}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2","name":"fluentd-es","resources":{"limits":{"memory":"500Mi"},"requests":{"cpu":"100m","memory":"200Mi"}},"volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/host/lib","name":"libsystemddir","readOnly":true},{"mountPath":"/etc/fluent/config.d","name":"config-volume"}]}],"serviceAccountName":"fluentd-es","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"hostPath":{"path":"/usr/lib64"},"name":"libsystemddir"},{"configMap":{"name":"fluentd-es-config-v0.1.1"},"name":"config-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:58Z
    labels:
      controller-revision-hash: "1193446001"
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      version: v2.0.2
    name: fluentd-es-v2.0.2-55f788b445
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-es-v2.0.2
      uid: f626704d-0dd1-11e8-871b-005056af9e97
    resourceVersion: "376"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/fluentd-es-v2.0.2-55f788b445
    uid: fd1718b9-0dd1-11e8-871b-005056af9e97
  revision: 1
- apiVersion: apps/v1beta1
  data:
    spec:
      template:
        $patch: replace
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-proxy
        spec:
          containers:
          - command:
            - /usr/local/bin/kube-proxy
            - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
            - --cluster-cidr=192.168.0.0/16
            image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
            imagePullPolicy: IfNotPresent
            name: kube-proxy
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /var/lib/kube-proxy
              name: kube-proxy
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
          dnsPolicy: ClusterFirst
          hostNetwork: true
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kube-proxy
          serviceAccountName: kube-proxy
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
          volumes:
          - configMap:
              defaultMode: 420
              name: kube-proxy
            name: kube-proxy
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
  kind: ControllerRevision
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
    name: kube-proxy-95856ccc5
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "330"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/controllerrevisions/kube-proxy-95856ccc5
    uid: fcf56484-0dd1-11e8-871b-005056af9e97
  revision: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get cronjobs --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get customresourcedefinition --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico BGP Configuration","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"bgpconfigurations.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"BGPConfiguration","plural":"bgpconfigurations","singular":"bgpconfiguration"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: bgpconfigurations.crd.projectcalico.org
    namespace: ""
    resourceVersion: "216"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/bgpconfigurations.crd.projectcalico.org
    uid: f59ee823-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: BGPConfiguration
      listKind: BGPConfigurationList
      plural: bgpconfigurations
      singular: bgpconfiguration
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: BGPConfiguration
      listKind: BGPConfigurationList
      plural: bgpconfigurations
      singular: bgpconfiguration
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico BGP Peers","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"bgppeers.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"BGPPeer","plural":"bgppeers","singular":"bgppeer"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: bgppeers.crd.projectcalico.org
    namespace: ""
    resourceVersion: "213"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/bgppeers.crd.projectcalico.org
    uid: f59b343d-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: BGPPeer
      listKind: BGPPeerList
      plural: bgppeers
      singular: bgppeer
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: BGPPeer
      listKind: BGPPeerList
      plural: bgppeers
      singular: bgppeer
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Cluster Information","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"clusterinformations.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"ClusterInformation","plural":"clusterinformations","singular":"clusterinformation"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: clusterinformations.crd.projectcalico.org
    namespace: ""
    resourceVersion: "220"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/clusterinformations.crd.projectcalico.org
    uid: f5a6010a-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: ClusterInformation
      listKind: ClusterInformationList
      plural: clusterinformations
      singular: clusterinformation
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: ClusterInformation
      listKind: ClusterInformationList
      plural: clusterinformations
      singular: clusterinformation
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Felix Configuration","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"felixconfigurations.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"FelixConfiguration","plural":"felixconfigurations","singular":"felixconfiguration"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: felixconfigurations.crd.projectcalico.org
    namespace: ""
    resourceVersion: "210"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/felixconfigurations.crd.projectcalico.org
    uid: f595a15e-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: FelixConfiguration
      listKind: FelixConfigurationList
      plural: felixconfigurations
      singular: felixconfiguration
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: FelixConfiguration
      listKind: FelixConfigurationList
      plural: felixconfigurations
      singular: felixconfiguration
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Global Network Policies","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"globalnetworkpolicies.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"GlobalNetworkPolicy","plural":"globalnetworkpolicies","singular":"globalnetworkpolicy"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: globalnetworkpolicies.crd.projectcalico.org
    namespace: ""
    resourceVersion: "222"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/globalnetworkpolicies.crd.projectcalico.org
    uid: f5a99650-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: GlobalNetworkPolicy
      listKind: GlobalNetworkPolicyList
      plural: globalnetworkpolicies
      singular: globalnetworkpolicy
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: GlobalNetworkPolicy
      listKind: GlobalNetworkPolicyList
      plural: globalnetworkpolicies
      singular: globalnetworkpolicy
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico IP Pools","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"ippools.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"IPPool","plural":"ippools","singular":"ippool"},"scope":"Cluster","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: ippools.crd.projectcalico.org
    namespace: ""
    resourceVersion: "218"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/ippools.crd.projectcalico.org
    uid: f5a2bed4-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: IPPool
      listKind: IPPoolList
      plural: ippools
      singular: ippool
    scope: Cluster
    version: v1
  status:
    acceptedNames:
      kind: IPPool
      listKind: IPPoolList
      plural: ippools
      singular: ippool
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apiextensions.k8s.io/v1beta1","description":"Calico Network Policies","kind":"CustomResourceDefinition","metadata":{"annotations":{},"name":"networkpolicies.crd.projectcalico.org","namespace":""},"spec":{"group":"crd.projectcalico.org","names":{"kind":"NetworkPolicy","plural":"networkpolicies","singular":"networkpolicy"},"scope":"Namespaced","version":"v1"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    name: networkpolicies.crd.projectcalico.org
    namespace: ""
    resourceVersion: "224"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/networkpolicies.crd.projectcalico.org
    uid: f5ad84b6-0dd1-11e8-871b-005056af9e97
  spec:
    group: crd.projectcalico.org
    names:
      kind: NetworkPolicy
      listKind: NetworkPolicyList
      plural: networkpolicies
      singular: networkpolicy
    scope: Namespaced
    version: v1
  status:
    acceptedNames:
      kind: NetworkPolicy
      listKind: NetworkPolicyList
      plural: networkpolicies
      singular: networkpolicy
    conditions:
    - lastTransitionTime: null
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: 2018-02-09T19:46:45Z
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get daemonsets --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"name":"ccphxvolume"},"name":"ccphxvolume","namespace":"default"},"spec":{"template":{"metadata":{"labels":{"name":"ccphxvolume"}},"spec":{"containers":[{"command":["/bin/sh","-c","while true; do sleep 2; done"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","imagePullPolicy":"IfNotPresent","name":"hxvolume","securityContext":{"privileged":true}}],"initContainers":[{"command":["sh","-c","SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name \u003e /etc/iscsi/initiatorname.iscsi"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"iscsi-initiator"},{"command":["sh","-c","cp /hxcache/hxvolume /hxhostmount/"],"image":"registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge","name":"hxvolume-copy","volumeMounts":[{"mountPath":"/hxhostmount","name":"hxvolume-mount"},{"mountPath":"/etc/iscsi","name":"iscsi-volume"}]}],"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"hostPath":{"path":"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/"},"name":"hxvolume-mount"},{"hostPath":{"path":"/etc/iscsi"},"name":"iscsi-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      name: ccphxvolume
    name: ccphxvolume
    namespace: default
    resourceVersion: "446863"
    selfLink: /apis/extensions/v1beta1/namespaces/default/daemonsets/ccphxvolume
    uid: f73cde15-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: ccphxvolume
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: ccphxvolume
      spec:
        containers:
        - command:
          - /bin/sh
          - -c
          - while true; do sleep 2; done
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: iscsi-initiator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - sh
          - -c
          - cp /hxcache/hxvolume /hxhostmount/
          image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
          imagePullPolicy: IfNotPresent
          name: hxvolume-copy
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /hxhostmount
            name: hxvolume-mount
          - mountPath: /etc/iscsi
            name: iscsi-volume
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - hostPath:
            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
            type: ""
          name: hxvolume-mount
        - hostPath:
            path: /etc/iscsi
            type: ""
          name: iscsi-volume
    templateGeneration: 1
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    creationTimestamp: 2018-02-13T19:01:11Z
    generation: 1
    labels:
      name: kube-keepalived-vip
    name: kube-keepalived-vip
    namespace: default
    resourceVersion: "473538"
    selfLink: /apis/extensions/v1beta1/namespaces/default/daemonsets/kube-keepalived-vip
    uid: 41d5383d-10f0-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: kube-keepalived-vip
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: kube-keepalived-vip
      spec:
        containers:
        - args:
          - --services-configmap=default/vip-configmap
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/kube-keepalived-vip:0.11
          imagePullPolicy: Always
          name: kube-keepalived-vip
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: modules
            readOnly: true
          - mountPath: /dev
            name: dev
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-keepalived-vip
        serviceAccountName: kube-keepalived-vip
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: modules
        - hostPath:
            path: /dev
            type: ""
          name: dev
    templateGeneration: 1
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"FELIX_LOGSEVERITYSCREEN","value":"info"},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_IPINIPMTU","value":"1440"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"CALICO_IPV4POOL_CIDR","value":"192.168.0.0/16"},{"name":"CALICO_IPV4POOL_IPIP","value":"Always"},{"name":"FELIX_IPINIPENABLED","value":"true"},{"name":"FELIX_TYPHAK8SSERVICENAME","valueFrom":{"configMapKeyRef":{"key":"typha_service_name","name":"calico-config"}}},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"IP","value":""},{"name":"FELIX_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master","livenessProbe":{"failureThreshold":6,"httpGet":{"path":"/liveness","port":9099},"initialDelaySeconds":10,"periodSeconds":10},"name":"calico-node","readinessProbe":{"httpGet":{"path":"/readiness","port":9099},"periodSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false}]},{"command":["/install-cni.sh"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master","name":"install-cni","volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"hostNetwork":true,"serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-node
    name: calico-node
    namespace: kube-system
    resourceVersion: "446861"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/calico-node
    uid: f590af99-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: FELIX_LOGSEVERITYSCREEN
            value: info
          - name: CLUSTER_TYPE
            value: k8s,bgp
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_IPINIPMTU
            value: "1440"
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: CALICO_IPV4POOL_CIDR
            value: 192.168.0.0/16
          - name: CALICO_IPV4POOL_IPIP
            value: Always
          - name: FELIX_IPINIPENABLED
            value: "true"
          - name: FELIX_TYPHAK8SSERVICENAME
            valueFrom:
              configMapKeyRef:
                key: typha_service_name
                name: calico-config
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: IP
          - name: FELIX_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /liveness
              port: 9099
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-node
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9099
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /var/run/calico
            name: var-run-calico
        - command:
          - /install-cni.sh
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: cni_network_config
                name: calico-config
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"},"name":"fluentd-es-v2.0.2","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-es","version":"v2.0.2"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true","version":"v2.0.2"}},"spec":{"containers":[{"env":[{"name":"FLUENTD_ARGS","value":"--no-supervisor -q"}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2","name":"fluentd-es","resources":{"limits":{"memory":"500Mi"},"requests":{"cpu":"100m","memory":"200Mi"}},"volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/host/lib","name":"libsystemddir","readOnly":true},{"mountPath":"/etc/fluent/config.d","name":"config-volume"}]}],"serviceAccountName":"fluentd-es","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"hostPath":{"path":"/usr/lib64"},"name":"libsystemddir"},{"configMap":{"name":"fluentd-es-config-v0.1.1"},"name":"config-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:46Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      version: v2.0.2
    name: fluentd-es-v2.0.2
    namespace: kube-system
    resourceVersion: "446860"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/fluentd-es-v2.0.2
    uid: f626704d-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-es
        version: v2.0.2
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: fluentd-es
          kubernetes.io/cluster-service: "true"
          version: v2.0.2
      spec:
        containers:
        - env:
          - name: FLUENTD_ARGS
            value: --no-supervisor -q
          image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
          imagePullPolicy: IfNotPresent
          name: fluentd-es
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/log
            name: varlog
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
          - mountPath: /host/lib
            name: libsystemddir
            readOnly: true
          - mountPath: /etc/fluent/config.d
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-es
        serviceAccountName: fluentd-es
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
        - hostPath:
            path: /usr/lib64
            type: ""
          name: libsystemddir
        - configMap:
            defaultMode: 420
            name: fluentd-es-config-v0.1.1
          name: config-volume
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 1
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "446859"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/kube-proxy
    uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
          - --cluster-cidr=192.168.0.0/16
          image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          value: "true"
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    templateGeneration: 1
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 1
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get deployments --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473488"
    selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/my-nginx-ingress-controller
    uid: a14f1a1b-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473454"
    selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/my-nginx-ingress-default-backend
    uid: a14f9e60-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        release: my-nginx-ingress
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-13T21:55:39Z
      lastUpdateTime: 2018-02-13T21:55:39Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"spec":{"replicas":0,"revisionHistoryLimit":2,"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha"}},"spec":{"containers":[{"env":[{"name":"TYPHA_LOGSEVERITYSCREEN","value":"info"},{"name":"TYPHA_LOGFILEPATH","value":"none"},{"name":"TYPHA_LOGSEVERITYSYS","value":"none"},{"name":"TYPHA_CONNECTIONREBALANCINGMODE","value":"kubernetes"},{"name":"TYPHA_DATASTORETYPE","value":"kubernetes"},{"name":"TYPHA_HEALTHENABLED","value":"true"}],"image":"registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master","livenessProbe":{"httpGet":{"path":"/liveness","port":9098},"initialDelaySeconds":30,"periodSeconds":30},"name":"calico-typha","ports":[{"containerPort":5473,"name":"calico-typha","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/readiness","port":9098},"periodSeconds":10}}],"hostNetwork":true,"serviceAccountName":"calico-node","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    generation: 1
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "360"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/calico-typha
    uid: f58c96a5-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: calico-typha
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    conditions:
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:46:57Z
      message: ReplicaSet "calico-typha-65b7467b56" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","kubernetes.io/cluster-service":"true"},"name":"kibana-logging","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"kibana-logging"}},"template":{"metadata":{"labels":{"k8s-app":"kibana-logging"}},"spec":{"containers":[{"env":[{"name":"ELASTICSEARCH_URL","value":"http://elasticsearch-logging:9200"},{"name":"XPACK_MONITORING_ENABLED","value":"false"},{"name":"XPACK_SECURITY_ENABLED","value":"false"}],"image":"registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4","name":"kibana-logging","ports":[{"containerPort":5601,"name":"ui","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}}}]}}}}
    creationTimestamp: 2018-02-09T19:46:47Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "807"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kibana-logging
    uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kibana-logging
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:45Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:45Z
      message: ReplicaSet "kibana-logging-767cf49759" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:44Z
    generation: 1
    labels:
      k8s-app: kube-dns
    name: kube-dns
    namespace: kube-system
    resourceVersion: "854"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-dns
    uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:50Z
      lastUpdateTime: 2018-02-09T19:47:50Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"},"spec":{"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"k8s-app":"kubernetes-dashboard"}},"template":{"metadata":{"labels":{"k8s-app":"kubernetes-dashboard"}},"spec":{"containers":[{"args":["--auto-generate-certificates"],"image":"k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1","livenessProbe":{"httpGet":{"path":"/","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":30,"timeoutSeconds":30},"name":"kubernetes-dashboard","ports":[{"containerPort":8443,"protocol":"TCP"}],"volumeMounts":[{"mountPath":"/certs","name":"kubernetes-dashboard-certs"},{"mountPath":"/tmp","name":"tmp-volume"}]}],"serviceAccountName":"kubernetes-dashboard","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"name":"kubernetes-dashboard-certs","secret":{"secretName":"kubernetes-dashboard-certs"}},{"emptyDir":{},"name":"tmp-volume"}]}}}}
    creationTimestamp: 2018-02-09T19:46:48Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "774"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kubernetes-dashboard
    uid: f79d83b7-0dd1-11e8-871b-005056af9e97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:47:42Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: 2018-02-09T19:46:57Z
      lastUpdateTime: 2018-02-09T19:47:42Z
      message: ReplicaSet "kubernetes-dashboard-7798c48646" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "1069"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/tiller-deploy
    uid: 4998e55c-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: 2018-02-09T19:49:06Z
      lastUpdateTime: 2018-02-09T19:49:06Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get endpoints --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:40Z
    name: kubernetes
    namespace: default
    resourceVersion: "32"
    selfLink: /api/v1/namespaces/default/endpoints/kubernetes
    uid: f2eb67e2-0dd1-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 10.10.97.20
    ports:
    - name: https
      port: 6443
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473486"
    selfLink: /api/v1/namespaces/default/endpoints/my-nginx-ingress-controller
    uid: a14ad324-1108-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.2.19
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: my-nginx-ingress-controller-d95d4979d-kzgk7
        namespace: default
        resourceVersion: "473485"
        uid: a1522304-1108-11e8-871b-005056af9e97
    ports:
    - name: https
      port: 443
      protocol: TCP
    - name: http
      port: 80
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473453"
    selfLink: /api/v1/namespaces/default/endpoints/my-nginx-ingress-default-backend
    uid: a14ef958-1108-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.2.18
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: my-nginx-ingress-default-backend-855d89f775-p8jk6
        namespace: default
        resourceVersion: "473451"
        uid: a155085d-1108-11e8-871b-005056af9e97
    ports:
    - port: 8080
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-12T16:02:17Z
    labels:
      app: tea
    name: tea-svc
    namespace: default
    resourceVersion: "327043"
    selfLink: /api/v1/namespaces/default/endpoints/tea-svc
    uid: 1990e081-100e-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.1.52
      nodeName: vhosakot1-we2d86faeb2
      targetRef:
        kind: Pod
        name: tea-rc-v8rxv
        namespace: default
        resourceVersion: "327028"
        uid: 16662cc9-100e-11e8-871b-005056af9e97
    - ip: 192.168.2.14
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: tea-rc-fxj6x
        namespace: default
        resourceVersion: "327030"
        uid: 1664100f-100e-11e8-871b-005056af9e97
    - ip: 192.168.2.15
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: tea-rc-96442
        namespace: default
        resourceVersion: "327037"
        uid: 1666089f-100e-11e8-871b-005056af9e97
    ports:
    - name: http
      port: 80
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "333"
    selfLink: /api/v1/namespaces/kube-system/endpoints/calico-typha
    uid: fcf4d65b-0dd1-11e8-871b-005056af9e97
  subsets: null
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Elasticsearch
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "842"
    selfLink: /api/v1/namespaces/kube-system/endpoints/elasticsearch-logging
    uid: fcf53f94-0dd1-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - hostname: elasticsearch-logging-0
      ip: 192.168.1.4
      nodeName: vhosakot1-we2d86faeb2
      targetRef:
        kind: Pod
        name: elasticsearch-logging-0
        namespace: kube-system
        resourceVersion: "810"
        uid: fd00ccb1-0dd1-11e8-871b-005056af9e97
    - hostname: elasticsearch-logging-1
      ip: 192.168.2.5
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: elasticsearch-logging-1
        namespace: kube-system
        resourceVersion: "841"
        uid: 1982328b-0dd2-11e8-871b-005056af9e97
    ports:
    - port: 9200
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Kibana
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "805"
    selfLink: /api/v1/namespaces/kube-system/endpoints/kibana-logging
    uid: fcf56a63-0dd1-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.2.4
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: kibana-logging-767cf49759-f8zjt
        namespace: kube-system
        resourceVersion: "803"
        uid: fd2570b3-0dd1-11e8-871b-005056af9e97
    ports:
    - port: 5601
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"vhosakot1-m51b5b468be","leaseDurationSeconds":15,"acquireTime":"2018-02-09T19:46:43Z","renewTime":"2018-02-13T22:47:22Z","leaderTransitions":0}'
    creationTimestamp: 2018-02-09T19:46:43Z
    name: kube-controller-manager
    namespace: kube-system
    resourceVersion: "477922"
    selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
    uid: f47b5da6-0dd1-11e8-871b-005056af9e97
  subsets: null
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "853"
    selfLink: /api/v1/namespaces/kube-system/endpoints/kube-dns
    uid: fcf55a98-0dd1-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.0.4
      nodeName: vhosakot1-m51b5b468be
      targetRef:
        kind: Pod
        name: kube-dns-545bc4bfd4-rzpz4
        namespace: kube-system
        resourceVersion: "851"
        uid: fd048a95-0dd1-11e8-871b-005056af9e97
    ports:
    - name: dns
      port: 53
      protocol: UDP
    - name: dns-tcp
      port: 53
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"vhosakot1-m51b5b468be","leaseDurationSeconds":15,"acquireTime":"2018-02-09T19:46:42Z","renewTime":"2018-02-13T22:47:21Z","leaderTransitions":0}'
    creationTimestamp: 2018-02-09T19:46:42Z
    name: kube-scheduler
    namespace: kube-system
    resourceVersion: "477921"
    selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
    uid: f41b7a0d-0dd1-11e8-871b-005056af9e97
  subsets: null
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "772"
    selfLink: /api/v1/namespaces/kube-system/endpoints/kubernetes-dashboard
    uid: fcf57020-0dd1-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.0.3
      nodeName: vhosakot1-m51b5b468be
      targetRef:
        kind: Pod
        name: kubernetes-dashboard-7798c48646-rjmch
        namespace: kube-system
        resourceVersion: "771"
        uid: fcff0932-0dd1-11e8-871b-005056af9e97
    ports:
    - port: 8443
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    creationTimestamp: 2018-02-09T19:49:06Z
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "1067"
    selfLink: /api/v1/namespaces/kube-system/endpoints/tiller-deploy
    uid: 49a75147-0dd2-11e8-871b-005056af9e97
  subsets:
  - addresses:
    - ip: 192.168.2.6
      nodeName: vhosakot1-wc80d3e5ab6
      targetRef:
        kind: Pod
        name: tiller-deploy-546cf9696c-w4kq6
        namespace: kube-system
        resourceVersion: "1066"
        uid: 5435726c-0dd2-11e8-871b-005056af9e97
    ports:
    - name: tiller
      port: 44134
      protocol: TCP
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get events --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:50Z
  involvedObject:
    apiVersion: extensions
    kind: Ingress
    name: cafe-ingress
    namespace: default
    resourceVersion: "447311"
    uid: 21048cc8-100e-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:50Z
  message: Ingress default/cafe-ingress
  metadata:
    creationTimestamp: 2018-02-13T21:55:49Z
    name: cafe-ingress.15130151c58ea80b
    namespace: default
    resourceVersion: "473474"
    selfLink: /api/v1/namespaces/default/events/cafe-ingress.15130151c58ea80b
    uid: a7029d48-1108-11e8-871b-005056af9e97
  reason: CREATE
  source:
    component: nginx-ingress-controller
  type: Normal
- apiVersion: v1
  count: 4
  firstTimestamp: 2018-02-13T19:01:12Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-keepalived-vip}
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:24Z
  message: pulling image "k8s.gcr.io/kube-keepalived-vip:0.11"
  metadata:
    creationTimestamp: 2018-02-13T21:55:25Z
    name: kube-keepalived-vip-fph6b.1512f7ca5b38a1cc
    namespace: default
    resourceVersion: "473531"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca5b38a1cc
    uid: 98d7b162-1108-11e8-871b-005056af9e97
  reason: Pulling
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 4
  firstTimestamp: 2018-02-13T19:01:13Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-keepalived-vip}
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:25Z
  message: Successfully pulled image "k8s.gcr.io/kube-keepalived-vip:0.11"
  metadata:
    creationTimestamp: 2018-02-13T21:55:26Z
    name: kube-keepalived-vip-fph6b.1512f7ca825390ad
    namespace: default
    resourceVersion: "473534"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca825390ad
    uid: 993b5c48-1108-11e8-871b-005056af9e97
  reason: Pulled
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 4
  firstTimestamp: 2018-02-13T19:01:13Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-keepalived-vip}
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:25Z
  message: Created container
  metadata:
    creationTimestamp: 2018-02-13T21:55:26Z
    name: kube-keepalived-vip-fph6b.1512f7ca85d1fe56
    namespace: default
    resourceVersion: "473535"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca85d1fe56
    uid: 9944fa54-1108-11e8-871b-005056af9e97
  reason: Created
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 4
  firstTimestamp: 2018-02-13T19:01:13Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-keepalived-vip}
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:25Z
  message: Started container
  metadata:
    creationTimestamp: 2018-02-13T21:55:26Z
    name: kube-keepalived-vip-fph6b.1512f7ca8bd6ddbd
    namespace: default
    resourceVersion: "473536"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1512f7ca8bd6ddbd
    uid: 9956cda9-1108-11e8-871b-005056af9e97
  reason: Started
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 3
  firstTimestamp: 2018-02-13T21:55:34Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-keepalived-vip}
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:09Z
  message: Back-off restarting failed container
  metadata:
    creationTimestamp: 2018-02-13T21:55:33Z
    name: kube-keepalived-vip-fph6b.1513014e24e93088
    namespace: default
    resourceVersion: "473509"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1513014e24e93088
    uid: 9db957a0-1108-11e8-871b-005056af9e97
  reason: BackOff
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Warning
- apiVersion: v1
  count: 3
  firstTimestamp: 2018-02-13T21:55:34Z
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-keepalived-vip-fph6b
    namespace: default
    resourceVersion: "458292"
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:56:09Z
  message: Error syncing pod
  metadata:
    creationTimestamp: 2018-02-13T21:55:33Z
    name: kube-keepalived-vip-fph6b.1513014e24eef2af
    namespace: default
    resourceVersion: "473510"
    selfLink: /api/v1/namespaces/default/events/kube-keepalived-vip-fph6b.1513014e24eef2af
    uid: 9db9c435-1108-11e8-871b-005056af9e97
  reason: FailedSync
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Warning
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:24Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-cs7mm
    namespace: default
    resourceVersion: "447142"
    uid: 7cd083ae-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:24Z
  message: 'Liveness probe failed: HTTP probe failed with statuscode: 500'
  metadata:
    creationTimestamp: 2018-02-13T21:55:23Z
    name: my-nginx-ingress-controller-d95d4979d-cs7mm.1513014bd41394f2
    namespace: default
    resourceVersion: "473346"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014bd41394f2
    uid: 97cbac64-1108-11e8-871b-005056af9e97
  reason: Unhealthy
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Warning
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:26Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-cs7mm
    namespace: default
    resourceVersion: "447142"
    uid: 7cd083ae-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:26Z
  message: 'Readiness probe failed: HTTP probe failed with statuscode: 500'
  metadata:
    creationTimestamp: 2018-02-13T21:55:26Z
    name: my-nginx-ingress-controller-d95d4979d-cs7mm.1513014c53ffe35a
    namespace: default
    resourceVersion: "473364"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014c53ffe35a
    uid: 99132654-1108-11e8-871b-005056af9e97
  reason: Unhealthy
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Warning
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:30Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-cs7mm
    namespace: default
    resourceVersion: "447142"
    uid: 7cd083ae-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:30Z
  message: Killing container with id docker://nginx-ingress-controller:Need to kill
    Pod
  metadata:
    creationTimestamp: 2018-02-13T21:55:30Z
    name: my-nginx-ingress-controller-d95d4979d-cs7mm.1513014d53723904
    namespace: default
    resourceVersion: "473377"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-cs7mm.1513014d53723904
    uid: 9ba11c75-1108-11e8-871b-005056af9e97
  reason: Killing
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    resourceVersion: "473420"
    uid: a1522304-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: Successfully assigned my-nginx-ingress-controller-d95d4979d-kzgk7 to vhosakot1-wc80d3e5ab6
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f669c2243
    namespace: default
    resourceVersion: "473429"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f669c2243
    uid: a154e90d-1108-11e8-871b-005056af9e97
  reason: Scheduled
  source:
    component: default-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:40Z
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    resourceVersion: "473425"
    uid: a1522304-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:40Z
  message: 'MountVolume.SetUp succeeded for volume "my-nginx-ingress-token-9ffd9" '
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f9ddacac3
    namespace: default
    resourceVersion: "473441"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014f9ddacac3
    uid: a17ea26a-1108-11e8-871b-005056af9e97
  reason: SuccessfulMountVolume
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    resourceVersion: "473425"
    uid: a1522304-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Container image "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2"
    already present on machine
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc229c270
    namespace: default
    resourceVersion: "473445"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc229c270
    uid: a1db4ee9-1108-11e8-871b-005056af9e97
  reason: Pulled
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    resourceVersion: "473425"
    uid: a1522304-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Created container
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc7026fae
    namespace: default
    resourceVersion: "473447"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fc7026fae
    uid: a1e7a1a6-1108-11e8-871b-005056af9e97
  reason: Created
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-controller}
    kind: Pod
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    resourceVersion: "473425"
    uid: a1522304-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Started container
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fceb03e13
    namespace: default
    resourceVersion: "473450"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d-kzgk7.1513014fceb03e13
    uid: a1fb50b5-1108-11e8-871b-005056af9e97
  reason: Started
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:19Z
  involvedObject:
    apiVersion: extensions
    kind: ReplicaSet
    name: my-nginx-ingress-controller-d95d4979d
    namespace: default
    resourceVersion: "473318"
    uid: 7cce3e08-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:19Z
  message: 'Deleted pod: my-nginx-ingress-controller-d95d4979d-cs7mm'
  metadata:
    creationTimestamp: 2018-02-13T21:55:19Z
    name: my-nginx-ingress-controller-d95d4979d.1513014a9836c301
    namespace: default
    resourceVersion: "473324"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d.1513014a9836c301
    uid: 95071c41-1108-11e8-871b-005056af9e97
  reason: SuccessfulDelete
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: extensions
    kind: ReplicaSet
    name: my-nginx-ingress-controller-d95d4979d
    namespace: default
    resourceVersion: "473413"
    uid: a14fe756-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: 'Created pod: my-nginx-ingress-controller-d95d4979d-kzgk7'
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-controller-d95d4979d.1513014f660b56e4
    namespace: default
    resourceVersion: "473427"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller-d95d4979d.1513014f660b56e4
    uid: a1539e36-1108-11e8-871b-005056af9e97
  reason: SuccessfulCreate
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:19Z
  involvedObject:
    apiVersion: extensions
    kind: Deployment
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473317"
    uid: 7ccd026e-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:19Z
  message: Scaled down replica set my-nginx-ingress-controller-d95d4979d to 0
  metadata:
    creationTimestamp: 2018-02-13T21:55:19Z
    name: my-nginx-ingress-controller.1513014a979e01a5
    namespace: default
    resourceVersion: "473321"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller.1513014a979e01a5
    uid: 9505af83-1108-11e8-871b-005056af9e97
  reason: ScalingReplicaSet
  source:
    component: deployment-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: extensions
    kind: Deployment
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473412"
    uid: a14f1a1b-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: Scaled up replica set my-nginx-ingress-controller-d95d4979d to 1
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-controller.1513014f64f57f7f
    namespace: default
    resourceVersion: "473416"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-controller.1513014f64f57f7f
    uid: a150ea6c-1108-11e8-871b-005056af9e97
  reason: ScalingReplicaSet
  source:
    component: deployment-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:23Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-default-backend}
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p495h
    namespace: default
    resourceVersion: "447149"
    uid: 7cd07bec-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:23Z
  message: Killing container with id docker://nginx-ingress-default-backend:Need to
    kill Pod
  metadata:
    creationTimestamp: 2018-02-13T21:55:22Z
    name: my-nginx-ingress-default-backend-855d89f775-p495h.1513014b7ac09d99
    namespace: default
    resourceVersion: "473341"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p495h.1513014b7ac09d99
    uid: 96e7015b-1108-11e8-871b-005056af9e97
  reason: Killing
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    resourceVersion: "473426"
    uid: a155085d-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: Successfully assigned my-nginx-ingress-default-backend-855d89f775-p8jk6
    to vhosakot1-wc80d3e5ab6
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f67b5a669
    namespace: default
    resourceVersion: "473436"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f67b5a669
    uid: a157ae5f-1108-11e8-871b-005056af9e97
  reason: Scheduled
  source:
    component: default-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:40Z
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    resourceVersion: "473430"
    uid: a155085d-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:40Z
  message: 'MountVolume.SetUp succeeded for volume "default-token-fc6vg" '
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f9dd3ee65
    namespace: default
    resourceVersion: "473440"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014f9dd3ee65
    uid: a17e3076-1108-11e8-871b-005056af9e97
  reason: SuccessfulMountVolume
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-default-backend}
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    resourceVersion: "473430"
    uid: a155085d-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Container image "k8s.gcr.io/defaultbackend:1.3" already present on machine
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc0342d14
    namespace: default
    resourceVersion: "473444"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc0342d14
    uid: a1d6572d-1108-11e8-871b-005056af9e97
  reason: Pulled
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-default-backend}
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    resourceVersion: "473430"
    uid: a155085d-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Created container
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc5b17d44
    namespace: default
    resourceVersion: "473446"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fc5b17d44
    uid: a1e445ca-1108-11e8-871b-005056af9e97
  reason: Created
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:41Z
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{nginx-ingress-default-backend}
    kind: Pod
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    resourceVersion: "473430"
    uid: a155085d-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:41Z
  message: Started container
  metadata:
    creationTimestamp: 2018-02-13T21:55:40Z
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fccc1a9d7
    namespace: default
    resourceVersion: "473449"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775-p8jk6.1513014fccc1a9d7
    uid: a1f675ef-1108-11e8-871b-005056af9e97
  reason: Started
  source:
    component: kubelet
    host: vhosakot1-wc80d3e5ab6
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:22Z
  involvedObject:
    apiVersion: extensions
    kind: ReplicaSet
    name: my-nginx-ingress-default-backend-855d89f775
    namespace: default
    resourceVersion: "473333"
    uid: 7ccea98b-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:22Z
  message: 'Deleted pod: my-nginx-ingress-default-backend-855d89f775-p495h'
  metadata:
    creationTimestamp: 2018-02-13T21:55:22Z
    name: my-nginx-ingress-default-backend-855d89f775.1513014b501f2edd
    namespace: default
    resourceVersion: "473339"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775.1513014b501f2edd
    uid: 96ddfd5f-1108-11e8-871b-005056af9e97
  reason: SuccessfulDelete
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: extensions
    kind: ReplicaSet
    name: my-nginx-ingress-default-backend-855d89f775
    namespace: default
    resourceVersion: "473418"
    uid: a151592a-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: 'Created pod: my-nginx-ingress-default-backend-855d89f775-p8jk6'
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-default-backend-855d89f775.1513014f670fda78
    namespace: default
    resourceVersion: "473433"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend-855d89f775.1513014f670fda78
    uid: a1565d8c-1108-11e8-871b-005056af9e97
  reason: SuccessfulCreate
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:22Z
  involvedObject:
    apiVersion: extensions
    kind: Deployment
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473332"
    uid: 7ccdfe1f-10de-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:22Z
  message: Scaled down replica set my-nginx-ingress-default-backend-855d89f775 to
    0
  metadata:
    creationTimestamp: 2018-02-13T21:55:22Z
    name: my-nginx-ingress-default-backend.1513014b4f9d7b8e
    namespace: default
    resourceVersion: "473335"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend.1513014b4f9d7b8e
    uid: 96dc8aa8-1108-11e8-871b-005056af9e97
  reason: ScalingReplicaSet
  source:
    component: deployment-controller
  type: Normal
- apiVersion: v1
  count: 1
  firstTimestamp: 2018-02-13T21:55:39Z
  involvedObject:
    apiVersion: extensions
    kind: Deployment
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473414"
    uid: a14f9e60-1108-11e8-871b-005056af9e97
  kind: Event
  lastTimestamp: 2018-02-13T21:55:39Z
  message: Scaled up replica set my-nginx-ingress-default-backend-855d89f775 to 1
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-default-backend.1513014f656e8854
    namespace: default
    resourceVersion: "473422"
    selfLink: /api/v1/namespaces/default/events/my-nginx-ingress-default-backend.1513014f656e8854
    uid: a1520e3d-1108-11e8-871b-005056af9e97
  reason: ScalingReplicaSet
  source:
    component: deployment-controller
  type: Normal
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get horizontalpodautoscalers --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get ingresses --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    creationTimestamp: 2018-02-12T16:02:30Z
    generation: 1
    name: cafe-ingress
    namespace: default
    resourceVersion: "447311"
    selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/cafe-ingress
    uid: 21048cc8-100e-11e8-871b-005056af9e97
  spec:
    rules:
    - host: cafe.example.com
      http:
        paths:
        - backend:
            serviceName: tea-svc
            servicePort: 80
          path: /tea
        - backend:
            serviceName: coffee-svc
            servicePort: 80
          path: /coffee
    tls:
    - hosts:
      - cafe.example.com
      secretName: cafe-secret
  status:
    loadBalancer:
      ingress:
      - {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get jobs --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get limitranges --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get namespaces --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: 2018-02-09T19:46:36Z
    name: default
    namespace: ""
    resourceVersion: "4"
    selfLink: /api/v1/namespaces/default
    uid: f086f363-0dd1-11e8-871b-005056af9e97
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: 2018-02-09T19:46:40Z
    name: kube-public
    namespace: ""
    resourceVersion: "33"
    selfLink: /api/v1/namespaces/kube-public
    uid: f2ec66e2-0dd1-11e8-871b-005056af9e97
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: 2018-02-09T19:46:37Z
    name: kube-system
    namespace: ""
    resourceVersion: "11"
    selfLink: /api/v1/namespaces/kube-system
    uid: f1162a30-0dd1-11e8-871b-005056af9e97
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get networkpolicies --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get nodes --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      node.alpha.kubernetes.io/ttl: "0"
      projectcalico.org/IPv4Address: 10.10.97.20/22
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: 2018-02-09T19:46:40Z
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/hostname: vhosakot1-m51b5b468be
      node-role.kubernetes.io/master: ""
    name: vhosakot1-m51b5b468be
    namespace: ""
    resourceVersion: "477918"
    selfLink: /api/v1/nodes/vhosakot1-m51b5b468be
    uid: f27efaf2-0dd1-11e8-871b-005056af9e97
  spec:
    externalID: vhosakot1-m51b5b468be
    podCIDR: 192.168.0.0/24
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      timeAdded: null
  status:
    addresses:
    - address: 10.10.97.20
      type: InternalIP
    - address: vhosakot1-m51b5b468be
      type: Hostname
    allocatable:
      cpu: "2"
      memory: 16330380Ki
      pods: "110"
    capacity:
      cpu: "2"
      memory: 16432780Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:46:36Z
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:46:36Z
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:46:36Z
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:47:10Z
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      sizeBytes: 876978121
    - names:
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      sizeBytes: 675097177
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      sizeBytes: 271002833
    - names:
      - gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
      - gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      sizeBytes: 194371442
    - names:
      - gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
      - gcr.io/google_containers/etcd-amd64:3.0.17
      sizeBytes: 168935516
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      sizeBytes: 134801501
    - names:
      - gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
      - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      sizeBytes: 129314896
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      sizeBytes: 120712806
    - names:
      - gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      - gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      sizeBytes: 93199092
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      sizeBytes: 67306811
    - names:
      - gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
      - gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      sizeBytes: 54976083
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha@sha256:1f66c171a4756d1e9f6c49b9a0b93e24a9ec2aa9b4e1bb40c69469edb20f8e25
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
      sizeBytes: 54773713
    - names:
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      sizeBytes: 49387411
    - names:
      - gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
      - gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      sizeBytes: 41819177
    - names:
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      sizeBytes: 41423617
    - names:
      - registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      - registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      sizeBytes: 13716608
    - names:
      - registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      - registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      sizeBytes: 3965926
    - names:
      - gcr.io/google_containers/pause-amd64@sha256:163ac025575b775d1c0f9bf0bdd0f086883171eb475b5068e7defa4ca9e76516
      - gcr.io/google_containers/pause-amd64:3.0
      sizeBytes: 746888
    nodeInfo:
      architecture: amd64
      bootID: 7787ad21-ad5f-4249-9c27-6fe98b4b2745
      containerRuntimeVersion: docker://1.13.1
      kernelVersion: 4.4.0-109-generic
      kubeProxyVersion: v1.8.4
      kubeletVersion: v1.8.4
      machineID: 9ebc80503a934280b09c657524f9dcb4
      operatingSystem: linux
      osImage: Ubuntu 16.04.3 LTS
      systemUUID: 422F6C6E-40D4-4689-47A1-347776818F58
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      node.alpha.kubernetes.io/ttl: "0"
      projectcalico.org/IPv4Address: 10.10.97.62/22
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: 2018-02-09T19:47:19Z
    labels:
      app: keepalived
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/hostname: vhosakot1-wc80d3e5ab6
    name: vhosakot1-wc80d3e5ab6
    namespace: ""
    resourceVersion: "477914"
    selfLink: /api/v1/nodes/vhosakot1-wc80d3e5ab6
    uid: 09ab323e-0dd2-11e8-871b-005056af9e97
  spec:
    externalID: vhosakot1-wc80d3e5ab6
    podCIDR: 192.168.2.0/24
  status:
    addresses:
    - address: 10.10.97.62
      type: InternalIP
    - address: vhosakot1-wc80d3e5ab6
      type: Hostname
    allocatable:
      cpu: "2"
      memory: 16330380Ki
      pods: "110"
    capacity:
      cpu: "2"
      memory: 16432780Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: 2018-02-13T22:47:18Z
      lastTransitionTime: 2018-02-09T19:47:28Z
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      sizeBytes: 876978121
    - names:
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      sizeBytes: 675097177
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      sizeBytes: 271002833
    - names:
      - quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
      - quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      sizeBytes: 203263851
    - names:
      - gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
      - gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      sizeBytes: 194371442
    - names:
      - gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
      - gcr.io/google_containers/etcd-amd64:3.0.17
      sizeBytes: 168935516
    - names:
      - k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
      - k8s.gcr.io/kube-keepalived-vip:0.11
      sizeBytes: 142480400
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      sizeBytes: 134801501
    - names:
      - gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
      - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      sizeBytes: 129314896
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      sizeBytes: 120712806
    - names:
      - gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      - gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      sizeBytes: 93199092
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      sizeBytes: 67306811
    - names:
      - gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
      - gcr.io/kubernetes-helm/tiller:v2.7.2
      sizeBytes: 56595240
    - names:
      - gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
      - gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      sizeBytes: 54976083
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha@sha256:1f66c171a4756d1e9f6c49b9a0b93e24a9ec2aa9b4e1bb40c69469edb20f8e25
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
      sizeBytes: 54773713
    - names:
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      sizeBytes: 49387411
    - names:
      - gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
      - gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      sizeBytes: 41819177
    - names:
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      sizeBytes: 41423617
    - names:
      - nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      - nginxdemos/hello:latest
      sizeBytes: 16826504
    - names:
      - registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      - registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      sizeBytes: 13716608
    - names:
      - registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      - registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      sizeBytes: 3965926
    - names:
      - k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
      - k8s.gcr.io/defaultbackend:1.3
      sizeBytes: 3618880
    - names:
      - gcr.io/google_containers/pause-amd64@sha256:163ac025575b775d1c0f9bf0bdd0f086883171eb475b5068e7defa4ca9e76516
      - gcr.io/google_containers/pause-amd64:3.0
      sizeBytes: 746888
    nodeInfo:
      architecture: amd64
      bootID: 156c77a6-1219-4d3f-bbfb-8b08e2227f82
      containerRuntimeVersion: docker://1.13.1
      kernelVersion: 4.4.0-109-generic
      kubeProxyVersion: v1.8.4
      kubeletVersion: v1.8.4
      machineID: 9ebc80503a934280b09c657524f9dcb4
      operatingSystem: linux
      osImage: Ubuntu 16.04.3 LTS
      systemUUID: 422F02B5-096A-AAD9-2560-FDBAB5F059B8
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      node.alpha.kubernetes.io/ttl: "0"
      projectcalico.org/IPv4Address: 10.10.97.46/22
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: 2018-02-09T19:47:18Z
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/hostname: vhosakot1-we2d86faeb2
    name: vhosakot1-we2d86faeb2
    namespace: ""
    resourceVersion: "477923"
    selfLink: /api/v1/nodes/vhosakot1-we2d86faeb2
    uid: 09859f5b-0dd2-11e8-871b-005056af9e97
  spec:
    externalID: vhosakot1-we2d86faeb2
    podCIDR: 192.168.1.0/24
    taints:
    - effect: NoSchedule
      key: key
      timeAdded: null
      value: value
  status:
    addresses:
    - address: 10.10.97.46
      type: InternalIP
    - address: vhosakot1-we2d86faeb2
      type: Hostname
    allocatable:
      cpu: "2"
      memory: 16330380Ki
      pods: "110"
    capacity:
      cpu: "2"
      memory: 16432780Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: 2018-02-13T22:47:22Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: 2018-02-13T22:47:22Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: 2018-02-13T22:47:22Z
      lastTransitionTime: 2018-02-09T19:47:18Z
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: 2018-02-13T22:47:22Z
      lastTransitionTime: 2018-02-09T19:47:28Z
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      sizeBytes: 876978121
    - names:
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
      - registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      sizeBytes: 675097177
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      sizeBytes: 271002833
    - names:
      - quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
      - quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      sizeBytes: 203263851
    - names:
      - gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
      - gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      sizeBytes: 194371442
    - names:
      - gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
      - gcr.io/google_containers/etcd-amd64:3.0.17
      sizeBytes: 168935516
    - names:
      - k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
      - k8s.gcr.io/kube-keepalived-vip:0.11
      sizeBytes: 142480400
    - names:
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      - registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      sizeBytes: 134801501
    - names:
      - gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
      - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      sizeBytes: 129314896
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      sizeBytes: 120712806
    - names:
      - gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      - gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      sizeBytes: 93199092
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      sizeBytes: 67306811
    - names:
      - gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
      - gcr.io/kubernetes-helm/tiller:v2.7.2
      sizeBytes: 56595240
    - names:
      - gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
      - gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      sizeBytes: 54976083
    - names:
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha@sha256:1f66c171a4756d1e9f6c49b9a0b93e24a9ec2aa9b4e1bb40c69469edb20f8e25
      - registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
      sizeBytes: 54773713
    - names:
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
      - gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      sizeBytes: 49387411
    - names:
      - gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
      - gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      sizeBytes: 41819177
    - names:
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
      - gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      sizeBytes: 41423617
    - names:
      - nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      - nginxdemos/hello:latest
      sizeBytes: 16826504
    - names:
      - registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      - registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      sizeBytes: 13716608
    - names:
      - registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      - registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      sizeBytes: 3965926
    - names:
      - k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
      - k8s.gcr.io/defaultbackend:1.3
      sizeBytes: 3618880
    - names:
      - gcr.io/google_containers/pause-amd64@sha256:163ac025575b775d1c0f9bf0bdd0f086883171eb475b5068e7defa4ca9e76516
      - gcr.io/google_containers/pause-amd64:3.0
      sizeBytes: 746888
    nodeInfo:
      architecture: amd64
      bootID: 3ad61f99-6f47-4ae6-8a7a-e2c88cc6f2a5
      containerRuntimeVersion: docker://1.13.1
      kernelVersion: 4.4.0-109-generic
      kubeProxyVersion: v1.8.4
      kubeletVersion: v1.8.4
      machineID: 9ebc80503a934280b09c657524f9dcb4
      operatingSystem: linux
      osImage: Ubuntu 16.04.3 LTS
      systemUUID: 422FA6EA-8678-E963-2F60-4AE55CA628FD
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get persistentvolumeclaims --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get persistentvolumes --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get poddisruptionbudgets --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get podpreset --all-namespaces -o yaml
================

the server doesn't have a resource type "podpreset"

================
kubectl get pods --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"534"}}
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-t972j
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "762"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-t972j
    uid: 09b09390-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:38Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://133d74a2f67c41276c95642a7ee2cc640806f1e710e38d528b615833a7cc6832
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:39Z
    hostIP: 10.10.97.62
    initContainerStatuses:
    - containerID: docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://86573aef1ccad22a3de139c948f10b19c3f4de3e20e0573765fff9684f63f873
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    - containerID: docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5307f33ad898b0342b08e4933e3da5125c9f207d0b2e573478d7e6f167f44015
          exitCode: 0
          finishedAt: 2018-02-09T19:47:39Z
          reason: Completed
          startedAt: 2018-02-09T19:47:39Z
    phase: Running
    podIP: 192.168.2.3
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"400"}}
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-tgb9r
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "753"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-tgb9r
    uid: 09937871-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:37Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9f4d367ff709fa795b1d3aa31b2a97474c57c8235735c1240f14aa1142059e4f
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:40Z
    hostIP: 10.10.97.46
    initContainerStatuses:
    - containerID: docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1200f5ec9ba1ab3c673eda266cd55564f9a3b74c4176ba583694e8f2567c5c69
          exitCode: 0
          finishedAt: 2018-02-09T19:47:37Z
          reason: Completed
          startedAt: 2018-02-09T19:47:37Z
    - containerID: docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://8b0955a75b12741cc6dea56a974bc3674e02b1300520a707fe352aa3fdda1d49
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    phase: Running
    podIP: 192.168.1.2
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"ccphxvolume","uid":"f73cde15-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"263"}}
    creationTimestamp: 2018-02-09T19:46:58Z
    generateName: ccphxvolume-
    labels:
      controller-revision-hash: "462338721"
      name: ccphxvolume
      pod-template-generation: "1"
    name: ccphxvolume-vlgmz
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ccphxvolume
      uid: f73cde15-0dd1-11e8-871b-005056af9e97
    resourceVersion: "751"
    selfLink: /api/v1/namespaces/default/pods/ccphxvolume-vlgmz
    uid: fd2456e4-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - while true; do sleep 2; done
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    initContainers:
    - command:
      - sh
      - -c
      - SCSI_initiator_name=$(iscsi-iname); echo $SCSI_initiator_name > /etc/iscsi/initiatorname.iscsi
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: iscsi-initiator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    - command:
      - sh
      - -c
      - cp /hxcache/hxvolume /hxhostmount/
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imagePullPolicy: IfNotPresent
      name: hxvolume-copy
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /hxhostmount
        name: hxvolume-mount
      - mountPath: /etc/iscsi
        name: iscsi-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hyperflex~hxvolume/
        type: ""
      name: hxvolume-mount
    - hostPath:
        path: /etc/iscsi
        type: ""
      name: iscsi-volume
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:39Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:10Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://db0ba7b2436c0809e058838a7510f8150b381d0c473ce93a46fa90d77c77ed55
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:40Z
    hostIP: 10.10.97.20
    initContainerStatuses:
    - containerID: docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: iscsi-initiator
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://a2bed0acf592f12a86097bfc7baf7a2017f5fe300b6cf06f973e25fc3868100b
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    - containerID: docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
      image: registry.ci.dfj.io/cpsg_ccp/hxvolume:0.6.0-10.g0ea9848.premerge
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/hxvolume@sha256:61ede2fdcca57bd02ea66c69bfa06b83d294913122ea2aefd338bef182b4d5f8
      lastState: {}
      name: hxvolume-copy
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://27d2fbe2d8916248b6b2511b0cccb72fd3c48ccec38668026a9709cea0002c12
          exitCode: 0
          finishedAt: 2018-02-09T19:47:38Z
          reason: Completed
          startedAt: 2018-02-09T19:47:38Z
    phase: Running
    podIP: 192.168.0.2
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:58Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"default","name":"kube-keepalived-vip","uid":"41d5383d-10f0-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"458290"}}
    creationTimestamp: 2018-02-13T19:01:11Z
    generateName: kube-keepalived-vip-
    labels:
      controller-revision-hash: "617734581"
      name: kube-keepalived-vip
      pod-template-generation: "1"
    name: kube-keepalived-vip-fph6b
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-keepalived-vip
      uid: 41d5383d-10f0-11e8-871b-005056af9e97
    resourceVersion: "473537"
    selfLink: /api/v1/namespaces/default/pods/kube-keepalived-vip-fph6b
    uid: 41d733d7-10f0-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - --services-configmap=default/vip-configmap
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/kube-keepalived-vip:0.11
      imagePullPolicy: Always
      name: kube-keepalived-vip
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: modules
        readOnly: true
      - mountPath: /dev
        name: dev
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-keepalived-vip-token-2w47w
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-keepalived-vip
    serviceAccountName: kube-keepalived-vip
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: modules
    - hostPath:
        path: /dev
        type: ""
      name: dev
    - name: kube-keepalived-vip-token-2w47w
      secret:
        defaultMode: 420
        secretName: kube-keepalived-vip-token-2w47w
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T19:01:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:56:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T19:01:14Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://00aa555e894c867aef4c94268145997e0d4e0fb8e2bc01194d34c2ee495a4137
      image: k8s.gcr.io/kube-keepalived-vip:0.11
      imageID: docker-pullable://k8s.gcr.io/kube-keepalived-vip@sha256:7b98b73b52fd01c362bd0cabcb59d3fc0c06a49e807fd796367cc395963f7958
      lastState:
        terminated:
          containerID: docker://8fa004a89dc92a38b29575c75273e73f34b1eea92b9d605a1ea13ce6477ba77d
          exitCode: 0
          finishedAt: 2018-02-13T21:55:53Z
          reason: Completed
          startedAt: 2018-02-13T21:55:47Z
      name: kube-keepalived-vip
      ready: true
      restartCount: 3
      state:
        running:
          startedAt: 2018-02-13T21:56:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: BestEffort
    startTime: 2018-02-13T19:01:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
      cni.projectcalico.org/podIP: 192.168.2.19/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-controller-d95d4979d","uid":"a14fe756-1108-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"473413"}}
    creationTimestamp: 2018-02-13T21:55:39Z
    generateName: my-nginx-ingress-controller-d95d4979d-
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: "851805358"
      release: my-nginx-ingress
    name: my-nginx-ingress-controller-d95d4979d-kzgk7
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-nginx-ingress-controller-d95d4979d
      uid: a14fe756-1108-11e8-871b-005056af9e97
    resourceVersion: "473485"
    selfLink: /api/v1/namespaces/default/pods/my-nginx-ingress-controller-d95d4979d-kzgk7
    uid: a1522304-1108-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=default/my-nginx-ingress-default-backend
      - --election-id=ingress-controller-leader
      - --ingress-class=nginx
      - --configmap=default/my-nginx-ingress-controller
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: nginx-ingress-controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: my-nginx-ingress-token-9ffd9
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: my-nginx-ingress
    serviceAccountName: my-nginx-ingress
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: my-nginx-ingress-token-9ffd9
      secret:
        defaultMode: 420
        secretName: my-nginx-ingress-token-9ffd9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:40Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:54Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:39Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b839ab72ad639b16185c8238762de9213cf04afa7c2aa93a80b6a23d87f36faa
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
      imageID: docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:20fb21709d0fa52c5f873ba68d464e04981d0cedf07e900f8a9def6874cf4cee
      lastState: {}
      name: nginx-ingress-controller
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-13T21:55:41Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.19
    qosClass: BestEffort
    startTime: 2018-02-13T21:55:40Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.18/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-nginx-ingress-default-backend-855d89f775","uid":"a151592a-1108-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"473418"}}
    creationTimestamp: 2018-02-13T21:55:39Z
    generateName: my-nginx-ingress-default-backend-855d89f775-
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: "4118459331"
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend-855d89f775-p8jk6
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-nginx-ingress-default-backend-855d89f775
      uid: a151592a-1108-11e8-871b-005056af9e97
    resourceVersion: "473451"
    selfLink: /api/v1/namespaces/default/pods/my-nginx-ingress-default-backend-855d89f775-p8jk6
    uid: a155085d-1108-11e8-871b-005056af9e97
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend:1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: nginx-ingress-default-backend
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:40Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:42Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-13T21:55:39Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://59a4fe54449af40c3801c5057d82fe3e17c6e7ea7e4b5b2020e4bb82fe98ff5c
      image: k8s.gcr.io/defaultbackend:1.3
      imageID: docker-pullable://k8s.gcr.io/defaultbackend@sha256:fb91f9395ddf44df1ca3adf68b25dcbc269e5d08ba14a40de9abdedafacf93d4
      lastState: {}
      name: nginx-ingress-default-backend
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-13T21:55:41Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.18
    qosClass: BestEffort
    startTime: 2018-02-13T21:55:40Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.15/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-96442
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327037"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-96442
    uid: 1666089f-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:16Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1d0b5fdc1a237a0d73e3fb736d0730125d76e145a5926d2c77210aa174e73355
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:15Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.15
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.14/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-fxj6x
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327030"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-fxj6x
    uid: 1664100f-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:15Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://353edef9a7f5eb68513f5a9b463dd7535ebc9abd1a7eec03941101351d84694b
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:14Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.14
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.52/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"default","name":"tea-rc","uid":"16628069-100e-11e8-871b-005056af9e97","apiVersion":"v1","resourceVersion":"326993"}}
    creationTimestamp: 2018-02-12T16:02:12Z
    generateName: tea-rc-
    labels:
      app: tea
    name: tea-rc-v8rxv
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicationController
      name: tea-rc
      uid: 16628069-100e-11e8-871b-005056af9e97
    resourceVersion: "327028"
    selfLink: /api/v1/namespaces/default/pods/tea-rc-v8rxv
    uid: 16662cc9-100e-11e8-871b-005056af9e97
  spec:
    containers:
    - image: nginxdemos/hello
      imagePullPolicy: Always
      name: tea
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-fc6vg
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-fc6vg
      secret:
        defaultMode: 420
        secretName: default-token-fc6vg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:14Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-12T16:02:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8f455e857f54947f253ab4a470b00b34f55086b42dd75e1a40ef0fc9d0a5ba11
      image: nginxdemos/hello:latest
      imageID: docker-pullable://nginxdemos/hello@sha256:f5a0b2a5fe9af497c4a7c186ef6412bb91ff19d39d6ac24a4997eaed2b0bb334
      lastState: {}
      name: tea
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-12T16:02:14Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 192.168.1.52
    qosClass: BestEffort
    startTime: 2018-02-12T16:02:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"526"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-2cpcv
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "890"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-2cpcv
    uid: 09acb8b2-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:48:08Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:26Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b9ba3c46ec16b030166cc9085a61ea34f8d39462ee880c5b088ab51332d36a53
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:24Z
    - containerID: docker://bcd4205803fe0b6f9d0d06b6f7d43e23f0bfa48a56a24efe5a118873f4ed6e55
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"208"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-g99qc
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "787"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-g99qc
    uid: fd057bd7-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:57Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:00Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://80863955dfa346126c501bfde786831975a65e4188f4697348220b10f902b15f
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:58Z
    - containerID: docker://3e78bce5869ac82e5283dd30c852640141f5914d3d2459534e1c94be1ed53470
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:59Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:57Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"f590af99-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"365"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: calico-node-
    labels:
      controller-revision-hash: "3277287842"
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-kw9hm
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: f590af99-0dd1-11e8-871b-005056af9e97
    resourceVersion: "880"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-kw9hm
    uid: 098dcdac-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_IPINIPMTU
        value: "1440"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: CALICO_IPV4POOL_CIDR
        value: 192.168.0.0/16
      - name: CALICO_IPV4POOL_IPIP
        value: Always
      - name: FELIX_IPINIPENABLED
        value: "true"
      - name: FELIX_TYPHAK8SSERVICENAME
        valueFrom:
          configMapKeyRef:
            key: typha_service_name
            name: calico-config
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: IP
      - name: FELIX_HEALTHENABLED
        value: "true"
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-node-token-m5t54
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - name: calico-node-token-m5t54
      secret:
        defaultMode: 420
        secretName: calico-node-token-m5t54
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:48:02Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:26Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://153accc108eab50e06c45a0aae1788230477384976ef825a64b55f6114d85856
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/node@sha256:fc4a0bd66f711970ce0c57433bc415a2bc17817994c83acb3a72e9951e3d3e52
      lastState: {}
      name: calico-node
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    - containerID: docker://185c93be449c0f711ce22e774a8ec43d4fd993cbca35bd4445cb286d9941814f
      image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni:master
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/quay.io/calico/cni@sha256:ce654cf5d5fd937ce75a1fceac3144cda9e5ba4d433a8b3b02e315beb53b1766
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:26Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 10.10.97.46
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-005056af9e97","apiVersion":"apps/v1beta1","resourceVersion":"247"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: elasticsearch-logging-
    labels:
      controller-revision-hash: elasticsearch-logging-7978c6964c
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging-0
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch-logging
      uid: f66f6df9-0dd1-11e8-871b-005056af9e97
    resourceVersion: "810"
    selfLink: /api/v1/namespaces/kube-system/pods/elasticsearch-logging-0
    uid: fd00ccb1-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging
      ports:
      - containerPort: 9200
        name: db
        protocol: TCP
      - containerPort: 9300
        name: transport
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: elasticsearch-logging
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    dnsPolicy: ClusterFirst
    hostname: elasticsearch-logging-0
    initContainers:
    - command:
      - /sbin/sysctl
      - -w
      - vm.max_map_count=262144
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging-init
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: elasticsearch-logging
    serviceAccountName: elasticsearch-logging
    subdomain: elasticsearch-logging
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: elasticsearch-logging
    - name: elasticsearch-logging-token-6cmks
      secret:
        defaultMode: 420
        secretName: elasticsearch-logging-token-6cmks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:45Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:29Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://26e38830a98fcd14580f1349b2e0643db39093b17e7f086e29cce5c07fe75d6a
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      lastState: {}
      name: elasticsearch-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    hostIP: 10.10.97.46
    initContainerStatuses:
    - containerID: docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      lastState: {}
      name: elasticsearch-logging-init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://d6d879131d1a2206e8f765061efdac50e846f49d0a59ac1688815f2927e85c57
          exitCode: 0
          finishedAt: 2018-02-09T19:47:43Z
          reason: Completed
          startedAt: 2018-02-09T19:47:43Z
    phase: Running
    podIP: 192.168.1.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:28Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.5/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"StatefulSet","namespace":"kube-system","name":"elasticsearch-logging","uid":"f66f6df9-0dd1-11e8-871b-005056af9e97","apiVersion":"apps/v1beta1","resourceVersion":"366"}}
    creationTimestamp: 2018-02-09T19:47:45Z
    generateName: elasticsearch-logging-
    labels:
      controller-revision-hash: elasticsearch-logging-7978c6964c
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging-1
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch-logging
      uid: f66f6df9-0dd1-11e8-871b-005056af9e97
    resourceVersion: "841"
    selfLink: /api/v1/namespaces/kube-system/pods/elasticsearch-logging-1
    uid: 1982328b-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging
      ports:
      - containerPort: 9200
        name: db
        protocol: TCP
      - containerPort: 9300
        name: transport
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: elasticsearch-logging
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    dnsPolicy: ClusterFirst
    hostname: elasticsearch-logging-1
    initContainers:
    - command:
      - /sbin/sysctl
      - -w
      - vm.max_map_count=262144
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imagePullPolicy: IfNotPresent
      name: elasticsearch-logging-init
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: elasticsearch-logging-token-6cmks
        readOnly: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: elasticsearch-logging
    serviceAccountName: elasticsearch-logging
    subdomain: elasticsearch-logging
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: elasticsearch-logging
    - name: elasticsearch-logging-token-6cmks
      secret:
        defaultMode: 420
        secretName: elasticsearch-logging-token-6cmks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:46Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:47Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:45Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://598eb6f9ef563829dcef2355b141ea06eb162e11e8d429236c9426580c22620b
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch@sha256:e2386ddc70a6f02770e05967e39f9a9a0b0346875f91aefe7c04c79577f89d2b
      lastState: {}
      name: elasticsearch-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:47Z
    hostIP: 10.10.97.62
    initContainerStatuses:
    - containerID: docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
      image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/alpine@sha256:d6eda1410b93902ac84bdd775167c84ab59e5abadad88791d742fea93b161e93
      lastState: {}
      name: elasticsearch-logging-init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f05f3a497949142b2c6e28f2363eff51ff2e50f024f54e3882a331d831e5b175
          exitCode: 0
          finishedAt: 2018-02-09T19:47:46Z
          reason: Completed
          startedAt: 2018-02-09T19:47:46Z
    phase: Running
    podIP: 192.168.2.5
    qosClass: Burstable
    startTime: 2018-02-09T19:47:44Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: d76e26fba3bf2bfd215eb29011d55250
      kubernetes.io/config.mirror: d76e26fba3bf2bfd215eb29011d55250
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807466634Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:50Z
    labels:
      component: etcd
      tier: control-plane
    name: etcd-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "858"
    selfLink: /api/v1/namespaces/kube-system/pods/etcd-vhosakot1-m51b5b468be
    uid: 1c935fc9-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - etcd
      - --listen-client-urls=http://127.0.0.1:2379
      - --advertise-client-urls=http://127.0.0.1:2379
      - --data-dir=/var/lib/etcd
      image: gcr.io/google_containers/etcd-amd64:3.0.17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2379
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://11eb72aba33cbf28f02623a3e01c996da3542f375262a338c769dc91417a8865
      image: gcr.io/google_containers/etcd-amd64:3.0.17
      imageID: docker-pullable://gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.2/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"532"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: fluentd-es-v2.0.2-
    labels:
      controller-revision-hash: "1193446001"
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
      version: v2.0.2
    name: fluentd-es-v2.0.2-9bmjw
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-es-v2.0.2
      uid: f626704d-0dd1-11e8-871b-005056af9e97
    resourceVersion: "716"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-es-v2.0.2-9bmjw
    uid: 09afd5c6-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: FLUENTD_ARGS
        value: --no-supervisor -q
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imagePullPolicy: IfNotPresent
      name: fluentd-es
      resources:
        limits:
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /host/lib
        name: libsystemddir
        readOnly: true
      - mountPath: /etc/fluent/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-es-token-zw92g
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-es
    serviceAccountName: fluentd-es
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - hostPath:
        path: /usr/lib64
        type: ""
      name: libsystemddir
    - configMap:
        defaultMode: 420
        name: fluentd-es-config-v0.1.1
      name: config-volume
    - name: fluentd-es-token-zw92g
      secret:
        defaultMode: 420
        secretName: fluentd-es-token-zw92g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:36Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:36Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5a66378b3e72ce2fd8df7b7181818302c86b7cb85ce53c1ac81410c171169e70
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      lastState: {}
      name: fluentd-es
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:36Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.2
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.1.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"fluentd-es-v2.0.2","uid":"f626704d-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"386"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: fluentd-es-v2.0.2-
    labels:
      controller-revision-hash: "1193446001"
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "1"
      version: v2.0.2
    name: fluentd-es-v2.0.2-kmbqt
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-es-v2.0.2
      uid: f626704d-0dd1-11e8-871b-005056af9e97
    resourceVersion: "764"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-es-v2.0.2-kmbqt
    uid: 09911ebd-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: FLUENTD_ARGS
        value: --no-supervisor -q
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imagePullPolicy: IfNotPresent
      name: fluentd-es
      resources:
        limits:
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /host/lib
        name: libsystemddir
        readOnly: true
      - mountPath: /etc/fluent/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-es-token-zw92g
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-es
    serviceAccountName: fluentd-es
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - hostPath:
        path: /usr/lib64
        type: ""
      name: libsystemddir
    - configMap:
        defaultMode: 420
        name: fluentd-es-config-v0.1.1
      name: config-volume
    - name: fluentd-es-token-zw92g
      secret:
        defaultMode: 420
        secretName: fluentd-es-token-zw92g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:40Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c650008103084454c1d238468843c301b568d1919e3ae02c60338e1303189954
      image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch:v2.0.2
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/fluentd-elasticsearch@sha256:d6f62f8e05cb18c24806b6c7bd561e27381e55a6e0e31b582602f381bfed2d77
      lastState: {}
      name: fluentd-es
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:39Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 192.168.1.3
    qosClass: Burstable
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kibana-logging-767cf49759","uid":"fcf34643-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"338"}}
    creationTimestamp: 2018-02-09T19:46:58Z
    generateName: kibana-logging-767cf49759-
    labels:
      k8s-app: kibana-logging
      pod-template-hash: "3237905315"
    name: kibana-logging-767cf49759-f8zjt
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kibana-logging-767cf49759
      uid: fcf34643-0dd1-11e8-871b-005056af9e97
    resourceVersion: "803"
    selfLink: /api/v1/namespaces/kube-system/pods/kibana-logging-767cf49759-f8zjt
    uid: fd2570b3-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: ELASTICSEARCH_URL
        value: http://elasticsearch-logging:9200
      - name: XPACK_MONITORING_ENABLED
        value: "false"
      - name: XPACK_SECURITY_ENABLED
        value: "false"
      image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      imagePullPolicy: IfNotPresent
      name: kibana-logging
      ports:
      - containerPort: 5601
        name: ui
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kxl84
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kxl84
      secret:
        defaultMode: 420
        secretName: default-token-kxl84
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:28Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:44Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:29Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://32e30c8b573b980690ec999dbb4f981dec9caadfb001356bd428b26187c1a451
      image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
      imageID: docker-pullable://registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana@sha256:b64d22a2ff6652797ae42b1e695cc65b6cbb339b307f501c6e06b931eb563c68
      lastState: {}
      name: kibana-logging
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:44Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:28Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 3f93d95099662b6d548520f6873d8454
      kubernetes.io/config.mirror: 3f93d95099662b6d548520f6873d8454
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807479364Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:45Z
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "859"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-apiserver-vhosakot1-m51b5b468be
    uid: 199882ac-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-apiserver
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --allow-privileged=true
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --requestheader-username-headers=X-Remote-User
      - --requestheader-group-headers=X-Remote-Group
      - --service-cluster-ip-range=10.96.0.0/12
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --insecure-port=0
      - --requestheader-allowed-names=front-proxy-client
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      - --secure-port=6443
      - --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --advertise-address=10.10.97.20
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --enable-bootstrap-token-auth=true
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --authorization-mode=Node,RBAC
      - --etcd-servers=http://127.0.0.1:2379
      image: gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      resources:
        requests:
          cpu: 250m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a04e800ebb80645cd9826489e7d56e778b23f495984e5d32822831c1178e0915
      image: gcr.io/google_containers/kube-apiserver-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-apiserver-amd64@sha256:f474819f3ebf18a064260e86fdca04f56a744db5c0d29741bc1bc461b6d5f223
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: ab7c88cd37d9d92c34bb35c6a377a96f
      kubernetes.io/config.mirror: ab7c88cd37d9d92c34bb35c6a377a96f
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807482481Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:44Z
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "861"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-controller-manager-vhosakot1-m51b5b468be
    uid: 18ff898f-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --address=127.0.0.1
      - --controllers=*,bootstrapsigner,tokencleaner
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --leader-elect=true
      - --use-service-account-credentials=true
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --allocate-node-cidrs=true
      - --cluster-cidr=192.168.0.0/16
      - --node-cidr-mask-size=24
      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10252
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://952f4f63dea04d6c2c7ed47c7c6d4a720b7eea4f69f43d3daaf0113ab685163c
      image: gcr.io/google_containers/kube-controller-manager-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-controller-manager-amd64@sha256:8adbcd2de1b1ef752ce92c0602f99aa4bd86798c7b546e56c398e18f9f60c26b
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.4/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kube-dns-545bc4bfd4","uid":"fcf396ae-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"332"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kube-dns-545bc4bfd4-
    labels:
      k8s-app: kube-dns
      pod-template-hash: "1016706980"
    name: kube-dns-545bc4bfd4-rzpz4
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-545bc4bfd4
      uid: fcf396ae-0dd1-11e8-871b-005056af9e97
    resourceVersion: "851"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-rzpz4
    uid: fd048a95-0dd1-11e8-871b-005056af9e97
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: beta.kubernetes.io/arch
              operator: In
              values:
              - amd64
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
      image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-66tfx
        readOnly: true
    dnsPolicy: Default
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-66tfx
      secret:
        defaultMode: 420
        secretName: kube-dns-token-66tfx
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:13Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:50Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:13Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ead0625a9610ac4f66bee25ee8ed4d6b5418bc31d7ac8b7e6eb3d38b339b3e7e
      image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:46b933bb70270c8a02fa6b6f87d440f6f1fce1a5a2a719e164f83f7b109f7544
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    - containerID: docker://585a3048cfcdc2498ba543ef02124e15c7c7e8ea79f84f5df37d87e46b6cfb19
      image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:1a3fc069de481ae690188f6f1ba4664b5cc7760af37120f70c86505c79eea61d
      lastState: {}
      name: kubedns
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:45Z
    - containerID: docker://313b3d5c54c63059e63c3bc2b8d23d2902e8033a007ebd78f015ae22556f9603
      image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
      imageID: docker-pullable://gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:9aab42bf6a2a068b797fe7d91a5d8d915b10dbbc3d6f2b10492848debfba6044
      lastState: {}
      name: sidecar
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:46Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 192.168.0.4
    qosClass: Burstable
    startTime: 2018-02-09T19:47:13Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"189"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-8vlsv
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "440"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-8vlsv
    uid: fd05a03b-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:57Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:59Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:59Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bf6367debcebe328805c54a362b8767b8e8810c327094c53b98ea7a59c4e5514
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:58Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: BestEffort
    startTime: 2018-02-09T19:46:57Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"432"}}
    creationTimestamp: 2018-02-09T19:47:18Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-nfkkf
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "643"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-nfkkf
    uid: 098dc202-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-we2d86faeb2
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d47a6b44a292ed53cc15d6e72546c50f08a3037a306a847e63f39660af3b8a4a
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.46
    phase: Running
    podIP: 10.10.97.46
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"kube-proxy","uid":"f4eaff3c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"524"}}
    creationTimestamp: 2018-02-09T19:47:19Z
    generateName: kube-proxy-
    labels:
      controller-revision-hash: "514127771"
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-q8ng8
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: f4eaff3c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "642"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-q8ng8
    uid: 09ac7c1f-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf
      - --cluster-cidr=192.168.0.0/16
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-ktcv9
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-ktcv9
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-ktcv9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:25Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3ac1e13578a22c5fe5e2f3c8209af657906680224d8faf34cf95880e1a320243
      image: gcr.io/google_containers/kube-proxy-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-proxy-amd64@sha256:8dce98802846d5219093f0313dcb8697a8a5d7cad647c3b71a816cf3a2b2eb2a
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:25Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 10.10.97.62
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:23Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: e554495c6f8701f21accd04866090b05
      kubernetes.io/config.mirror: e554495c6f8701f21accd04866090b05
      kubernetes.io/config.seen: 2018-02-09T19:46:22.807485209Z
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: 2018-02-09T19:47:43Z
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-vhosakot1-m51b5b468be
    namespace: kube-system
    resourceVersion: "860"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-scheduler-vhosakot1-m51b5b468be
    uid: 1866988a-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - command:
      - kube-scheduler
      - --address=127.0.0.1
      - --leader-elect=true
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      image: gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:29Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:46:27Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dc6d33ae2ff92d6b18190f6edd6d943f352b37dabb80d6785d1e40238ec1530d
      image: gcr.io/google_containers/kube-scheduler-amd64:v1.8.4
      imageID: docker-pullable://gcr.io/google_containers/kube-scheduler-amd64@sha256:72608b511275a1661b66f113cff09a0737b4d8e1405ad0ddb2e98c9cad0a8323
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:46:28Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 10.10.97.20
    qosClass: Burstable
    startTime: 2018-02-09T19:46:27Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.0.3/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kubernetes-dashboard-7798c48646","uid":"fcf3720c-0dd1-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"339"}}
    creationTimestamp: 2018-02-09T19:46:57Z
    generateName: kubernetes-dashboard-7798c48646-
    labels:
      k8s-app: kubernetes-dashboard
      pod-template-hash: "3354704202"
    name: kubernetes-dashboard-7798c48646-rjmch
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubernetes-dashboard-7798c48646
      uid: fcf3720c-0dd1-11e8-871b-005056af9e97
    resourceVersion: "771"
    selfLink: /api/v1/namespaces/kube-system/pods/kubernetes-dashboard-7798c48646-rjmch
    uid: fcff0932-0dd1-11e8-871b-005056af9e97
  spec:
    containers:
    - args:
      - --auto-generate-certificates
      image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8443
          scheme: HTTPS
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: kubernetes-dashboard
      ports:
      - containerPort: 8443
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: kubernetes-dashboard-certs
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kubernetes-dashboard-token-bjcwc
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-m51b5b468be
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubernetes-dashboard
    serviceAccountName: kubernetes-dashboard
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kubernetes-dashboard-certs
      secret:
        defaultMode: 420
        secretName: kubernetes-dashboard-certs
    - emptyDir: {}
      name: tmp-volume
    - name: kubernetes-dashboard-token-bjcwc
      secret:
        defaultMode: 420
        secretName: kubernetes-dashboard-token-bjcwc
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:12Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:42Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:47:12Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://15d41739dac144360b50543a9b2ed5269720107fc11c5e19c2c9a61716c4b626
      image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
      imageID: docker-pullable://k8s.gcr.io/kubernetes-dashboard-amd64@sha256:3861695e962972965a4c611bcabc2032f885d8cbdb0bccc9bf513ef16335fe33
      lastState: {}
      name: kubernetes-dashboard
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:47:42Z
    hostIP: 10.10.97.20
    phase: Running
    podIP: 192.168.0.3
    qosClass: BestEffort
    startTime: 2018-02-09T19:47:12Z
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 192.168.2.6/32
      kubernetes.io/created-by: |
        {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"tiller-deploy-546cf9696c","uid":"54347a31-0dd2-11e8-871b-005056af9e97","apiVersion":"extensions","resourceVersion":"1022"}}
    creationTimestamp: 2018-02-09T19:49:24Z
    generateName: tiller-deploy-546cf9696c-
    labels:
      app: helm
      name: tiller
      pod-template-hash: "1027952527"
    name: tiller-deploy-546cf9696c-w4kq6
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tiller-deploy-546cf9696c
      uid: 54347a31-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1066"
    selfLink: /api/v1/namespaces/kube-system/pods/tiller-deploy-546cf9696c-w4kq6
    uid: 5435726c-0dd2-11e8-871b-005056af9e97
  spec:
    containers:
    - env:
      - name: TILLER_NAMESPACE
        value: kube-system
      - name: TILLER_HISTORY_MAX
        value: "0"
      image: gcr.io/kubernetes-helm/tiller:v2.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /liveness
          port: 44135
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: tiller
      ports:
      - containerPort: 44134
        name: tiller
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 44135
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: tiller-token-chfrl
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: vhosakot1-wc80d3e5ab6
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: tiller
    serviceAccountName: tiller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.alpha.kubernetes.io/notReady
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.alpha.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tiller-token-chfrl
      secret:
        defaultMode: 420
        secretName: tiller-token-chfrl
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:23Z
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:31Z
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: 2018-02-09T19:49:24Z
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9aa05a99a4fe19195842dbbe5f406912fdd380339d496b401a690d15d6033379
      image: gcr.io/kubernetes-helm/tiller:v2.7.2
      imageID: docker-pullable://gcr.io/kubernetes-helm/tiller@sha256:df7f227fa722afc4931c912c1cad2c47856ec94f4d052ccceebcb16dd483dad8
      lastState: {}
      name: tiller
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: 2018-02-09T19:49:27Z
    hostIP: 10.10.97.62
    phase: Running
    podIP: 192.168.2.6
    qosClass: BestEffort
    startTime: 2018-02-09T19:49:23Z
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get podsecuritypolicies --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get podtemplates --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get replicasets --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: "851805358"
      release: my-nginx-ingress
    name: my-nginx-ingress-controller-d95d4979d
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-controller
      uid: a14f1a1b-1108-11e8-871b-005056af9e97
    resourceVersion: "473487"
    selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/my-nginx-ingress-controller-d95d4979d
    uid: a14fe756-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: "851805358"
        release: my-nginx-ingress
    template:
      metadata:
        annotations:
          checksum/config: 98924996605c53736b7e76024d687d1c245f3a29a59a52c589c028a599153f8c
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: "851805358"
          release: my-nginx-ingress
      spec:
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=default/my-nginx-ingress-default-backend
          - --election-id=ingress-controller-leader
          - --ingress-class=nginx
          - --configmap=default/my-nginx-ingress-controller
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: my-nginx-ingress
        serviceAccountName: my-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-13T21:55:39Z
    generation: 1
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: "4118459331"
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend-855d89f775
    namespace: default
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-nginx-ingress-default-backend
      uid: a14f9e60-1108-11e8-871b-005056af9e97
    resourceVersion: "473452"
    selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/my-nginx-ingress-default-backend-855d89f775
    uid: a151592a-1108-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        pod-template-hash: "4118459331"
        release: my-nginx-ingress
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          pod-template-hash: "4118459331"
          release: my-nginx-ingress
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: calico-typha
      pod-template-hash: "2163023612"
    name: calico-typha-65b7467b56
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: f58c96a5-0dd1-11e8-871b-005056af9e97
    resourceVersion: "340"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/calico-typha-65b7467b56
    uid: fcf331c4-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: "2163023612"
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
          pod-template-hash: "2163023612"
      spec:
        containers:
        - env:
          - name: TYPHA_LOGSEVERITYSCREEN
            value: info
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: registry.ci.dfj.io/cpsg_ccp/quay.io/calico/typha:master
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kibana-logging
      pod-template-hash: "3237905315"
    name: kibana-logging-767cf49759
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kibana-logging
      uid: f6d3b8f6-0dd1-11e8-871b-005056af9e97
    resourceVersion: "806"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kibana-logging-767cf49759
    uid: fcf34643-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kibana-logging
        pod-template-hash: "3237905315"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kibana-logging
          pod-template-hash: "3237905315"
      spec:
        containers:
        - env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: XPACK_MONITORING_ENABLED
            value: "false"
          - name: XPACK_SECURITY_ENABLED
            value: "false"
          image: registry.ci.dfj.io/cpsg_ccp/docker.elastic.co/kibana/kibana:5.6.4
          imagePullPolicy: IfNotPresent
          name: kibana-logging
          ports:
          - containerPort: 5601
            name: ui
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: "1016706980"
    name: kube-dns-545bc4bfd4
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: f4e5f6d2-0dd1-11e8-871b-005056af9e97
    resourceVersion: "852"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kube-dns-545bc4bfd4
    uid: fcf396ae-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: "1016706980"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: "1016706980"
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/arch
                  operator: In
                  values:
                  - amd64
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
          image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:46:57Z
    generation: 1
    labels:
      k8s-app: kubernetes-dashboard
      pod-template-hash: "3354704202"
    name: kubernetes-dashboard-7798c48646
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubernetes-dashboard
      uid: f79d83b7-0dd1-11e8-871b-005056af9e97
    resourceVersion: "773"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kubernetes-dashboard-7798c48646
    uid: fcf3720c-0dd1-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: "3354704202"
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubernetes-dashboard
          pod-template-hash: "3354704202"
      spec:
        containers:
        - args:
          - --auto-generate-certificates
          image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: kubernetes-dashboard
          ports:
          - containerPort: 8443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: kubernetes-dashboard-certs
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubernetes-dashboard
        serviceAccountName: kubernetes-dashboard
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            defaultMode: 420
            secretName: kubernetes-dashboard-certs
        - emptyDir: {}
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: 2018-02-09T19:49:24Z
    generation: 1
    labels:
      app: helm
      name: tiller
      pod-template-hash: "1027952527"
    name: tiller-deploy-546cf9696c
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1068"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/tiller-deploy-546cf9696c
    uid: 54347a31-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "1027952527"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "1027952527"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: tiller
        serviceAccountName: tiller
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: 2018-02-09T19:49:06Z
    generation: 2
    labels:
      app: helm
      name: tiller
      pod-template-hash: "165821739"
    name: tiller-deploy-5b9d65c7f
    namespace: kube-system
    ownerReferences:
    - apiVersion: extensions/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: tiller-deploy
      uid: 4998e55c-0dd2-11e8-871b-005056af9e97
    resourceVersion: "1037"
    selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/tiller-deploy-5b9d65c7f
    uid: 499996c4-0dd2-11e8-871b-005056af9e97
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: helm
        name: tiller
        pod-template-hash: "165821739"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
          pod-template-hash: "165821739"
      spec:
        containers:
        - env:
          - name: TILLER_NAMESPACE
            value: kube-system
          - name: TILLER_HISTORY_MAX
            value: "0"
          image: gcr.io/kubernetes-helm/tiller:v2.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: tiller
          ports:
          - containerPort: 44134
            name: tiller
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 44135
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get replicationcontrollers --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: ReplicationController
  metadata:
    creationTimestamp: 2018-02-12T16:02:12Z
    generation: 1
    labels:
      app: tea
    name: tea-rc
    namespace: default
    resourceVersion: "327038"
    selfLink: /api/v1/namespaces/default/replicationcontrollers/tea-rc
    uid: 16628069-100e-11e8-871b-005056af9e97
  spec:
    replicas: 3
    selector:
      app: tea
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: tea
      spec:
        containers:
        - image: nginxdemos/hello
          imagePullPolicy: Always
          name: tea
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get resourcequotas --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get rolebindings --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress
    namespace: default
    resourceVersion: "473405"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/rolebindings/my-nginx-ingress
    uid: a148f9e9-1108-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: my-nginx-ingress
  subjects:
  - kind: ServiceAccount
    name: my-nginx-ingress
    namespace: default
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:bootstrap-signer-clusterinfo
    namespace: kube-public
    resourceVersion: "176"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/rolebindings/kubeadm%3Abootstrap-signer-clusterinfo
    uid: f4d53f8c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: kubeadm:bootstrap-signer-clusterinfo
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:anonymous
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:43Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:bootstrap-signer
    namespace: kube-public
    resourceVersion: "138"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/rolebindings/system%3Acontroller%3Abootstrap-signer
    uid: f43bb0f6-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system:controller:bootstrap-signer
  subjects:
  - kind: ServiceAccount
    name: bootstrap-signer
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"RoleBinding","metadata":{"annotations":{},"name":"kubernetes-dashboard-minimal","namespace":"kube-system"},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"Role","name":"kubernetes-dashboard-minimal"},"subjects":[{"kind":"ServiceAccount","name":"kubernetes-dashboard","namespace":"kube-system"}]}
    creationTimestamp: 2018-02-09T19:46:48Z
    name: kubernetes-dashboard-minimal
    namespace: kube-system
    resourceVersion: "270"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/kubernetes-dashboard-minimal
    uid: f799b5b2-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: kubernetes-dashboard-minimal
  subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system::leader-locking-kube-controller-manager
    namespace: kube-system
    resourceVersion: "132"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system%3A%3Aleader-locking-kube-controller-manager
    uid: f41d299a-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system::leader-locking-kube-controller-manager
  subjects:
  - kind: ServiceAccount
    name: kube-controller-manager
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:43Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system::leader-locking-kube-scheduler
    namespace: kube-system
    resourceVersion: "133"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system%3A%3Aleader-locking-kube-scheduler
    uid: f4234a6e-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system::leader-locking-kube-scheduler
  subjects:
  - kind: ServiceAccount
    name: kube-scheduler
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:43Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:bootstrap-signer
    namespace: kube-system
    resourceVersion: "134"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system%3Acontroller%3Abootstrap-signer
    uid: f429684b-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system:controller:bootstrap-signer
  subjects:
  - kind: ServiceAccount
    name: bootstrap-signer
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:43Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:cloud-provider
    namespace: kube-system
    resourceVersion: "136"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system%3Acontroller%3Acloud-provider
    uid: f42f7a5c-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system:controller:cloud-provider
  subjects:
  - kind: ServiceAccount
    name: cloud-provider
    namespace: kube-system
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:43Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:token-cleaner
    namespace: kube-system
    resourceVersion: "137"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system%3Acontroller%3Atoken-cleaner
    uid: f43596b8-0dd1-11e8-871b-005056af9e97
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: system:controller:token-cleaner
  subjects:
  - kind: ServiceAccount
    name: token-cleaner
    namespace: kube-system
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get roles --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress
    namespace: default
    resourceVersion: "473404"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/roles/my-nginx-ingress
    uid: a1489201-1108-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    - namespaces
    - pods
    - secrets
    verbs:
    - get
  - apiGroups:
    - ""
    resourceNames:
    - ingress-controller-leader-nginx
    resources:
    - configmaps
    verbs:
    - get
    - update
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - create
    - get
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kubeadm:bootstrap-signer-clusterinfo
    namespace: kube-public
    resourceVersion: "174"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/roles/kubeadm%3Abootstrap-signer-clusterinfo
    uid: f4d44ea8-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resourceNames:
    - cluster-info
    resources:
    - configmaps
    verbs:
    - get
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:bootstrap-signer
    namespace: kube-public
    resourceVersion: "129"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/roles/system%3Acontroller%3Abootstrap-signer
    uid: f4170a58-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resourceNames:
    - cluster-info
    resources:
    - configmaps
    verbs:
    - update
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: extension-apiserver-authentication-reader
    namespace: kube-system
    resourceVersion: "122"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/extension-apiserver-authentication-reader
    uid: f3f26e97-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resourceNames:
    - extension-apiserver-authentication
    resources:
    - configmaps
    verbs:
    - get
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"Role","metadata":{"annotations":{},"name":"kubernetes-dashboard-minimal","namespace":"kube-system"},"rules":[{"apiGroups":[""],"resources":["secrets"],"verbs":["create"]},{"apiGroups":[""],"resources":["configmaps"],"verbs":["create"]},{"apiGroups":[""],"resourceNames":["kubernetes-dashboard-key-holder","kubernetes-dashboard-certs"],"resources":["secrets"],"verbs":["get","update","delete"]},{"apiGroups":[""],"resourceNames":["kubernetes-dashboard-settings"],"resources":["configmaps"],"verbs":["get","update"]},{"apiGroups":[""],"resourceNames":["heapster"],"resources":["services"],"verbs":["proxy"]},{"apiGroups":[""],"resourceNames":["heapster","http:heapster:","https:heapster:"],"resources":["services/proxy"],"verbs":["get"]}]}
    creationTimestamp: 2018-02-09T19:46:48Z
    name: kubernetes-dashboard-minimal
    namespace: kube-system
    resourceVersion: "269"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/kubernetes-dashboard-minimal
    uid: f795cd09-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - create
  - apiGroups:
    - ""
    resourceNames:
    - kubernetes-dashboard-key-holder
    - kubernetes-dashboard-certs
    resources:
    - secrets
    verbs:
    - get
    - update
    - delete
  - apiGroups:
    - ""
    resourceNames:
    - kubernetes-dashboard-settings
    resources:
    - configmaps
    verbs:
    - get
    - update
  - apiGroups:
    - ""
    resourceNames:
    - heapster
    resources:
    - services
    verbs:
    - proxy
  - apiGroups:
    - ""
    resourceNames:
    - heapster
    - 'http:heapster:'
    - 'https:heapster:'
    resources:
    - services/proxy
    verbs:
    - get
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system::leader-locking-kube-controller-manager
    namespace: kube-system
    resourceVersion: "126"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system%3A%3Aleader-locking-kube-controller-manager
    uid: f40ee194-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - watch
  - apiGroups:
    - ""
    resourceNames:
    - kube-controller-manager
    resources:
    - configmaps
    verbs:
    - get
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system::leader-locking-kube-scheduler
    namespace: kube-system
    resourceVersion: "128"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system%3A%3Aleader-locking-kube-scheduler
    uid: f410f2a8-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - watch
  - apiGroups:
    - ""
    resourceNames:
    - kube-scheduler
    resources:
    - configmaps
    verbs:
    - get
    - update
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:bootstrap-signer
    namespace: kube-system
    resourceVersion: "123"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system%3Acontroller%3Abootstrap-signer
    uid: f3f88ade-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:cloud-provider
    namespace: kube-system
    resourceVersion: "124"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system%3Acontroller%3Acloud-provider
    uid: f3fea1f0-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - configmaps
    verbs:
    - create
    - get
    - list
    - watch
- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    creationTimestamp: 2018-02-09T19:46:42Z
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:controller:token-cleaner
    namespace: kube-system
    resourceVersion: "125"
    selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system%3Acontroller%3Atoken-cleaner
    uid: f404c062-0dd1-11e8-871b-005056af9e97
  rules:
  - apiGroups:
    - ""
    resources:
    - secrets
    verbs:
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get secrets --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  data:
    tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ4RENDQXRpZ0F3SUJBZ0lKQU9jbHdCelprYmlhTUEwR0NTcUdTSWIzRFFFQkJRVUFNRmd4Q3pBSkJnTlYKQkFZVEFsVlRNUXN3Q1FZRFZRUUlFd0pEUVRFaE1COEdBMVVFQ2hNWVNXNTBaWEp1WlhRZ1YybGtaMmwwY3lCUQpkSGtnVEhSa01Sa3dGd1lEVlFRREV4QmpZV1psTG1WNFlXMXdiR1V1WTI5dE1CNFhEVEUzTURnek1URXdNVGN5Ck1Gb1hEVEU0TURnek1URXdNVGN5TUZvd1dERUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdUQWtOQk1TRXcKSHdZRFZRUUtFeGhKYm5SbGNtNWxkQ0JYYVdSbmFYUnpJRkIwZVNCTWRHUXhHVEFYQmdOVkJBTVRFR05oWm1VdQpaWGhoYlhCc1pTNWpiMjB3Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLQW9JQkFRQzdIT0xJCm5oZjE1aUcxOE16RXBzN0lvZmxHQmovMi9NVjA0OWtBS0hrTnZOem1XaXRXWDV2QU1yRkF5THY0dXBIWDI5b0IKa3l6YUhlYyt2TlFibEh1bStINUtkUWZXWHFXNkJ6UnVhMzBreEkrcG91cnhpNy9jaDJORS92djBhRGtvaTJ0RAovOUI2aHAyVkoxWXFJdm9hQ2wwSmFYWDd0WEc4SGU1S1BSZzBYMm1Mblcwa29tay9ZVGRPbS9xOVRjUDRnUmhrCms3bUhJMDlSME5vNUhTbURydmVBWFEyY3lGWVJQVUNjWkNPd0h6UUdVUVB1UU0wNVArWUNnVlNRcElKWFV0b0kKbGdEMHhZZUw4UU1rZjZ0TWpYcXpTVVRhQlhzNkRKU2x1YWN1aHpkV212UnFPUVNNYlVpZ3dVUEZCRDVLRUFIcwozM0hHeVZ5dkI4cVlrYUczQWdNQkFBR2pnYnd3Z2Jrd0hRWURWUjBPQkJZRUZOdTQvMTdpSituRGxPMkoyVisvCitqM2x5SzVZTUlHSkJnTlZIU01FZ1lFd2Y0QVUyN2ovWHVJbjZjT1U3WW5aWDcvNlBlWElybGloWEtSYU1GZ3gKQ3pBSkJnTlZCQVlUQWxWVE1Rc3dDUVlEVlFRSUV3SkRRVEVoTUI4R0ExVUVDaE1ZU1c1MFpYSnVaWFFnVjJsawpaMmwwY3lCUWRIa2dUSFJrTVJrd0Z3WURWUVFERXhCallXWmxMbVY0WVcxd2JHVXVZMjl0Z2drQTV5WEFITm1SCnVKb3dEQVlEVlIwVEJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQkFRVUZBQU9DQVFFQUtGUHJBcXA3a3lzTDVGNnMKWFhWdXZkZzAyc0srUlpzb2F3QWVxbHlSRmpJeUlQL2VTajBhQjQwQmNOcWFyRHhwNjhBd1pZNG4yQk9EVmo5WgphOFlvV1YyOFpwamloaThxNnBPSElOa0MrOXpCY1hsZ2lvVUZBTERCcXFPTXFUZkw1cjNGejNUTGN1clozajhuCnUzL2hRVHNXZG5TZENWbmN0aXhaUHJ5cnhJSFlWSERiVHF4ZWdTQUN6WkU1MHMwdlRpMFJkNUkrcVdubVpIUloKL0hLNVZnNWlNS2E1clBPRTFaT2M3L2VnVjZ6R2p4THJiNEdlQ2JyTjBBb01tazNpL2d2K2kzL0N6aTlXOVhNNApwa2hQSjJUcEtMSjVaOHgxUVhjUW5Dem5yOEdtL2FuVzV4b3lDdWhjZzlXMlVSYzRKVTZ1UXh0WU9tczYrc0RxCjBBN1Y3UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    tls.key: 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
  kind: Secret
  metadata:
    creationTimestamp: 2018-02-12T16:02:25Z
    name: cafe-secret
    namespace: default
    resourceVersion: "327054"
    selfLink: /api/v1/namespaces/default/secrets/cafe-secret
    uid: 1dd49bdf-100e-11e8-871b-005056af9e97
  type: Opaque
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: ZGVmYXVsdA==
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dFptTTJkbWNpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbVprTURJd1ptUmhMVEJrWkRFdE1URmxPQzA0TnpGaUxUQXdOVEExTm1GbU9XVTVOeUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEueFpNaFZtM0ZJVTdPVXFwLWQ0MkVqc05QaWUyM1RBWVY5SXhPRnF1OVQ2YmRTbDZtSURhVjYydFg2V0o0Q3dwM2gwZ3Z3bE1ydGJOMENkT2NNTWN4RTZUVGdQX3BNenZadDBUc0pYVkpxUzFXNEVTSUxBYjhkd3Nsb3h5S3VSYWo3U0ZnWU80RlkyUjF3NmdpLUJxb2pPYjJtc3BGdFZlX2N6Y3lJMVRtMlVIajVsREhod0hvaHo3ZG1Ia0RQQ2MzbHdDRk1GbEVIZExtWExOTXNTZmR3cXFqTnJ5SUJWWGpQWlEweUItaFktSWRuTHRsS09pMkkxVC16ci1tMEYxNDd5N1JBbWZ3MnBWNzZGaXZBblJTWHZwNThwa3JMbGQ5Qm5MNEs5eXBmb2t3ODJ5QWRORUtFbkxvaGY5WTR0NXZfNm1Ra3FERHJsejYxQkotbE16YVdn
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: default
      kubernetes.io/service-account.uid: fd020fda-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:57Z
    name: default-token-fc6vg
    namespace: default
    resourceVersion: "369"
    selfLink: /api/v1/namespaces/default/secrets/default-token-fc6vg
    uid: fd11fd87-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: ZGVmYXVsdA==
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbXQxWW1VdGEyVmxjR0ZzYVhabFpDMTJhWEF0ZEc5clpXNHRNbmMwTjNjaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaWEzVmlaUzFyWldWd1lXeHBkbVZrTFhacGNDSXNJbXQxWW1WeWJtVjBaWE11YVc4dmMyVnlkbWxqWldGalkyOTFiblF2YzJWeWRtbGpaUzFoWTJOdmRXNTBMblZwWkNJNklqTmxNVEkzT1Roa0xURXdaREl0TVRGbE9DMDROekZpTFRBd05UQTFObUZtT1dVNU55SXNJbk4xWWlJNkluTjVjM1JsYlRwelpYSjJhV05sWVdOamIzVnVkRHBrWldaaGRXeDBPbXQxWW1VdGEyVmxjR0ZzYVhabFpDMTJhWEFpZlEuYTVJTklkRjhuTVJaRld5SUxQVk9lc2RQcEJzYVNGSUFBaEcyeERVa25xSmJjZXpHblpybG1UV1JZcHZQWklKYmxsZkJXV3h0QWxXamlCZHJFV3hPVHZDTnFFS1haci1SUll5amdWWTZCZnpPRHpVcnc4QUVhaWxpUkdCNW54ZkR1NUZ1Nk5rSkEycXh5NnNfTFJIb1R1WXY2cHZxQW1icTlkRUFlV0JLcThGVEJmam4xWW9UbmhwdEcySkt2b0ZodGFVTG9LR21ZVHVCT2s4bVRNX01KRjc2dEd4SXdKM2dhUTBOdEJvM1JqVWhkV3k0YlpTMldOWTNOUWR0cjVIbVhpX3F1RVRWN1pQNHVqUmN6NUNIeW9WdHUwX1RYSGRlMXdnR2d1RDhUc3d3WDVpdlBGbWt0MUZHdHk1a0ZDYVZObDdnbXRrUWNLZmxBc1d0Y1FZRk1n
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: kube-keepalived-vip
      kubernetes.io/service-account.uid: 3e12798d-10d2-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-13T15:26:20Z
    name: kube-keepalived-vip-token-2w47w
    namespace: default
    resourceVersion: "438738"
    selfLink: /api/v1/namespaces/default/secrets/kube-keepalived-vip-token-2w47w
    uid: 3e145be8-10d2-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: ZGVmYXVsdA==
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbTE1TFc1bmFXNTRMV2x1WjNKbGMzTXRkRzlyWlc0dE9XWm1aRGtpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pYlhrdGJtZHBibmd0YVc1bmNtVnpjeUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbUV4TkRSbFpqVmlMVEV4TURndE1URmxPQzA0TnpGaUxUQXdOVEExTm1GbU9XVTVOeUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT20xNUxXNW5hVzU0TFdsdVozSmxjM01pZlEuSDdiOWtpVlFoVzdsSXU5UTVwbmI1azFEWmpFazN5d1FTUThoRnA3QXppUF82N2VYQ08wMVZWNVVINGQ5UTE4dS1PRWNZMWZ1SzEyaV9xQXhWcEJCMFBuaV81SEkwald2Z2M5cmlHaHNxRjg4cWJRX2hHX1JoODVKQlpSR252WDBsd1pEOEhTb3Ayb1JQbnRzRU4xRkdKOVRkR1lNNXhPWU16cm85TmFBcGNHZEpncTJrVi1RbVYxdHZzY0FMb2pzc0JzT2ZiaHg3TE14cWJuVFJQeEZvU09aS3FTdlZQU2RtM0oxZWM4a2JHXzU1Z2k3Tng4WUh4Z25ObW91OXRYRlBld2RvX2RIaWlubjhETGViTnVqa1NabHBBVzhRN2sxcVgzQ2V0b1FBUkh0RTE2bXh0Y0dJcXBKN3RxYzFRcHFxZEI1QV9CM0dvcTctUlRyYjNHRTZB
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: my-nginx-ingress
      kubernetes.io/service-account.uid: a144ef5b-1108-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-13T21:55:39Z
    name: my-nginx-ingress-token-9ffd9
    namespace: default
    resourceVersion: "473400"
    selfLink: /api/v1/namespaces/default/secrets/my-nginx-ingress-token-9ffd9
    uid: a1462235-1108-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1wdWJsaWM=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYQjFZbXhwWXlJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1pXWmhkV3gwTFhSdmEyVnVMVGRuTkRReUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltUmxabUYxYkhRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbVkyWmpNbU0xWWkwd1pHUXhMVEV4WlRndE9EY3hZaTB3TURVd05UWmhaamxsT1RjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxd2RXSnNhV002WkdWbVlYVnNkQ0o5LnUza2dNd0xPT0J2Wm91ZWM0YjBka1dfbWJYX051UHNobVNFQ0I3bWRNdmhkRUYzTFB4VGFrQTM0Y0EwU3VvM2lsSW1zTExibF9FMUNZdm5sVDN0SnRmc2NBeHVwV1lDYXlUenFvYmpWX3RLUDBPX3V6UF9hT1dKUXdKQmt0T0phSWthUjZOUVIyYjV1M2F4SlEtUWIzMDJYMlpsM3ptZGgyUHJObUhTVk9Ga0RldG1rSUt3Q1g3eWhZMEpJY214NlpJdU5XT01YaVVJcElmaEs3bW1zakRiWkZNY3d3YkZEd0FnaVQya3BWS3RiX3RuemVhVERxUHozUkZuQW5BdzFtc0lEbTIydjBwV0puLV9wZVdOLW0yazlOQ0FLR1pPSWNWS2hpNUw2bzViRmZCZlotVFAtbXhHLXpwaExocUFieDI5a3U3dFNPeENEblFvZE1HNzdKQQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: default
      kubernetes.io/service-account.uid: fcfc2c5b-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:57Z
    name: default-token-7g442
    namespace: kube-public
    resourceVersion: "350"
    selfLink: /api/v1/namespaces/kube-public/secrets/default-token-7g442
    uid: fd01e135-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKaGRIUmhZMmhrWlhSaFkyZ3RZMjl1ZEhKdmJHeGxjaTEwYjJ0bGJpMXdaRzEwTkNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExtNWhiV1VpT2lKaGRIUmhZMmhrWlhSaFkyZ3RZMjl1ZEhKdmJHeGxjaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbVppWkRrME5qVmpMVEJrWkRFdE1URmxPQzA0TnpGaUxUQXdOVEExTm1GbU9XVTVOeUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwcmRXSmxMWE41YzNSbGJUcGhkSFJoWTJoa1pYUmhZMmd0WTI5dWRISnZiR3hsY2lKOS5kVnd5b1U3OURNd0FZMHNNYlZrU3JWNHpQUGJSdVFJWE5kZ0toNFRFTndWYVIxdXQwQmJLVEh1Z3J0cVNibnFHQVZnVlBPaEJodFpZVmtBS0l3bl9aUTY4WTgzZ1VrSDdMWGl0cHJseE1SOGo4aWN2TDFIUk9BSERVSzZ2TVQwRG8wRHBKWER2SGtoSE9LUFYzRTRsUzB1S0hWUWRMOHFUNmxVRG8tMk1WUFNoLUEtNVNpSHVyMmdlVkU0Z3d3aktJMkQ0bTQwbDVPSGtGME9xNGlyNWNvd0VTc3dEQUxFUmNsQmxubzRSRXIwYm9kWWtWZXFqS1c1d2ZlamZxMHU2QVB2S2xfQ0JZVGtiUkZJUVpFUnJzMG5jajdwZnRhdDZjT3JxQzlvQXFXTjIzNG9XRDlEUEFoeUJaYi05Ump1UFZoa0xpVWctQm5nUXR4ZlctNXVCWHc=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: attachdetach-controller
      kubernetes.io/service-account.uid: fbd9465c-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:55Z
    name: attachdetach-controller-token-pdmt4
    namespace: kube-system
    resourceVersion: "316"
    selfLink: /api/v1/namespaces/kube-system/secrets/attachdetach-controller-token-pdmt4
    uid: fbda88d9-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKaWIyOTBjM1J5WVhBdGMybG5ibVZ5TFhSdmEyVnVMVzVvYzJoa0lpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltSnZiM1J6ZEhKaGNDMXphV2R1WlhJaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbU5EazFaV00yTVMwd1pHUXhMVEV4WlRndE9EY3hZaTB3TURVd05UWmhaamxsT1RjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2WW05dmRITjBjbUZ3TFhOcFoyNWxjaUo5Lm5IcEdCeENSaUFpMF9QUTVSWHFhWjIyb1kwTHpSWjJhSllQWmxOaUpzMkdrc1R1OU11NDF5ckFERTFfOE01cUd0YlJnMHh3U3dzLXhETUR6ZnpPOUdBdzhvRjdvdjJoUEtfQ2pkQ2dZTXhaT2JJbl9jRDVWMURoS0xDVFJSdjExdHdVNGlXNTVidEdHdGJVeFVrYThocHRlZXlwSHA1U2xsTVNjcmtxMV9vU0t6bENaRDRnU2o2MEZuNGtIRVRsZEwyZnZIcjZHTUJ1UEVTQTJJSzEtQzBsTTd1a1VES1ZadzBVN2o3dGxVdTkxSll0NDlqYXgyX2wxNk5UNm83d0JqNHlHRmZiYTdKYkxxSlNCSTZheFFtTHBnVGZrNVpNR25reUZXWGJOYUxSVDIySjlIbkxsTExpSHJYSUo2bjFjc005eHVQS0VURE9DdGFfR3A2SEF4Zw==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: bootstrap-signer
      kubernetes.io/service-account.uid: f495ec61-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: bootstrap-signer-token-nhshd
    namespace: kube-system
    resourceVersion: "152"
    selfLink: /api/v1/namespaces/kube-system/secrets/bootstrap-signer-token-nhshd
    uid: f49704b5-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKallXeHBZMjh0Ym05a1pTMTBiMnRsYmkxdE5YUTFOQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG01aGJXVWlPaUpqWVd4cFkyOHRibTlrWlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExuVnBaQ0k2SW1ZMVlqRmlNalV3TFRCa1pERXRNVEZsT0MwNE56RmlMVEF3TlRBMU5tRm1PV1U1TnlJc0luTjFZaUk2SW5ONWMzUmxiVHB6WlhKMmFXTmxZV05qYjNWdWREcHJkV0psTFhONWMzUmxiVHBqWVd4cFkyOHRibTlrWlNKOS55NGtvSlZZUDVqR0NwemZacUZKcVNZa2xqUUI2a3dsSDhnaW5WTmFLcU5Oc25Cbl9GRHBhZUZyelZWcHhOcG9iZzhqR1VEOEthMWJ5Z1lDMmNmWVdfQXNvM3k5OVV4U0Y5LWVUaWFEcEJlZFlVR05QYUg1LVdsRzRtTFM0c1gxX0htTUlVVllZeEJPMG9jY0pYVUV0WEN5ZGtUOEQwdG00NzZWcnB4ZU9KSzdrNkVxY2ktTzhfYXVWYzZreEpmNDI1N1Z1eGRnLVZDajdfV3FCOW1NajdJT0xMRFYtd2tJLVlPQ0c4ajEycV9MMXlFWG1OWHk1dmk4eGlaaFhfemZEczRzVzVSWi1zbkVPaW10bm1MUldDcmVMVUJKQ2ZWTEVTVWZlcm1fMGNiT0RnVlREbmN4TWpJYkZybVd4SUdjY29kdE0zZ0dNTkRqdmY5Rnp0cnNBcHc=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: calico-node
      kubernetes.io/service-account.uid: f5b1b250-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:45Z
    name: calico-node-token-m5t54
    namespace: kube-system
    resourceVersion: "227"
    selfLink: /api/v1/namespaces/kube-system/secrets/calico-node-token-m5t54
    uid: f5b420c3-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKalpYSjBhV1pwWTJGMFpTMWpiMjUwY205c2JHVnlMWFJ2YTJWdUxYTjRjekpvSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWJtRnRaU0k2SW1ObGNuUnBabWxqWVhSbExXTnZiblJ5YjJ4c1pYSWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzUxYVdRaU9pSm1ORGt6TWpKaU1pMHdaR1F4TFRFeFpUZ3RPRGN4WWkwd01EVXdOVFpoWmpsbE9UY2lMQ0p6ZFdJaU9pSnplWE4wWlcwNmMyVnlkbWxqWldGalkyOTFiblE2YTNWaVpTMXplWE4wWlcwNlkyVnlkR2xtYVdOaGRHVXRZMjl1ZEhKdmJHeGxjaUo5Llo1dlI2Rm9ORnl2Sm1FcHJrLW1BMjlhUmthV1RSSTNVRVNrdDJXRkxoNmFzQ3puSUhMa1VQOHkwZWU1Y0pOTzA5WVRMZjFnMXl4WkpwQjVzbFpLQnBYV2RpaVFzZHV3UDB2a3laamxGZVVLWU1UX0JhOG84RWNmY21MVW43bUx4aXJOT2psZlNjeklBdDlrWE1iZzRmS3ZDVmFnRWxFMHpGNl9wcXVhM2NodVFxMEJ5VXZJLW1kS0czMXR2eGR0WWhWbGUzdHB5dVBlUVJEeUhCbVZ5SDZMTm1SZHVQcnYxQUpFeEw2emFWMFdLR0xrcjhSMmNqbV9QS2hEVHpKbkhwYnlBaG0zRjlOeXBfODFuSDE4elYwOTZObms0UnJjQkRmZGsydTFZSmFOZHloWWNnU1BIcDJoUkYzTm9pZHZsS3FLUWpMX2Z0c1ZXR2VaX0dRWkVhdw==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: certificate-controller
      kubernetes.io/service-account.uid: f49322b2-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: certificate-controller-token-sxs2h
    namespace: kube-system
    resourceVersion: "149"
    selfLink: /api/v1/namespaces/kube-system/secrets/certificate-controller-token-sxs2h
    uid: f4947ebe-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKamNtOXVhbTlpTFdOdmJuUnliMnhzWlhJdGRHOXJaVzR0TkdSc2JUY2lMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzV1WVcxbElqb2lZM0p2Ym1wdllpMWpiMjUwY205c2JHVnlJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1ZFdsa0lqb2laalE0TVRNMk5qRXRNR1JrTVMweE1XVTRMVGczTVdJdE1EQTFNRFUyWVdZNVpUazNJaXdpYzNWaUlqb2ljM2x6ZEdWdE9uTmxjblpwWTJWaFkyTnZkVzUwT210MVltVXRjM2x6ZEdWdE9tTnliMjVxYjJJdFkyOXVkSEp2Ykd4bGNpSjkuYk41aHRkQTEwR2NZUzRRYjRpZW9VLW1raS1leEFaQzh2T095NHRnNDRYMW5hQ21reFd2QmxNcFA0OEVlZWVMdVF1eUoyaERLR1d3N2Vtb0ZuX3dHd3FvTFdUU3Z4NHVMMjRTOFEyOVBnNUl1allncDhnekgzOWFZRE8xazZQeVc0amw4X3ZZT0RmbG5aMW53V1BuanI0OHdIVEE1Q0pvMlV2dWwyckt5OXlsSEVvZkI1UlZYcUd6SU9SVEg0bU9UYUFYU1JuZjY0T1luNkxrNHZEYy0tZWdRcWtzb0xCUWxocTREemthT2IxYUJnWEh3Zm14bC05LWd0bDBhaU5mdUFMZTU4QW1NbWVadHlTcVBZSmRxRkhKVXF0aTZzUXBjc1J2Yl9zbE5Ma3ROZnZQbUFuNEUwVzUtTFJTV2dGUGhqbzBaNzNkLTljQy14UktsYTFXR1FR
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: cronjob-controller
      kubernetes.io/service-account.uid: f4813661-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: cronjob-controller-token-4dlm7
    namespace: kube-system
    resourceVersion: "146"
    selfLink: /api/v1/namespaces/kube-system/secrets/cronjob-controller-token-4dlm7
    uid: f490b393-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1lXVnRiMjR0YzJWMExXTnZiblJ5YjJ4c1pYSXRkRzlyWlc0dE5HcDNkSGdpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdGbGJXOXVMWE5sZEMxamIyNTBjbTlzYkdWeUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVptSTJObVExWmpNdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T21SaFpXMXZiaTF6WlhRdFkyOXVkSEp2Ykd4bGNpSjkuVy14YmxoU3VOSHdiU2Q1YnpTS3FGeUV6bWZhM3pfS3VDaExQS3hVbWdtTG5JUlVwY3Nfc0g3V25mcnM4NjBEd2JHU2tqaS1Ba0N2MUh2SkpzTmp3RHdTWVBsdmtTWVkzU2ZOQ3V2U3h4aU0xdU1sb0ZoOEt3cUhvU3NEYjdqalhfWnVCYWdDZEpVSzdJR3Q2MUp3THo2cGl3LUhyZU9uX21mQU5MR2lhTXpJQ0g1SWhoVlJIT24ycmdRY1BoZVM2dnR0b19FMlNsOE85blprT1dBRW1oQllzQmVUdDBBZ1NXRUNsam5HQ1dRaVZFcHdwUWtMM2VsRjBYdnF2Sk5BV0NaT296Mjl6YzZCamxoWXg0aUNtZVJUbFZDQXllak50VE1jcDctUF9TYi1wamN2Yy1aZDdONlQ5eDRNaFN3WmtZOC1aaHd3OU1uUWlLcjhsSDRzaE9B
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: daemon-set-controller
      kubernetes.io/service-account.uid: fb66d5f3-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:55Z
    name: daemon-set-controller-token-4jwtx
    namespace: kube-system
    resourceVersion: "306"
    selfLink: /api/v1/namespaces/kube-system/secrets/daemon-set-controller-token-4jwtx
    uid: fb68b9bd-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1pXWmhkV3gwTFhSdmEyVnVMV3Q0YkRnMElpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltUmxabUYxYkhRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbVpEQmtNR1F3TWkwd1pHUXhMVEV4WlRndE9EY3hZaTB3TURVd05UWmhaamxsT1RjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2WkdWbVlYVnNkQ0o5LkNuSE01X0RCQnVQWkZWTTdlckJ5WklMQlFVUjV4dU02RGV4Znh1Q0tFNGpKRDZ5VFVSNmtCQmpsM3hSckkwbkRBVDh3LVZpcGNKTDRjLTNzd0ZEdjZLaVJHX0RMQUpXdXlHbjNhb2pwVUUyZWYxNjc0b2VrMEhZU1p5XzVKY2xqdGpCMWtKcHZCZTVKMTk2VE1xbDhmMG5ta2JiTVlZUDc3QlFOZlF6X0FQWEJKYnlwU1RJMUd6ZGY0VHNGbzhJSmt1UmJUQnh2RWVkMEZLbFBxbVpUemNOOW56UzNQYWZPd1VWUzdQSGtKTWE2V1lWVGJkXzZTb3o3c1pJQUVRWFNiaVlqa3B3TmJvLTQwNlZRbHNsWlU2VjJOMENtZTFEOVVISG55aWFDb1R4NXZmbHRLSlQ3bFpiN2R4YXhYeU5pU0lLX21TblQ3VlBVQjB0YkRsbHJmdw==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: default
      kubernetes.io/service-account.uid: fd0d0d02-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:58Z
    name: default-token-kxl84
    namespace: kube-system
    resourceVersion: "384"
    selfLink: /api/v1/namespaces/kube-system/secrets/default-token-kxl84
    uid: fd1bb611-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1pYQnNiM2x0Wlc1MExXTnZiblJ5YjJ4c1pYSXRkRzlyWlc0dE9HZDZOMjRpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWd2JHOTViV1Z1ZEMxamIyNTBjbTlzYkdWeUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVptSXhOekEzTmpjdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T21SbGNHeHZlVzFsYm5RdFkyOXVkSEp2Ykd4bGNpSjkuQnhsU3Nadi04dXdOcU40dURUTm5hMUpKeklCNWl3M3FkbDRlbjh2cTdkeDIzLXRLY0Z4SVlSZHh3WjhnVmpSOXc5dUVCVDF5a05jZmxmWGh2NXZRVHVPU3lTNm53N0lMbVplVlMtWWFzWU9aM20ySl93andjSlA0SF8tZWhiVlZ2RXJQMTVXZW9QVmhmRUI5UUFxUnNWZy1LclFLVk9uVHEtQlpKQk9ZRUpSZ3pnUkpoTy11ckFlaTB6Zlczb2FSZmJGbXozc3FzaUNCVTJ1cVVqcjFMYkl0cnJaNUhQaEpRd282d09ZS1ZDeUd5MGxaQ1ljWUxwVG8zeWdULWh5eDhSbGdkVGpEMnRyR2N0VEg3VGhQbWpYaUpMS0tPWGhzLTBieXRIR0c1ZVNiRnVVVDVxaEFwRXJlakF6cmE4U2I3dmR3cGJGY3lidUhnQ204eHdtSGZn
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: deployment-controller
      kubernetes.io/service-account.uid: fb170767-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: deployment-controller-token-8gz7n
    namespace: kube-system
    resourceVersion: "287"
    selfLink: /api/v1/namespaces/kube-system/secrets/deployment-controller-token-8gz7n
    uid: fb18542a-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa2FYTnlkWEIwYVc5dUxXTnZiblJ5YjJ4c1pYSXRkRzlyWlc0dGMyNWtkSE1pTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdsemNuVndkR2x2YmkxamIyNTBjbTlzYkdWeUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVptSTRZMlpsT1dVdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T21ScGMzSjFjSFJwYjI0dFkyOXVkSEp2Ykd4bGNpSjkub3ZWNm1xYXVuZjdaa0EzSjlPOVhSNFRuS1pZWEZ0V3dqVEwyU3hMc1J1UWZwVVR1MlZMQWVPT1hTX2k2VHZwM3UzYzk4d05Qd0RxNW04MDdXYklLY0dRZTRlQW12UnFBR29HSGtqa3RSWkNUaGNySzU3REE5dWd2Q2ZJdjFjckhSanFPWlZqb2JnZkh0VDA4cmdEQ3JnZW12VjR6Y3dLd2cxdV9vbklERWlhYkdUdE1rNUptSUV2UUNmaVlYZ1FOOWRPazFlOGR6ZmtpUkM3R3RPR1ZZVmI2bno0WGxxSnA3dklma0Q5U05TMkN4RE5IdkFqTmxmWVJMRjExaktlX1Jqck1keDRhY2ZsRFB2UFdNTWZfeE9pOWI4ZE95UXVhRE1jUkVUR1YzWC14d2lramFfbTlBSG5hZzNxcG9zWU9JcEZMWWpPT1VhTjFRRy13YmMwUDNn
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: disruption-controller
      kubernetes.io/service-account.uid: fb8cfe9e-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:55Z
    name: disruption-controller-token-sndts
    namespace: kube-system
    resourceVersion: "309"
    selfLink: /api/v1/namespaces/kube-system/secrets/disruption-controller-token-sndts
    uid: fb8e0646-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKbGJHRnpkR2xqYzJWaGNtTm9MV3h2WjJkcGJtY3RkRzlyWlc0dE5tTnRhM01pTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWld4aGMzUnBZM05sWVhKamFDMXNiMmRuYVc1bklpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVpqWTFOREJtTXpVdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T21Wc1lYTjBhV056WldGeVkyZ3RiRzluWjJsdVp5SjkudjM3YnZjUElFTE85Y21iR29mTklVN0ZucGVwVS16dFVtNTFnbW1DTExRZkZRV0dqTnZPZGNHVVNxZnoxdDlteFNZUzVQWklhUDlaaDZuRXBSdGNMaHUwb0FGWFVCeWpEbldrVS01Nm54OFdhMEV4M1dPTHlrdWpEMTQ3NUl3Nkw0ZW1FZWRabXpZX3Q2dTRFakhtNllhcWFqckZCU2tCUzluNjY3V3djc0RvWENKcDhxLUoyVV9JWHBETF8zNGV4Rm1ZUVYxSktCTUNPU1JwOGtLWjAyODZkRVBVckxWc09DSjd6V2J6d3NLU3poSEE4bG5TSS1wYnJPV3NmbHowbzFKSkNMVlpQZVFoeVNLV1QyR0R4MXdEWTZEeEFHLUZDZTR4MExKZldTNUE1NWVVRTkybzM1bktUX0w5TmdScXRKNmNIeEtCVDZIQjNJcVhydGdFWlFn
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: elasticsearch-logging
      kubernetes.io/service-account.uid: f6540f35-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:46Z
    name: elasticsearch-logging-token-6cmks
    namespace: kube-system
    resourceVersion: "243"
    selfLink: /api/v1/namespaces/kube-system/secrets/elasticsearch-logging-token-6cmks
    uid: f656225b-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKbGJtUndiMmx1ZEMxamIyNTBjbTlzYkdWeUxYUnZhMlZ1TFRaNGNXcHVJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1Ym1GdFpTSTZJbVZ1WkhCdmFXNTBMV052Ym5SeWIyeHNaWElpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUptWWpJeE5HUTVOQzB3WkdReExURXhaVGd0T0RjeFlpMHdNRFV3TlRaaFpqbGxPVGNpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNmEzVmlaUzF6ZVhOMFpXMDZaVzVrY0c5cGJuUXRZMjl1ZEhKdmJHeGxjaUo5LkdYSWd6WUNTTDhabkhacVhjamYwUXJMU2d4OEZfMWgxcVVNb1RJVlBVRm5sa01Na00ySlE0Q2dKV2tFeU9EcUE4SWJyN2xONjUtZFpoVkl3RU40VjZlVTJpdXQ0UGZSX3FoRHBxT08zLV9lNXZvYTNNS0dzMjRPTk4tTTEwRG1YNVV0U0VocElBWlhVZmI5d1hxbHA2TlBpMGlZc2ZRbDJySmd3aTBiX2VWd2VINUtMV1pDTkZYR2VFRjRMemFkUlRLNW5aZ0RabWJrRElXeWdkUHFjN3BUZ1pqLUZfOFpWWEExeTFhWjA5Vm42SkN6V2pqLTVnUkFZZjB1STY2ejA3YjF6d0JjWV8zNnNKVE55ekNZSG05bFhIY3Nlb2taRndMZHNwcXdMQVRjMHQxUVBQdlhIZ1k2bWwxX2g0SV9qLXpvd2VGcHZnMWdHRk1pMGlTSUpKdw==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: endpoint-controller
      kubernetes.io/service-account.uid: fb214d94-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: endpoint-controller-token-6xqjn
    namespace: kube-system
    resourceVersion: "299"
    selfLink: /api/v1/namespaces/kube-system/secrets/endpoint-controller-token-6xqjn
    uid: fb224d46-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKbWJIVmxiblJrTFdWekxYUnZhMlZ1TFhwM09USm5JaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1Ym1GdFpTSTZJbVpzZFdWdWRHUXRaWE1pTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUptTmpFMk1XVmpNUzB3WkdReExURXhaVGd0T0RjeFlpMHdNRFV3TlRaaFpqbGxPVGNpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNmEzVmlaUzF6ZVhOMFpXMDZabXgxWlc1MFpDMWxjeUo5LmQybXNKZU05MFFmUkl2b29BZ2gwbjNsalk4a2VkOE14dGlNTVlqYkw4bVhuZjJWTGM0OFAzRWFzVEh3aFpzOXJualdDR3VpbWtRMU1mSm5BNkx2S2dmbTRrQjlkOWtXdW1vOUZMUVVMUHBfWG84UWV6VV82eHhNVEJCT3ZpM2JvODJ2ZkcybElrYmZuQnB5YXhhVnZSU3M4enRoWnpobGRaOE5WS3VoOUd3Y21oRlEwQ2xRMURsS0gxamJHTk16S2VkX3FIWHJZVk9GcXYwMThJU29PNTBEY1hmdm9WZ3BYZFR4SEwyenVfWWFhXzNZRERCdGF0RElsZGtycmRkc2hiTnIxbUcwNkdjWlJvX3ZCRDBUaFlrYnRhVnNtM2pPZEg2SXdReFBMTzU0MHhqUFB1SXFTbmJ0Tl9rbFgxUXdiUl9kQUEzZElyOHBIcFczZGVPRFhRUQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: fluentd-es
      kubernetes.io/service-account.uid: f6161ec1-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:46Z
    name: fluentd-es-token-zw92g
    namespace: kube-system
    resourceVersion: "235"
    selfLink: /api/v1/namespaces/kube-system/secrets/fluentd-es-token-zw92g
    uid: f61927af-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKblpXNWxjbWxqTFdkaGNtSmhaMlV0WTI5c2JHVmpkRzl5TFhSdmEyVnVMVzB5Tldaeklpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltZGxibVZ5YVdNdFoyRnlZbUZuWlMxamIyeHNaV04wYjNJaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbVl6UmlZakpqTmkwd1pHUXhMVEV4WlRndE9EY3hZaTB3TURVd05UWmhaamxsT1RjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2WjJWdVpYSnBZeTFuWVhKaVlXZGxMV052Ykd4bFkzUnZjaUo5Lkw2N250M0dHNXg3Ui00NlJXdDNxNjBQYXNWTExIUDR3eDh2Yi1TNVpOVXI3dnpERmxNMWlaREhXeWFXTXVzTGhmaFRHUlBaZDdVV0ZZM05heWJNX3FaLVhCdXltUU1pQXJabzV4RnZPc0VoZF9rbHg4VE9nVFlnOHZPR0xLWHhzbUxweW9QZDc5S21LNFZlTlhIcnExcl9uQjBJOGhRYk02QXJJYjBRYXNiUkNkdTR2Vzd3WkdvVlBwS2p1TVEzVk5ZM1lHdGNKR21ZdGFXNjVMejZjb1YxRWJLTmZ5LWh3emF6d0pLbVByQmt5aTNyNHBoX19fNFlvSXdSVjZoeEVuNUZjenh1anB4eEhNdmpnMFJXT3owV2F4N0RFZWo1TzA4dUs4OFgyUUZKX2VZRVhxeFh5THRid3F2NldEVllueTg1U0dzWTVmQzd2LUNuc0JVNzQ2QQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: generic-garbage-collector
      kubernetes.io/service-account.uid: fc4bb2c6-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:56Z
    name: generic-garbage-collector-token-m25fs
    namespace: kube-system
    resourceVersion: "325"
    selfLink: /api/v1/namespaces/kube-system/secrets/generic-garbage-collector-token-m25fs
    uid: fc4cf13d-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKb2IzSnBlbTl1ZEdGc0xYQnZaQzFoZFhSdmMyTmhiR1Z5TFhSdmEyVnVMV3BpWjNGeElpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltaHZjbWw2YjI1MFlXd3RjRzlrTFdGMWRHOXpZMkZzWlhJaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbU5HRXhPR05oTlMwd1pHUXhMVEV4WlRndE9EY3hZaTB3TURVd05UWmhaamxsT1RjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2YUc5eWFYcHZiblJoYkMxd2IyUXRZWFYwYjNOallXeGxjaUo5LnZVbUhGbkhObWxOTUhYUEJOcVUzRzgxcVYtNTBBN2xsNjE5bFJjdDBfbklGSDRQcnJDS3k2SDlWYnpaTW0yWjRxZjZGaDZvMWRxQkxRVFJ2enJ5Q1J5M2ZMY1ZTcWlKcU1UeVRBc3E5amdraTFSOUVqZTlhSV9HR252Mnh2T2xNbENVRDRrejR4Y3hWRWNYWjVlbGhXUDIyNWRHZm9SdlJvYm92bnpJOGQ3alpRUmVlcDBsbWF6NVh5VkNuM19lbmZJSDRZOVlDeWQtQ3NCWUV4NlJZX2htNWJZZWNfWVVxZExYNmtUU1BJZmxqNGQ5N1dsUGxLWGtpc3hXOWpzWXhVOXJSeGwxQWV5Sm9WaGJJZEhfVUM0Tm1uaVZYZkxaMlN5eGFlRnZ5U3FrdE5EQnk2RnM2QWlfUzdoREFRMEp6SFY1WVctZi01SVBvaEhEU0JVR2pLQQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: horizontal-pod-autoscaler
      kubernetes.io/service-account.uid: f4a18ca5-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: horizontal-pod-autoscaler-token-jbgqq
    namespace: kube-system
    resourceVersion: "164"
    selfLink: /api/v1/namespaces/kube-system/secrets/horizontal-pod-autoscaler-token-jbgqq
    uid: f4a2f41f-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKcWIySXRZMjl1ZEhKdmJHeGxjaTEwYjJ0bGJpMWtOelk1WWlJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExtNWhiV1VpT2lKcWIySXRZMjl1ZEhKdmJHeGxjaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbVkwT1dZd1pHUmtMVEJrWkRFdE1URmxPQzA0TnpGaUxUQXdOVEExTm1GbU9XVTVOeUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwcmRXSmxMWE41YzNSbGJUcHFiMkl0WTI5dWRISnZiR3hsY2lKOS5CZ2duNFpUVkV5SGxsUWdpWmtxYXVsT1ExU25RbEljeHYxSlBzVHE3LWJkZlY1U2E2M0JDT2lwQTZkT25YUzVOeE9RX0QxbHFPbXdWR1VPT3kzQjNieUZ2bG01amRobDVlUVBESjRWdUY1WFVRdzhCYmFCaUFnUEd5X3lLdWhtd3EzcU1LLWtud21YcFFMOGc2MVhWanhaYUszSlBrRjV1MVpUelRNV1RsODNFaDFScXU2Y0I0OU5jNkw4cHEydWNYb3lXbG9xXzRLbHhoRjJqR0l2MlhtZXNuX0oySmdMUHNIaUsyLXdSSzhSM2MtclhiT2Q3Y29NR0dIQXBMLTYzOG1nVGRQSnBvSVBoM1hwSUxxVUJSWDhMZzBUN2V3dFNoZW8zdTFVSHdJSE5TVVZzYzdQZmlZV1RxN1hSR09TZmE4bUp5aGdITE1HTkpYVHNSQlE3UlE=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: job-controller
      kubernetes.io/service-account.uid: f49f0ddd-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: job-controller-token-d769b
    namespace: kube-system
    resourceVersion: "161"
    selfLink: /api/v1/namespaces/kube-system/secrets/job-controller-token-d769b
    uid: f4a033ab-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKcmRXSmxMV1J1Y3kxMGIydGxiaTAyTm5SbWVDSXNJbXQxWW1WeWJtVjBaWE11YVc4dmMyVnlkbWxqWldGalkyOTFiblF2YzJWeWRtbGpaUzFoWTJOdmRXNTBMbTVoYldVaU9pSnJkV0psTFdSdWN5SXNJbXQxWW1WeWJtVjBaWE11YVc4dmMyVnlkbWxqWldGalkyOTFiblF2YzJWeWRtbGpaUzFoWTJOdmRXNTBMblZwWkNJNkltWTBaRFkxWkdZd0xUQmtaREV0TVRGbE9DMDROekZpTFRBd05UQTFObUZtT1dVNU55SXNJbk4xWWlJNkluTjVjM1JsYlRwelpYSjJhV05sWVdOamIzVnVkRHByZFdKbExYTjVjM1JsYlRwcmRXSmxMV1J1Y3lKOS5vMjVfNldXWXFXSmdkcW9JNWJ1VEJqU1VuRmlrSVhzNW1tQmZZeEExTzhGM2RaeVNkVGxOU19EZm5iWENnNWx3a1JQTl9CVWlWVHdhc1V6TUFIMDlmLURKWTUxcXBycW0wanJCRll2dG1vdVJhMnZYNUtUV2U5QWN2UFpPcl96aC1fSnUyUmNNa3JUMk1JV0J6ZzlCNmNkZ3dRSTQ3MXVLQ1AxTzhBM2lRb3JUMk9WdkJLb0xqSUI0bUhNX29WT0R0LXZ6bnVoQUQ5Rzl0NlU0YVBTbF9yYkRWbXhjNm1KNEZBMVNmNDhOQXlWOHFKMUh5Wmtzci1td0ZTNDVvNDEtbGhweDhfU2lFZ1pRdUJFd05xNjZIVktENUFub3IzWmFQUWR1Vkk1S2c5T0puYld1ODVpLWh2UG9yX1Z3WXpkOHJpTDNWLUtERmtSSlB5Y0I1M0pMOGc=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: kube-dns
      kubernetes.io/service-account.uid: f4d65df0-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kube-dns-token-66tfx
    namespace: kube-system
    resourceVersion: "178"
    selfLink: /api/v1/namespaces/kube-system/secrets/kube-dns-token-66tfx
    uid: f4d91a1e-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKcmRXSmxMWEJ5YjNoNUxYUnZhMlZ1TFd0MFkzWTVJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1Ym1GdFpTSTZJbXQxWW1VdGNISnZlSGtpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUptTkdVNFpEUXhPQzB3WkdReExURXhaVGd0T0RjeFlpMHdNRFV3TlRaaFpqbGxPVGNpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNmEzVmlaUzF6ZVhOMFpXMDZhM1ZpWlMxd2NtOTRlU0o5LmxWeWN0czcwSWpUd2tHNENCaWF5MmFybTFRcG04UnZQeVBiZ0dIamlBRWdaX0pXYUlRc2VWVnBuUGtKU0c1T1Z5R201SnE2WFVCNnBucXRESXN6Y2pOOW9WTVZyT1NmMlZuN1hla1ZONnNrWmNnbU93blBQQ2NXSW1qZVFaN09KaUlMX01Yd0d5UmZGY3ppYWdhbGFocnRsU045NkJycEFqQ1Frdk92VHRndWlpTlE4d3pxTXBoY3EzTnFBZHhnNWV6LUFJUGZhajNaUWNRX1RkNktxbE5VN0xhTk5jc0xmNkV5TF90REVNeGlFclk3ZjdWM1NtU1RfRk1jZUo2ZGxjOWhDa0FNRkdvZ3hTQk1vMjlXd0w2eUdqN0RRalE4aUhsVXNvM0V0UFBVTVhLajUtOTJZd1pxcTM3STRab041U1dGSm9fU0FuMDhxeFRSLW1QbUJCQQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: kube-proxy
      kubernetes.io/service-account.uid: f4e8d418-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kube-proxy-token-ktcv9
    namespace: kube-system
    resourceVersion: "188"
    selfLink: /api/v1/namespaces/kube-system/secrets/kube-proxy-token-ktcv9
    uid: f4e9e7ef-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    dashboard.key: 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
  kind: Secret
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Secret","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard-certs","namespace":"kube-system"},"type":"Opaque"}
    creationTimestamp: 2018-02-09T19:46:48Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard-certs
    namespace: kube-system
    resourceVersion: "265"
    selfLink: /api/v1/namespaces/kube-system/secrets/kubernetes-dashboard-certs
    uid: f75c815e-0dd1-11e8-871b-005056af9e97
  type: Opaque
- apiVersion: v1
  data:
    priv: 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
    pub: LS0tLS1CRUdJTiBSU0EgUFVCTElDIEtFWS0tLS0tCk1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBNWtSY1FsL0tTZ1JpMVJUeWF6WGgKRlYrZGFCcXFjdEdLWUJwN1JLZ1hYb1cyTmtqdkZtbC9YcHUxTldObDhFV2U0RlNMWFZDTjBRNnMxbUdkLytuWQpBUkcxU3BpSnlnR0lJckVjOXVyN0VCWG5PYm1DVjFPajhQNUJmS2l3Q3Bsb0N6VkR3OS9HUEtOTytjQzZRQmE0CitMcHJvcFAzdzBCNVZ4V0h0aGZoRE9EUDdmOE45cklSOXFBblZReDg2M2lEVEFLQjFJUHhndVVaQUIxaXFMamoKOEFUclEva1VMeWQ5N2YxV2owQnE1ZW44SWUwVGRtOTRKOXY3NEl3Vk1ucGhGM1llSGFZcTdIYlVIWUNBQlkycwpwNlhUdm12bUhxd0JvYWgyV0VXTnZVeDNaL05aandhc2hGalgvU3BuZXJpSjNhOEtmb2daMk9YRm45ZFlKUFd5ClZ3SURBUUFCCi0tLS0tRU5EIFJTQSBQVUJMSUMgS0VZLS0tLS0K
  kind: Secret
  metadata:
    creationTimestamp: 2018-02-09T19:47:42Z
    name: kubernetes-dashboard-key-holder
    namespace: kube-system
    resourceVersion: "775"
    selfLink: /api/v1/namespaces/kube-system/secrets/kubernetes-dashboard-key-holder
    uid: 17c2fbc0-0dd2-11e8-871b-005056af9e97
  type: Opaque
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKcmRXSmxjbTVsZEdWekxXUmhjMmhpYjJGeVpDMTBiMnRsYmkxaWFtTjNZeUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG01aGJXVWlPaUpyZFdKbGNtNWxkR1Z6TFdSaGMyaGliMkZ5WkNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExuVnBaQ0k2SW1ZM09UQm1ZakpsTFRCa1pERXRNVEZsT0MwNE56RmlMVEF3TlRBMU5tRm1PV1U1TnlJc0luTjFZaUk2SW5ONWMzUmxiVHB6WlhKMmFXTmxZV05qYjNWdWREcHJkV0psTFhONWMzUmxiVHByZFdKbGNtNWxkR1Z6TFdSaGMyaGliMkZ5WkNKOS5HdG1mTGtUVXd3cFIyaWJsLTUxQVE4ME1la2VSTGlnekhYaElGdmVoRzJKUjlnRnZhcTNjLWtXQzR4Z1hjdWZudDFyR0ZqSnV2NEF2M3NIampIV1JqSThBZzlzOWZad0RoUkVvdWZjdmp0R3c4MFhyNTRWZU9BV2lYRGdFR0tDNW9zUlpJUnBPbmFSQ3Rzby1Ta2VnQWxvWm44d2FNVUJCMFJYRS11OV8yTzJlc09fRUVvV2g4b21JRFNPV3YtaEVIV3hMQjhXeHBLN0pnNDJVRnhodEp0YmpPelBWb3BxZGRCQmd3MGY3QnpXRi13QWVQTEN3YklRc3hLT202OFRwYmFUcTZVZnZ3TzlSSlE4UGp0WDB5MW1UWnlSZ0ZzbEpYRV90S0RzWVNaWDNSaGFNMW00YklFYWh1QmxhOUlYVG5FbGd6M0xrOHQ5MnFzLURtcHZHT0E=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: kubernetes-dashboard
      kubernetes.io/service-account.uid: f790fb2e-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:48Z
    name: kubernetes-dashboard-token-bjcwc
    namespace: kube-system
    resourceVersion: "267"
    selfLink: /api/v1/namespaces/kube-system/secrets/kubernetes-dashboard-token-bjcwc
    uid: f7924ee1-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKdVlXMWxjM0JoWTJVdFkyOXVkSEp2Ykd4bGNpMTBiMnRsYmkxM04zcHhhaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG01aGJXVWlPaUp1WVcxbGMzQmhZMlV0WTI5dWRISnZiR3hsY2lJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExuVnBaQ0k2SW1aaU5EQmhaV1V3TFRCa1pERXRNVEZsT0MwNE56RmlMVEF3TlRBMU5tRm1PV1U1TnlJc0luTjFZaUk2SW5ONWMzUmxiVHB6WlhKMmFXTmxZV05qYjNWdWREcHJkV0psTFhONWMzUmxiVHB1WVcxbGMzQmhZMlV0WTI5dWRISnZiR3hsY2lKOS5HXzlHdlM4RDk5WWVVSG1nUEtFZUtlVGZZajZuOTdIU09RYVVSUmlHZXJyMzc3bm9CZmFtVjFEQ2JqUEctQzdqdGJYOTlZNHlrZ2pxb3BheHJ4VXNicTcxenMtU2dzTnFvVUtieXhZZFB3Q19LRldnSkZZaHpfVjkyRHVYeXZBZTlIWmFmSXZYTno1OTVOVW9YN3BUYmhsRTVkN2dUZEh6bS0zZk4tSXpTUlRRb3NmUlFaT0t5ZXRzeDFDNl8yTUNKRDJ5SW05bHBiOGRtWlMzTkU3bnhfaWQxQ3VmVGtFWVVHRGtTUVJWTDB5NHdnMWRLNGhhOHBIRnJteEFlVHM0MXRrRHU2UHpoNDFkeGYwdkdiX2RBYXJSSkhmY3dWZWdfNVdZQVdmQ0ZHVWdVdFpwRHlaVDYxdFlpUGRFeFpHTVY0X3hxVXdZcHVudnM3TU80VzdQZ1E=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: namespace-controller
      kubernetes.io/service-account.uid: fb40aee0-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: namespace-controller-token-w7zqj
    namespace: kube-system
    resourceVersion: "302"
    selfLink: /api/v1/namespaces/kube-system/secrets/namespace-controller-token-w7zqj
    uid: fb41bba9-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKdWIyUmxMV052Ym5SeWIyeHNaWEl0ZEc5clpXNHRiV2MxYlRraUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaWJtOWtaUzFqYjI1MGNtOXNiR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWRXbGtJam9pWmpVd1pHTTROelF0TUdSa01TMHhNV1U0TFRnM01XSXRNREExTURVMllXWTVaVGszSWl3aWMzVmlJam9pYzNsemRHVnRPbk5sY25acFkyVmhZMk52ZFc1ME9tdDFZbVV0YzNsemRHVnRPbTV2WkdVdFkyOXVkSEp2Ykd4bGNpSjkueVhiTzZ3V3Rxc1lSVjh2ZFBNaVZlelNrRW9wd25wc3AwZFJ2YjBCVUpWTFYxY3FRelB0UzNDcHRrZExOcTIxcERoSjh0YWxJZmdwNmV3VERfVWd2M1czODJMMmtFbnQ4SlVENVhJUWFnaFdoelo5c1pzbzdLSkM1Y1pma2ZtS0xKUmlvZ01JVWFTeVRVcXM5V0l3czVLRThaR0Yyd1d5eXU3U3NvTE5BeGs1WkhGSGVXWFR5U3p2TWkwdS14QzBuOHlwYnlITTlJaGFhZ2VsLWxia0g0am5JZkZvQ05VdVVLREdSdmJONHlQUVdKUmpkQy16Q01ILWQ5S2pGTVNsRlQxaHZiVFlIVG1oMjIybzdULVV0VWM0ZWJuT19tbmk5ZEw1Unp5T0lQUXFjVk41Nm9haVZvRmRQY2tFcnFNMUxOaDhPT3lzb0hhMllJQ2ZiT0tKZzVB
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: node-controller
      kubernetes.io/service-account.uid: f50dc874-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:44Z
    name: node-controller-token-mg5m9
    namespace: kube-system
    resourceVersion: "195"
    selfLink: /api/v1/namespaces/kube-system/secrets/node-controller-token-mg5m9
    uid: f50ed60e-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKd1pYSnphWE4wWlc1MExYWnZiSFZ0WlMxaWFXNWtaWEl0ZEc5clpXNHRhbTQ1T1hjaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaWNHVnljMmx6ZEdWdWRDMTJiMngxYldVdFltbHVaR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWRXbGtJam9pWm1KaU16RmxaR1F0TUdSa01TMHhNV1U0TFRnM01XSXRNREExTURVMllXWTVaVGszSWl3aWMzVmlJam9pYzNsemRHVnRPbk5sY25acFkyVmhZMk52ZFc1ME9tdDFZbVV0YzNsemRHVnRPbkJsY25OcGMzUmxiblF0ZG05c2RXMWxMV0pwYm1SbGNpSjkuTEdON0I3aXNSNTY5bDY5N0VVR1VNbkRpOTg1dWNSY1RHVXhsazFCM2ExS3ZsYzRYUHh4S3BDQWVNaGlTYUZhNXQ0S2JPem1NTkFFVm1NQ09fWEFhXzVmMXFRbWl5NlJwTVFfZkJfcWo0STF1T3VkX1d1R3doSzRQNWxZeUlVRzBzazV4bkFhUWt3VFYwYy1IdjIwS25feFFBaWpaRW83S3JiSUVyTjd1NUR0bnZfTmZXbTBHWmMzdGtGRW1PYWNlUGtIZ1lSeFFMUEZCOUhYWkUyTlRSaU1xamQtVGl3a0VwMWhhM2xidk5xNzFWdFJ0VUxCNWJxSk9JYUM0aGo3dmhvUG1ueVp4VW1QeU9UdEgxZ1g5UlNseXUyQUNnc3R0YXlsXzk5bE04VV9mRDR1eWdjakR4TnQwamd5Yi0xRUktWnM1eTBWWDNEbmU1T05PV1gxUUxB
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: persistent-volume-binder
      kubernetes.io/service-account.uid: fbb31edd-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:55Z
    name: persistent-volume-binder-token-jn99w
    namespace: kube-system
    resourceVersion: "313"
    selfLink: /api/v1/namespaces/kube-system/secrets/persistent-volume-binder-token-jn99w
    uid: fbb42b23-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKd2IyUXRaMkZ5WW1GblpTMWpiMnhzWldOMGIzSXRkRzlyWlc0dGEzUnVOemtpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pY0c5a0xXZGhjbUpoWjJVdFkyOXNiR1ZqZEc5eUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVptSm1aamN5TldJdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T25CdlpDMW5ZWEppWVdkbExXTnZiR3hsWTNSdmNpSjkuVzlYTVphcEt4Q0kzNU1pNkxnMV9Db29YbnZZblRfUVNvaXMzRE02aHU4aUx4cW01NHM1RlZ2ZU1RUWpGRVRsNTF0RDlaVnUwcXJtdWFRQzJRUG81b3JVNXNIdmFkT2NRcGJzb3Vic2JlWnFzcmFkcUFzZTgzZjdkLS1QU1B0ZU5hUHNwU0ZqMm50d0E3RDdaYXpCV1FOd2tzR3Q0Z1pubFBCUE82b1l4TllYc1pPa21LS1VHYnNqVGRoSmV1TUJaZ2lrSzJueDBmWjNOalNfb3RXY3hlSUNyN0swRjJtUkhPekJHMjB4T2ZxbFd0NUdsM0pJV2dZZzBxWHBRZk83NHZ2eFhWNWpuNHFBeElZVXlBTW5od1FXTUFkbWhwZ3FfOEczanlEUm14U0puaUQ4eTkzZ09HQUwtek5zc3hPWkdRT19keWxXSHhwNFoxcVktYkk5a2F3
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: pod-garbage-collector
      kubernetes.io/service-account.uid: fbff725b-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:56Z
    name: pod-garbage-collector-token-ktn79
    namespace: kube-system
    resourceVersion: "319"
    selfLink: /api/v1/namespaces/kube-system/secrets/pod-garbage-collector-token-ktn79
    uid: fc018709-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKeVpYQnNhV05oYzJWMExXTnZiblJ5YjJ4c1pYSXRkRzlyWlc0dGNXZHlPRzBpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pY21Wd2JHbGpZWE5sZEMxamIyNTBjbTlzYkdWeUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaVptSXhZVGczWldNdE1HUmtNUzB4TVdVNExUZzNNV0l0TURBMU1EVTJZV1k1WlRrM0lpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbXQxWW1VdGMzbHpkR1Z0T25KbGNHeHBZMkZ6WlhRdFkyOXVkSEp2Ykd4bGNpSjkuemY1MVlSSEdFeTRSaWhFYnMxWXo5Rmk3eWdNby1FLThVaVNhbjJtTVM2TkdEekxnZ1BfR1A2QzFackdybE0waXN2cm5RWUtZNk9IRDRjLWhVdmFUVjg2eDl5clI2NjMtRXFUdXBENXRVcHZJZVdrVlRSamJUQkpNc0pabHdkX2ZsWW5LajhYVUVGVTZJQkZfa3lDc1YxTE9NUEtjcEk0WHZ0Ym5VTzBqOFpLUV9mQllEaUk2dzhfUjl2WmVOYzQ2djFKQTVNTzBQTFdLdndCNmt2Q2dxN0dabXhpT0R4eDE3amViZ2FlWURHWE9PYXRDdXQ5TkdvWjE3ZGZYRDRUaE1VRFJVYUlKUElIQ1piRnBWREFXU3RkWnFYM0hYd2RxN1pqMWV0N2N5RnNHdTdON0ZVak1WWmpQSzNJMVY2RU5ZTXlXX1V2RnZEU0NEN1dyb2h6UnRB
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: replicaset-controller
      kubernetes.io/service-account.uid: fb1a87ec-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: replicaset-controller-token-qgr8m
    namespace: kube-system
    resourceVersion: "290"
    selfLink: /api/v1/namespaces/kube-system/secrets/replicaset-controller-token-qgr8m
    uid: fb1b99bf-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKeVpYQnNhV05oZEdsdmJpMWpiMjUwY205c2JHVnlMWFJ2YTJWdUxXZDNOSFo2SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWJtRnRaU0k2SW5KbGNHeHBZMkYwYVc5dUxXTnZiblJ5YjJ4c1pYSWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzUxYVdRaU9pSm1ZakUwTkdNM09TMHdaR1F4TFRFeFpUZ3RPRGN4WWkwd01EVXdOVFpoWmpsbE9UY2lMQ0p6ZFdJaU9pSnplWE4wWlcwNmMyVnlkbWxqWldGalkyOTFiblE2YTNWaVpTMXplWE4wWlcwNmNtVndiR2xqWVhScGIyNHRZMjl1ZEhKdmJHeGxjaUo5LlhUUXpjVy1QZ21VcUJWZFFtQWVvemtKcjhQY3RPS0FyZWZVU0JjbDNybmNQU3cweFpKaVpYTG1SWFpnY1U3dkRfWnVqa3ZQSnE4U0FnRURvb3IyRm9Ka21BcUMtcEgwOUtQUHlkUkcxZjQtQlQtRTRzUS1HYzkyd29STE9fMGFjVEZhakxNUUNQdkRnRFlYYng1RGRxcVVEV3ZqYUxaRExFa3hLNm05Y0NXd25VU3VLcFZ4TmowMUlwYXJuYjgtSjNBeWJWaTRZaHFqTDJKcko3REJIZng0RFNJbGh3UzJqVUl6R2hNenlZUXhBT1loOFZPTFAtbmFPdEVUWU8weldyU3hqOTRvZ2xMeVM3S3FKZTdsYVp0elRfWnJla1pwTVZEaWNVOHdqR0F5OUl3Y1RPRVFac1RZbTBpR3RfZHJsMkFhb2EwclNSYnM3R0xvVGhOUExpQQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: replication-controller
      kubernetes.io/service-account.uid: fb144c79-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: replication-controller-token-gw4vz
    namespace: kube-system
    resourceVersion: "284"
    selfLink: /api/v1/namespaces/kube-system/secrets/replication-controller-token-gw4vz
    uid: fb15a514-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKeVpYTnZkWEpqWlhGMWIzUmhMV052Ym5SeWIyeHNaWEl0ZEc5clpXNHRabkJ6WjNRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaWNtVnpiM1Z5WTJWeGRXOTBZUzFqYjI1MGNtOXNiR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWRXbGtJam9pWmpRNVkyRTBORFV0TUdSa01TMHhNV1U0TFRnM01XSXRNREExTURVMllXWTVaVGszSWl3aWMzVmlJam9pYzNsemRHVnRPbk5sY25acFkyVmhZMk52ZFc1ME9tdDFZbVV0YzNsemRHVnRPbkpsYzI5MWNtTmxjWFZ2ZEdFdFkyOXVkSEp2Ykd4bGNpSjkudWcydDBCbHQ4bjBZTE9rMDQ1SkpOUE5MeFZIUnB3T0owanJoWVJkV2g1RnFxNXBzQk45aW9RTlRodHowSWJoamRkVFZyakJOdmFhR2YxQ3BzTWc3QVpRT0plSVVXLXljZ2dNc3ZzOERNaXh0cnRBUlowb1JsemN0RFhIUFk5SDAzRlVibzlNajhOa1h6VTJYTW1URDBWc2ktSGh4b0tJemVnaktfWGp6cFNkMzFvQVdQdDYxa2ZOVGEta00yX0lHZVpsQXJwLXJxajdnM2ctMzBibEh1X28xeXBBNU1RVEp3S01WdF9TZlpIZGJRTjVXTmY4Xy1YVWRrQm9tU1pjYnNmeHZoOHdSSjdrcWhMSGhKaGZ5ZjdGdUZMbnlYcWRBOGllRkRsU2UyZnB3NWhJQ1lnOUpUYjcwNGNLcTlYTFpuN1dyclJDbFZOdjU1OG9vNWNfNFJR
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: resourcequota-controller
      kubernetes.io/service-account.uid: f49ca445-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: resourcequota-controller-token-fpsgt
    namespace: kube-system
    resourceVersion: "158"
    selfLink: /api/v1/namespaces/kube-system/secrets/resourcequota-controller-token-fpsgt
    uid: f49dc035-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKelpYSjJhV05sTFdGalkyOTFiblF0WTI5dWRISnZiR3hsY2kxMGIydGxiaTE0Y1d4dU5TSXNJbXQxWW1WeWJtVjBaWE11YVc4dmMyVnlkbWxqWldGalkyOTFiblF2YzJWeWRtbGpaUzFoWTJOdmRXNTBMbTVoYldVaU9pSnpaWEoyYVdObExXRmpZMjkxYm5RdFkyOXVkSEp2Ykd4bGNpSXNJbXQxWW1WeWJtVjBaWE11YVc4dmMyVnlkbWxqWldGalkyOTFiblF2YzJWeWRtbGpaUzFoWTJOdmRXNTBMblZwWkNJNkltWmpNalU0T0RrMUxUQmtaREV0TVRGbE9DMDROekZpTFRBd05UQTFObUZtT1dVNU55SXNJbk4xWWlJNkluTjVjM1JsYlRwelpYSjJhV05sWVdOamIzVnVkRHByZFdKbExYTjVjM1JsYlRwelpYSjJhV05sTFdGalkyOTFiblF0WTI5dWRISnZiR3hsY2lKOS5hVTF1c2dXSFpQR3pNYUVQTHQ5bWRRUTFPdHZJVWRic1hJeThjNWNuQ3o4eEpzRFVMMWFnX2d5SmU3c1hMMDU4bTNmRXJjbkhDUGRiNjFtQlVZdGg5VXhBSW1jT256bTlESDhSdGVQSnNBYXBTbmFjcEJfSlBMQmpUX2NEVHFPV3VtTmE1VXo0Nm9rMlNvNkJQbmNtUzVFaXpRVnp5QWhUWktyRHg1XzkwN1YxSmIyYkJZZ25wazJ5VURHVkhFRWZURlVzUEMxQWNOVHNRNl9lcHdoOVpYUlFmLVV3Q0ljeWV6SllQVFVjSGxFUWZrU3pIcnUyd2hZd3V3MzZMOUdUZ1Q3bG5vVmRvVFVzVzNnRkE0NEtKRDZ4RTJvdHI3RHZEbzcxNmgwaUM0NTZSRW1tM2REc0tfZDdhRldHRlJSM1E4WkFNTEYtQzVpd0o0QThzNnV0THc=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: service-account-controller
      kubernetes.io/service-account.uid: fc258895-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:56Z
    name: service-account-controller-token-xqln5
    namespace: kube-system
    resourceVersion: "322"
    selfLink: /api/v1/namespaces/kube-system/secrets/service-account-controller-token-xqln5
    uid: fc26af7c-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKelpYSjJhV05sTFdOdmJuUnliMnhzWlhJdGRHOXJaVzR0Y0RjMWQzRWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzV1WVcxbElqb2ljMlZ5ZG1salpTMWpiMjUwY205c2JHVnlJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1ZFdsa0lqb2laalE1WVRNM1kyTXRNR1JrTVMweE1XVTRMVGczTVdJdE1EQTFNRFUyWVdZNVpUazNJaXdpYzNWaUlqb2ljM2x6ZEdWdE9uTmxjblpwWTJWaFkyTnZkVzUwT210MVltVXRjM2x6ZEdWdE9uTmxjblpwWTJVdFkyOXVkSEp2Ykd4bGNpSjkuWWtndjRxVm9jYVdOZE5FX2xVaGVPMlV5d2lyS0JDR3VsQnRIZE5UMXJiSlNhLThORmtXamQ4Y1YxY1ZjS1hPczZsVkRyWVdGVVVGcUZjOWN6RE8ya3ZmVFIxQ2tNZXlTR0hiTWJuYk5NZHE4SmdIMjIxRTZUNkVxSG1mdHJ4LWZxbzRZdFE0TFNfRzU3Vkl4R0trd3BuQWxsWDdwYk1HY2RHdEZGYVJlMzBpRXEyWk1uNTNwSzNVNlIyTG9xa2Y3SDRWdkFDaVJsaTNSWV95M0RrVUhGTk85bVdlS1lwa05BSWdWajN2ajZCSkdXNGZPNE9QcHp5c2lTc251cWR4c3BhWEN2elItSTZkTTU0REtBSHNVY2t2eDdqRmNHVzZjMDhUdHdibGpnLUppdkNyZk1aMWs2TnVsT0VhNFNCYTdNTmF5d1o5YmhWNWxKcUJyVkR4aV9R
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: service-controller
      kubernetes.io/service-account.uid: f49a37cc-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:43Z
    name: service-controller-token-p75wq
    namespace: kube-system
    resourceVersion: "155"
    selfLink: /api/v1/namespaces/kube-system/secrets/service-controller-token-p75wq
    uid: f49b300f-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKemRHRjBaV1oxYkhObGRDMWpiMjUwY205c2JHVnlMWFJ2YTJWdUxXczNhemt5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWJtRnRaU0k2SW5OMFlYUmxablZzYzJWMExXTnZiblJ5YjJ4c1pYSWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzUxYVdRaU9pSm1ZakZqWTJSa1l5MHdaR1F4TFRFeFpUZ3RPRGN4WWkwd01EVXdOVFpoWmpsbE9UY2lMQ0p6ZFdJaU9pSnplWE4wWlcwNmMyVnlkbWxqWldGalkyOTFiblE2YTNWaVpTMXplWE4wWlcwNmMzUmhkR1ZtZFd4elpYUXRZMjl1ZEhKdmJHeGxjaUo5LmdQWkp6QU5OVlV1MDFwZ19SQ1FFYzJ2OF9lMmZKQzhxTExQUUR6VTVNVkt6WEUyUzB3QkZoOFdqVXNSRjBNdW50U0JueFRWVTlMY2ZqSW05bzgwc2UtZVctc1pubEpaWldoTlpDR1ljQkFSYnQ4QkRXckc5c1d6TEcwaV9hMWlJczBEOUZiY0xHNFVYY1hvNEJta000S0pGRGFQV2RzS25wMUR6VV9oMmlWd01TMDBhb3B3WDZ0dzA2SzU3RUxVa0xtOFpBZVRTM0FrZTZLaVlMaTRHVFFxTHVnUVJoTGFPNFJIQkJvZS1YWHFfOW0tQk5HelhUX3ZkeHNlb3J1U3ZIT1dhT2sweXFOdWg1dHBndnQwQ2h0anlaMklZS2JkWjNaOWhya1R0YXlPMDNxN2NkczVWNHctWVlrMkRkN0c1UkNGZ1RRaUo2cFk3Mnl1YTVXZDRGUQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: statefulset-controller
      kubernetes.io/service-account.uid: fb1ccddc-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: statefulset-controller-token-k7k92
    namespace: kube-system
    resourceVersion: "293"
    selfLink: /api/v1/namespaces/kube-system/secrets/statefulset-controller-token-k7k92
    uid: fb1dbce7-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKMGFXeHNaWEl0ZEc5clpXNHRZMmhtY213aUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNXVZVzFsSWpvaWRHbHNiR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWRXbGtJam9pTlRObVlUSmlObVV0TUdSa01pMHhNV1U0TFRnM01XSXRNREExTURVMllXWTVaVGszSWl3aWMzVmlJam9pYzNsemRHVnRPbk5sY25acFkyVmhZMk52ZFc1ME9tdDFZbVV0YzNsemRHVnRPblJwYkd4bGNpSjkuQkdETVBOQ1VNaWExYVhKZThfaVRXNjJzSWxzeTNoY2lrT3I3SmdhWWdoNFBaVnI1S1NXTDJ2ekJqaG01NWRJODVDaXlqUFItYmRGWUJBN2drSzlSYXpybVk1Vzk0WXg1emI4eU1ESmk5emh5VGlrUnlpYWZla3BwVDdoc1FuRDNUM1cxeXBYTzJwUWdXaWF2dzktVzM2bjNzMmJRTkF3V0Zab2RZLURJejYtdkpfalBXbWZYaTFrTTBheXBfZVIyOW9Pcmg5RGc0dGw2U1ZGVEdFckFYVjk5QWpySUdrVFc3Mmo5R3d4Mk03THhNbzFMbFd3WjhEcG5Oai1ONktYckZfYWZjdVU3NWVkQ1MybGhBLXpWMEg1QnA0Z0FGM2JDc2Z4cU5KT3hJbmlzZ3ZGUEIxVGZETXo0OXNud2p0VXhQSGJ1M05oUnZ6WHJ2ck0tbW5lWE9R
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: tiller
      kubernetes.io/service-account.uid: 53fa2b6e-0dd2-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:49:23Z
    name: tiller-token-chfrl
    namespace: kube-system
    resourceVersion: "1017"
    selfLink: /api/v1/namespaces/kube-system/secrets/tiller-token-chfrl
    uid: 53fb7c10-0dd2-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKMGIydGxiaTFqYkdWaGJtVnlMWFJ2YTJWdUxUbDZiRFp1SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaWEoyYVdObExXRmpZMjkxYm5RdWJtRnRaU0k2SW5SdmEyVnVMV05zWldGdVpYSWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzUxYVdRaU9pSm1OR1UzT1RWbE9DMHdaR1F4TFRFeFpUZ3RPRGN4WWkwd01EVXdOVFpoWmpsbE9UY2lMQ0p6ZFdJaU9pSnplWE4wWlcwNmMyVnlkbWxqWldGalkyOTFiblE2YTNWaVpTMXplWE4wWlcwNmRHOXJaVzR0WTJ4bFlXNWxjaUo5LnU2blkxcUNQd0EwVTBtZ1o5clplRkRoSHlBbWV6MXFHSUhqeUZHdXR4alByUkw1bTNYZ3VPMGhZUTB5dTRlVkxQR2hwMTh3aU1CTDRodHhidjZaaEFjV0xSZnlaQVk4NzVCNFd4QWVSN3VDS3NTcjZlREFEa1pEdGl4SnZPOUtvRFdKcXF1NlNxcVBOWUF1RzExWWVFc2pQRkhCZ3pxajBINXNoM19hbkFOTm5pdFpWNU4ySXptNE9QTVhPT3hpVm9kdEJZU1U2T1E1TWtYQWxMa0lONVQ3dS1qVUVJemVraGw1cG83NDFiYTJjcDlmd2R6QlhXNmVNSGRrejRDQ3UtUVd2NjdxRHNoME9QVHFTQ2ozVHZfNUlrOEtHeTZlRFVhVWRKQW1IREstdUJZQTgyWDdwV2ZCbjhXc1VIa3VDQ3VpektkU3BIc0s5cVV3Z0E3dF9tQQ==
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: token-cleaner
      kubernetes.io/service-account.uid: f4e795e8-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:44Z
    name: token-cleaner-token-9zl6n
    namespace: kube-system
    resourceVersion: "187"
    selfLink: /api/v1/namespaces/kube-system/secrets/token-cleaner-token-9zl6n
    uid: f4e90e45-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1ESXdPVEU1TkRZeE5Gb1hEVEk0TURJd056RTVORFl4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHF1CmFyU2g5eG8rUFJxNGJUK2EvRC8yZUZ4TnhldGo3amNqY3dCMW84TUx6UEN2cHU5NHZvUUxYYWc0NG1KdXVERUcKVHBFZGUyU0orMVBZdi9Yd1BzcWFmcHhOU3ZmOTJVOVVWQkFVWHFocUJlc2ZTK1NLeStsYWF1WXU0RWluL1U0SQoxQzJOZG9hdjJ1RWtrYXNhT0FYN21yU0NvMDlweXNTS1IreWJFZG44cTB3V0xabE9nL054OVljYU5uOVR2c2l5CnA2elBBOW9EZFlRN2NydUVBcHdhbytOQzgrNEFOdi9abFVsaGNCRnRvaHVJaTJFYkRHSUZHUjFhMWNhMFBTOWMKWC9tM3o2TER0VlpaWTFXNDREamsrMVczNHY5SHN6M2YyZTcwQ3Q3bHh0Z2pka2F5ODlSZWV6MklVOHAxWGpyTwpqV1dOa081NENCTmVUKyt6R08wQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLenpQbXRXb2ZaZXpYYmhpVmQwdkRGQkhhTGIKUkl3QmVJN1lpMUM4M1plRC9HWjV0QkZHRTZYYzlkZWFxcVJJRllSZjdZOW1pZ284dmVCRDR0RG40TUdGcVBkOQp3UlpNRlhzbHp6Ulg3YWlSOGhQQXhJa0V6d0FhRm9ZUm1zNjdhVG5xdkRLYi9mbkwvTENaUTc2RUIrQWlGaVJzCjQxNWthSnNzLzExWWIxelJJdFpZMGRIeEJ1K21PcXYwdXZaMXlDN25BUnAzUWJIeFZzei94SDBrRmlhMTJCVWcKZVl5ZFN0d2ZFenBsWEcwSDl6T3A1UGtEbjcwSTcxNGllL1JwQ3IyQUVJZWxtQll5ZjFzbTIrU3hEdHVWL3NrbQo0VUhKR1c2YlRSZXMvakVCV1NVblh2Ulc0SW1IOTdzcVI2TU5ZWG9pV2hjaE16cnRtcDNHMXRjZkNSUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: a3ViZS1zeXN0ZW0=
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKMGRHd3RZMjl1ZEhKdmJHeGxjaTEwYjJ0bGJpMTRaMnM0YkNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExtNWhiV1VpT2lKMGRHd3RZMjl1ZEhKdmJHeGxjaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbVppTVdWbU1tUmpMVEJrWkRFdE1URmxPQzA0TnpGaUxUQXdOVEExTm1GbU9XVTVOeUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwcmRXSmxMWE41YzNSbGJUcDBkR3d0WTI5dWRISnZiR3hsY2lKOS54TVFxNHUzMUV3RGRzbGxSNkVhODloSDVDaF9jNUk5Q0lYdzJTMk1EN3NHZVNNcjhmcUhiX1ZzNTVzTkNkSDhmM2lDaDI5MzFjcXZpWjd1ZjNNYmI3LXpuazROblpURDBnZWJnNGRZME9md25yM3BIQnJLY2c0MzZ6RzlOdWU5NmJHaEJidUFOUWVVQ2NsMmpad3BiSFc0VGwxOXdUR0Y1UExMZU4ySnQwZXhPUkRGZTJyY0k5ZHFleXZ3TDBoMGlyUjBTank4YUlzaUF4THpKYzR1eWc1Z0lHb21yd3BTSV8ydnFENjJoNHRQRVBRNGlkQXhXb0dFQVliX0t6LU1BeUhQa1E5dDJtZExENGFhcFc4NWQ0WUdXNmcyVkZzZzJ3M2FDQzVKcFdhTGxSaHpma3lNa2tkbUZwUEl4azFhMzF3RGV0T1dMa1FQUU5fekxPUjl4RHc=
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: ttl-controller
      kubernetes.io/service-account.uid: fb1ef2dc-0dd1-11e8-871b-005056af9e97
    creationTimestamp: 2018-02-09T19:46:54Z
    name: ttl-controller-token-xgk8l
    namespace: kube-system
    resourceVersion: "296"
    selfLink: /api/v1/namespaces/kube-system/secrets/ttl-controller-token-xgk8l
    uid: fb20028b-0dd1-11e8-871b-005056af9e97
  type: kubernetes.io/service-account-token
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get serviceaccounts --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    name: default
    namespace: default
    resourceVersion: "374"
    selfLink: /api/v1/namespaces/default/serviceaccounts/default
    uid: fd020fda-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: default-token-fc6vg
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-13T15:26:20Z
    name: kube-keepalived-vip
    namespace: default
    resourceVersion: "438739"
    selfLink: /api/v1/namespaces/default/serviceaccounts/kube-keepalived-vip
    uid: 3e12798d-10d2-11e8-871b-005056af9e97
  secrets:
  - name: kube-keepalived-vip-token-2w47w
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress
    namespace: default
    resourceVersion: "473401"
    selfLink: /api/v1/namespaces/default/serviceaccounts/my-nginx-ingress
    uid: a144ef5b-1108-11e8-871b-005056af9e97
  secrets:
  - name: my-nginx-ingress-token-9ffd9
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    name: default
    namespace: kube-public
    resourceVersion: "363"
    selfLink: /api/v1/namespaces/kube-public/serviceaccounts/default
    uid: fcfc2c5b-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: default-token-7g442
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:55Z
    name: attachdetach-controller
    namespace: kube-system
    resourceVersion: "317"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/attachdetach-controller
    uid: fbd9465c-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: attachdetach-controller-token-pdmt4
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: bootstrap-signer
    namespace: kube-system
    resourceVersion: "153"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/bootstrap-signer
    uid: f495ec61-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: bootstrap-signer-token-nhshd
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"calico-node","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:45Z
    name: calico-node
    namespace: kube-system
    resourceVersion: "228"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/calico-node
    uid: f5b1b250-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: calico-node-token-m5t54
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: certificate-controller
    namespace: kube-system
    resourceVersion: "150"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/certificate-controller
    uid: f49322b2-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: certificate-controller-token-sxs2h
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: cronjob-controller
    namespace: kube-system
    resourceVersion: "147"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/cronjob-controller
    uid: f4813661-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: cronjob-controller-token-4dlm7
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:55Z
    name: daemon-set-controller
    namespace: kube-system
    resourceVersion: "307"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/daemon-set-controller
    uid: fb66d5f3-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: daemon-set-controller-token-4jwtx
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:57Z
    name: default
    namespace: kube-system
    resourceVersion: "389"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/default
    uid: fd0d0d02-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: default-token-kxl84
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: deployment-controller
    namespace: kube-system
    resourceVersion: "288"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/deployment-controller
    uid: fb170767-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: deployment-controller-token-8gz7n
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:55Z
    name: disruption-controller
    namespace: kube-system
    resourceVersion: "310"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/disruption-controller
    uid: fb8cfe9e-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: disruption-controller-token-sndts
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true"},"name":"elasticsearch-logging","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "244"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/elasticsearch-logging
    uid: f6540f35-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: elasticsearch-logging-token-6cmks
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: endpoint-controller
    namespace: kube-system
    resourceVersion: "300"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/endpoint-controller
    uid: fb214d94-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: endpoint-controller-token-6xqjn
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-es","kubernetes.io/cluster-service":"true"},"name":"fluentd-es","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:46Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-es
      kubernetes.io/cluster-service: "true"
    name: fluentd-es
    namespace: kube-system
    resourceVersion: "236"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/fluentd-es
    uid: f6161ec1-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: fluentd-es-token-zw92g
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:56Z
    name: generic-garbage-collector
    namespace: kube-system
    resourceVersion: "326"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/generic-garbage-collector
    uid: fc4bb2c6-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: generic-garbage-collector-token-m25fs
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: horizontal-pod-autoscaler
    namespace: kube-system
    resourceVersion: "165"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/horizontal-pod-autoscaler
    uid: f4a18ca5-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: horizontal-pod-autoscaler-token-jbgqq
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: job-controller
    namespace: kube-system
    resourceVersion: "162"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/job-controller
    uid: f49f0ddd-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: job-controller-token-d769b
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kube-dns
    namespace: kube-system
    resourceVersion: "179"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/kube-dns
    uid: f4d65df0-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: kube-dns-token-66tfx
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "191"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/kube-proxy
    uid: f4e8d418-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: kube-proxy-token-ktcv9
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"}}
    creationTimestamp: 2018-02-09T19:46:48Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "268"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/kubernetes-dashboard
    uid: f790fb2e-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: kubernetes-dashboard-token-bjcwc
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: namespace-controller
    namespace: kube-system
    resourceVersion: "303"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/namespace-controller
    uid: fb40aee0-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: namespace-controller-token-w7zqj
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: node-controller
    namespace: kube-system
    resourceVersion: "196"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/node-controller
    uid: f50dc874-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: node-controller-token-mg5m9
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:55Z
    name: persistent-volume-binder
    namespace: kube-system
    resourceVersion: "314"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/persistent-volume-binder
    uid: fbb31edd-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: persistent-volume-binder-token-jn99w
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:56Z
    name: pod-garbage-collector
    namespace: kube-system
    resourceVersion: "320"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/pod-garbage-collector
    uid: fbff725b-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: pod-garbage-collector-token-ktn79
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: replicaset-controller
    namespace: kube-system
    resourceVersion: "291"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/replicaset-controller
    uid: fb1a87ec-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: replicaset-controller-token-qgr8m
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: replication-controller
    namespace: kube-system
    resourceVersion: "285"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/replication-controller
    uid: fb144c79-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: replication-controller-token-gw4vz
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: resourcequota-controller
    namespace: kube-system
    resourceVersion: "159"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/resourcequota-controller
    uid: f49ca445-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: resourcequota-controller-token-fpsgt
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:56Z
    name: service-account-controller
    namespace: kube-system
    resourceVersion: "323"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/service-account-controller
    uid: fc258895-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: service-account-controller-token-xqln5
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:43Z
    name: service-controller
    namespace: kube-system
    resourceVersion: "156"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/service-controller
    uid: f49a37cc-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: service-controller-token-p75wq
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: statefulset-controller
    namespace: kube-system
    resourceVersion: "294"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/statefulset-controller
    uid: fb1ccddc-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: statefulset-controller-token-k7k92
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:49:23Z
    name: tiller
    namespace: kube-system
    resourceVersion: "1018"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/tiller
    uid: 53fa2b6e-0dd2-11e8-871b-005056af9e97
  secrets:
  - name: tiller-token-chfrl
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    name: token-cleaner
    namespace: kube-system
    resourceVersion: "190"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/token-cleaner
    uid: f4e795e8-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: token-cleaner-token-9zl6n
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-02-09T19:46:54Z
    name: ttl-controller
    namespace: kube-system
    resourceVersion: "297"
    selfLink: /api/v1/namespaces/kube-system/serviceaccounts/ttl-controller
    uid: fb1ef2dc-0dd1-11e8-871b-005056af9e97
  secrets:
  - name: ttl-controller-token-xgk8l
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get services --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:46:40Z
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "31"
    selfLink: /api/v1/namespaces/default/services/kubernetes
    uid: f2ea6e5d-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: controller
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-controller
    namespace: default
    resourceVersion: "473407"
    selfLink: /api/v1/namespaces/default/services/my-nginx-ingress-controller
    uid: a149d7a0-1108-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.107.38.119
    externalIPs:
    - 10.10.97.200
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    selector:
      app: nginx-ingress
      component: controller
      release: my-nginx-ingress
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-13T21:55:39Z
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.9.2
      component: default-backend
      heritage: Tiller
      release: my-nginx-ingress
    name: my-nginx-ingress-default-backend
    namespace: default
    resourceVersion: "473410"
    selfLink: /api/v1/namespaces/default/services/my-nginx-ingress-default-backend
    uid: a14e0e37-1108-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.69.91
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: nginx-ingress
      component: default-backend
      release: my-nginx-ingress
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-12T16:02:17Z
    labels:
      app: tea
    name: tea-svc
    namespace: default
    resourceVersion: "327042"
    selfLink: /api/v1/namespaces/default/services/tea-svc
    uid: 198efef6-100e-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.196.71
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: tea
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"calico-typha"},"name":"calico-typha","namespace":"kube-system"},"spec":{"ports":[{"name":"calico-typha","port":5473,"protocol":"TCP","targetPort":"calico-typha"}],"selector":{"k8s-app":"calico-typha"}}}
    creationTimestamp: 2018-02-09T19:46:45Z
    labels:
      k8s-app: calico-typha
    name: calico-typha
    namespace: kube-system
    resourceVersion: "206"
    selfLink: /api/v1/namespaces/kube-system/services/calico-typha
    uid: f587a909-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.99.51.177
    ports:
    - name: calico-typha
      port: 5473
      protocol: TCP
      targetPort: calico-typha
    selector:
      k8s-app: calico-typha
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Elasticsearch"},"name":"elasticsearch-logging","namespace":"kube-system"},"spec":{"ports":[{"port":9200,"protocol":"TCP","targetPort":"db"}],"selector":{"k8s-app":"elasticsearch-logging"}}}
    creationTimestamp: 2018-02-09T19:46:47Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Elasticsearch
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "252"
    selfLink: /api/v1/namespaces/kube-system/services/elasticsearch-logging
    uid: f69f1197-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.105.23.60
    ports:
    - port: 9200
      protocol: TCP
      targetPort: db
    selector:
      k8s-app: elasticsearch-logging
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kibana-logging","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Kibana"},"name":"kibana-logging","namespace":"kube-system"},"spec":{"ports":[{"nodePort":30601,"port":5601,"protocol":"TCP","targetPort":"ui"}],"selector":{"k8s-app":"kibana-logging"},"type":"NodePort"}}
    creationTimestamp: 2018-02-09T19:46:47Z
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kibana-logging
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Kibana
    name: kibana-logging
    namespace: kube-system
    resourceVersion: "261"
    selfLink: /api/v1/namespaces/kube-system/services/kibana-logging
    uid: f70bbbe0-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.208.116
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 30601
      port: 5601
      protocol: TCP
      targetPort: ui
    selector:
      k8s-app: kibana-logging
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:46:44Z
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "184"
    selfLink: /api/v1/namespaces/kube-system/services/kube-dns
    uid: f4e7eb3e-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.96.0.10
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kube-system"},"spec":{"ports":[{"nodePort":31443,"port":443,"targetPort":8443}],"selector":{"k8s-app":"kubernetes-dashboard"},"type":"NodePort"}}
    creationTimestamp: 2018-02-09T19:46:48Z
    labels:
      k8s-app: kubernetes-dashboard
    name: kubernetes-dashboard
    namespace: kube-system
    resourceVersion: "274"
    selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard
    uid: f7a33124-0dd1-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.106.31.65
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 31443
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      k8s-app: kubernetes-dashboard
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: 2018-02-09T19:49:06Z
    labels:
      app: helm
      name: tiller
    name: tiller-deploy
    namespace: kube-system
    resourceVersion: "979"
    selfLink: /api/v1/namespaces/kube-system/services/tiller-deploy
    uid: 49a392f0-0dd2-11e8-871b-005056af9e97
  spec:
    clusterIP: 10.104.87.82
    ports:
    - name: tiller
      port: 44134
      protocol: TCP
      targetPort: tiller
    selector:
      app: helm
      name: tiller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get statefulsets --all-namespaces -o yaml
================

apiVersion: v1
items:
- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta2","kind":"StatefulSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"},"name":"elasticsearch-logging","namespace":"kube-system"},"spec":{"replicas":2,"selector":{"matchLabels":{"k8s-app":"elasticsearch-logging","version":"v5.6.4"}},"serviceName":"elasticsearch-logging","template":{"metadata":{"labels":{"k8s-app":"elasticsearch-logging","kubernetes.io/cluster-service":"true","version":"v5.6.4"}},"spec":{"containers":[{"env":[{"name":"NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4","name":"elasticsearch-logging","ports":[{"containerPort":9200,"name":"db","protocol":"TCP"},{"containerPort":9300,"name":"transport","protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"},"requests":{"cpu":"100m"}},"volumeMounts":[{"mountPath":"/data","name":"elasticsearch-logging"}]}],"initContainers":[{"command":["/sbin/sysctl","-w","vm.max_map_count=262144"],"image":"registry.ci.dfj.io/cpsg_ccp/alpine:3.6","name":"elasticsearch-logging-init","securityContext":{"privileged":true}}],"serviceAccountName":"elasticsearch-logging","volumes":[{"emptyDir":{},"name":"elasticsearch-logging"}]}}}}
    creationTimestamp: 2018-02-09T19:46:46Z
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: elasticsearch-logging
      kubernetes.io/cluster-service: "true"
      version: v5.6.4
    name: elasticsearch-logging
    namespace: kube-system
    resourceVersion: "843"
    selfLink: /apis/apps/v1beta1/namespaces/kube-system/statefulsets/elasticsearch-logging
    uid: f66f6df9-0dd1-11e8-871b-005056af9e97
  spec:
    podManagementPolicy: OrderedReady
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: elasticsearch-logging
        version: v5.6.4
    serviceName: elasticsearch-logging
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: elasticsearch-logging
          kubernetes.io/cluster-service: "true"
          version: v5.6.4
      spec:
        containers:
        - env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: registry.ci.dfj.io/cpsg_ccp/gcr.io/google-containers/elasticsearch:v5.6.4
          imagePullPolicy: IfNotPresent
          name: elasticsearch-logging
          ports:
          - containerPort: 9200
            name: db
            protocol: TCP
          - containerPort: 9300
            name: transport
            protocol: TCP
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: elasticsearch-logging
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /sbin/sysctl
          - -w
          - vm.max_map_count=262144
          image: registry.ci.dfj.io/cpsg_ccp/alpine:3.6
          imagePullPolicy: IfNotPresent
          name: elasticsearch-logging-init
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: elasticsearch-logging
        serviceAccountName: elasticsearch-logging
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: elasticsearch-logging
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    collisionCount: 0
    currentReplicas: 2
    currentRevision: elasticsearch-logging-7978c6964c
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updateRevision: elasticsearch-logging-7978c6964c
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get storageclasses --all-namespaces -o yaml
================

apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

================
kubectl get kubectl --all-namespaces -o yaml && echo ================n && kubectl get kubectl --all-namespaces -o yaml
